{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC3c aggregated data\n",
    "https://www.kaggle.com/datasets/drscarlat/mimic3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path_append = \"../../\"\n",
    "sys.path.append(path_append)  # Go up one directory from where you are.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(path_append + \"../data/mimic_custom/mimic3c.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['religion'].fillna('NOT SPECIFIED', inplace=True)\n",
    "df['marital_status'].fillna('UNKNOWN (DEFAULT)', inplace=True)\n",
    "df.dropna(inplace=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 58951 entries, 0 to 58975\n",
      "Data columns (total 28 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   hadm_id           58951 non-null  int64  \n",
      " 1   gender            58951 non-null  object \n",
      " 2   age               58951 non-null  int64  \n",
      " 3   LOSdays           58951 non-null  float64\n",
      " 4   admit_type        58951 non-null  object \n",
      " 5   admit_location    58951 non-null  object \n",
      " 6   AdmitDiagnosis    58951 non-null  object \n",
      " 7   insurance         58951 non-null  object \n",
      " 8   religion          58951 non-null  object \n",
      " 9   marital_status    58951 non-null  object \n",
      " 10  ethnicity         58951 non-null  object \n",
      " 11  NumCallouts       58951 non-null  float64\n",
      " 12  NumDiagnosis      58951 non-null  float64\n",
      " 13  NumProcs          58951 non-null  float64\n",
      " 14  AdmitProcedure    58951 non-null  object \n",
      " 15  NumCPTevents      58951 non-null  float64\n",
      " 16  NumInput          58951 non-null  float64\n",
      " 17  NumLabs           58951 non-null  float64\n",
      " 18  NumMicroLabs      58951 non-null  float64\n",
      " 19  NumNotes          58951 non-null  float64\n",
      " 20  NumOutput         58951 non-null  float64\n",
      " 21  NumRx             58951 non-null  float64\n",
      " 22  NumProcEvents     58951 non-null  float64\n",
      " 23  NumTransfers      58951 non-null  float64\n",
      " 24  NumChartEvents    58951 non-null  float64\n",
      " 25  ExpiredHospital   58951 non-null  int64  \n",
      " 26  TotalNumInteract  58951 non-null  float64\n",
      " 27  LOSgroupNum       58951 non-null  int64  \n",
      "dtypes: float64(15), int64(4), object(9)\n",
      "memory usage: 13.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected_target = df[['LOSdays']]\n",
    "df = df.drop('LOSdays', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tools.preprocessing.data_frame import auto_preprocess_dataframe\n",
    "# target_columns = ['ExpiredHospital']\n",
    "# drop_columns = ['hadm_id', 'AdmitDiagnosis', 'LOSgroupNum', 'AdmitProcedure']\n",
    "# encode_columns = ['gender', 'admit_type', 'admit_location', 'insurance', 'religion', 'marital_status',]\n",
    "# df, description = auto_preprocess_dataframe(df, target_columns=target_columns, drop_columns=drop_columns, encode_columns=encode_columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tools.preprocessing.template_dataset import TemplateDataset\n",
    "\n",
    "# train_df, test_df = train_test_split(df, test_size=0.2, shuffle=True, random_state=42)\n",
    "# # predict the next value in the sequence\n",
    "# train_df_x = train_df.iloc[:, :-1] # all columns except the last one\n",
    "# train_df_y = train_df.iloc[:, -1:] # only the last column\n",
    "\n",
    "# test_df_x = test_df.iloc[:, :-1] # all columns except the last one\n",
    "# test_df_y = test_df.iloc[:, -1:] # only the last column\n",
    "\n",
    "# print('train df shape: ', train_df.shape)\n",
    "# print('test df shape: ', test_df.shape)\n",
    "# trainset = TemplateDataset(train_df_x, train_df_y)\n",
    "# testset = TemplateDataset(test_df_x, test_df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tools.setting.data_config import DataConfig\n",
    "# from tools.setting.ml_params import MLParameters\n",
    "# from trainer_hub import TrainerHub\n",
    "\n",
    "# num_features = description['num_features']\n",
    "# num_classes = description['num_classes']\n",
    "# data_config = DataConfig(dataset_name = 'mimic3', task_type='binary_classification', obs_shape=[num_features], label_size=num_classes)\n",
    "\n",
    "# #  Set training configuration from the AlgorithmConfig class, returning them as a Namespace object.\n",
    "# ml_params = MLParameters(ccnet_network = 'tabnet', encoder_network = 'none')\n",
    "# ml_params.algorithm.error_function = 'mae'\n",
    "# ml_params.model.ccnet_config.num_layers = 3\n",
    "# ml_params.training.num_epoch = 5\n",
    "\n",
    "# # Set the device to GPU if available, else CPU\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# # Initialize the TrainerHub class with the training configuration, data configuration, device, and use_print and use_wandb flags\n",
    "# trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb= False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer_hub.train(trainset, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn.tabnet import TabNet \n",
    "from tools.setting.ml_params import ModelConfig\n",
    "import torch\n",
    "\n",
    "class AttributeClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers=3, hidden_size=256):\n",
    "        super(AttributeClassifier, self).__init__()\n",
    "        \n",
    "        model_config = ModelConfig('tabnet')\n",
    "        model_config.num_layers = num_layers\n",
    "        model_config.d_model = hidden_size\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Create a list to hold all layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(torch.nn.Linear(input_size, hidden_size))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        ## Add TabNet layers\n",
    "        layers.append(TabNet(model_config))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(torch.nn.Linear(hidden_size, output_size))\n",
    "        \n",
    "        # Register all layers\n",
    "        self.layers = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:18<02:49, 18.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.3920, Train Acc: 0.9337, Train F1: 0.9269, Val Acc: 0.9316, Val F1: 0.9249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:37<02:27, 18.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.3838, Train Acc: 0.9348, Train F1: 0.9258, Val Acc: 0.9317, Val F1: 0.9223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:55<02:08, 18.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.3772, Train Acc: 0.9372, Train F1: 0.9299, Val Acc: 0.9326, Val F1: 0.9250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:13<01:49, 18.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.3488, Train Acc: 0.9366, Train F1: 0.9262, Val Acc: 0.9325, Val F1: 0.9215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:31<01:30, 18.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.4029, Train Acc: 0.9370, Train F1: 0.9317, Val Acc: 0.9311, Val F1: 0.9255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:49<01:12, 18.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.3762, Train Acc: 0.9406, Train F1: 0.9350, Val Acc: 0.9341, Val F1: 0.9282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [02:07<00:53, 18.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.3523, Train Acc: 0.9394, Train F1: 0.9317, Val Acc: 0.9323, Val F1: 0.9234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [02:24<00:35, 17.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.3377, Train Acc: 0.9406, Train F1: 0.9357, Val Acc: 0.9329, Val F1: 0.9275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [02:42<00:17, 17.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.3849, Train Acc: 0.9423, Train F1: 0.9343, Val Acc: 0.9358, Val F1: 0.9269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:00<00:00, 18.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.3847, Train Acc: 0.9439, Train F1: 0.9384, Val Acc: 0.9348, Val F1: 0.9282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "\n",
    "# data = pd.read_csv(path_append + \"../data/mimic_custom/mimic3c.csv\")\n",
    "df.drop(columns=['hadm_id', 'AdmitDiagnosis', 'LOSgroupNum', 'AdmitProcedure'], inplace=True)\n",
    "\n",
    "\n",
    "y = LabelEncoder().fit_transform(df['ExpiredHospital'])\n",
    "X = df.drop('ExpiredHospital', axis=1)\n",
    "\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        X[col] = LabelEncoder().fit_transform(X[col])\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "dataset_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AttributeClassifier(input_size=X.shape[1], output_size=len(set(y)), num_layers=3, hidden_size=256)\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# 데이터 세트 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 훈련 및 검증 데이터셋 생성\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    true_labels, predicted_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    return accuracy, f1\n",
    "\n",
    "def train_model(train_loader, val_loader):\n",
    "    model.train()\n",
    "    for epoch in tqdm.tqdm(range(10)):\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, torch.nn.functional.one_hot(labels, num_classes=model.output_size).float())\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 에폭마다 성능 평가\n",
    "        train_accuracy, train_f1 = evaluate_model(model, train_loader)\n",
    "        val_accuracy, val_f1 = evaluate_model(model, val_loader)\n",
    "        print(f'Epoch [{epoch + 1}/10], Loss: {loss.item():.4f}, Train Acc: {train_accuracy:.4f}, Train F1: {train_f1:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1:.4f}')\n",
    "\n",
    "# 모델 훈련 시작\n",
    "train_model(train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
