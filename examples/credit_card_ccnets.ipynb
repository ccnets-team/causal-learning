{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author:\n",
    "        \n",
    "        PARK, JunHo, junho@ccnets.org\n",
    "\n",
    "        \n",
    "        KIM, JeongYoong, jeongyoong@ccnets.org\n",
    "        \n",
    "    COPYRIGHT (c) 2024. CCNets. All Rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path_append = \"../\"\n",
    "sys.path.append(path_append)  # Go up one directory from where you are.\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5   \n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321  \\\n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22   \n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838  \\\n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount   \n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62  \\\n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataroot = path_append + \"../data/credit_card_fraud_detection/creditcard.csv\"\n",
    "df = pd.read_csv(dataroot)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.83 %of the dataset\n",
      "Frauds 0.17 %of the dataset\n"
     ]
    }
   ],
   "source": [
    "print('No Frauds', round(df['Class'].value_counts()[0] / len(df) *100,2), '%of the dataset')\n",
    "print('Frauds', round(df['Class'].value_counts()[1] / len(df) *100,2), '%of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://fraud-detection-handbook.github.io/fraud-detection-handbook/Chapter_7_DeepLearning/FeedForwardNeuralNetworks.html\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        vals = torch.tensor(self.x[index], dtype = torch.float32)\n",
    "        label = torch.tensor(self.y[index], dtype = torch.float32).unsqueeze(-1)\n",
    "        return vals, label\n",
    "\n",
    "y = df[['Class']]\n",
    "X = df.drop(['Class'],axis=1)\n",
    "\n",
    "sc = RobustScaler()\n",
    "X['scaled_amount'] = sc.fit_transform(X['Amount'].values.reshape(-1,1))\n",
    "X['scaled_time'] = sc.fit_transform(X['Time'].values.reshape(-1,1))\n",
    "X.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "X = X[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features\n",
    "n_features = X.shape[1]\n",
    "# number of label classes\n",
    "n_classes = y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.setting.ml_params import MLParameters\n",
    "from tools.setting.data_config import DataConfig\n",
    "from nn.utils.init import set_random_seed\n",
    "set_random_seed(0)\n",
    "\n",
    "from trainer_hub import TrainerHub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, shuffle= True)\n",
    "\n",
    "X_train = X_train.iloc[:, :].values \n",
    "X_test = X_test.iloc[:, :].values \n",
    "y_train = y_train.iloc[:, -1].values\n",
    "y_test = y_test.iloc[:, -1].values\n",
    "\n",
    "trainset = Dataset(X_train, y_train)\n",
    "testset = Dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = DataConfig(dataset_name = 'CreditCardFraudDetection', task_type='binary_classification', obs_shape=[n_features], label_size=n_classes)\n",
    "\n",
    "#  Set training configuration from the AlgorithmConfig class, returning them as a Namespace object.\n",
    "ml_params = MLParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_params.core_model_name = 'gpt' \n",
    "ml_params.encoder_model_name = 'none'\n",
    "ml_params.training.max_epoch = 1\n",
    "ml_params.learning_rate = 1e-4\n",
    "ml_params.decay_rate_100k = 0.001\n",
    "# Set the device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Initialize the TrainerHub class with the training configuration, data configuration, device, and use_print and use_wandb flags\n",
    "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb=False, use_full_eval=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e7369519164a5e8bffee63a1b649c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1df42ffeac54a82abbcd7620f20a7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/2225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][50/2225][Time 4.16]\n",
      "Unified LR across all optimizers: 9.96483243133418e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.2189\tGen: 0.5123\tRec: 0.5639\tE: 0.1673\tR: 0.2705\tP: 0.8572\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][100/2225][Time 3.47]\n",
      "Unified LR across all optimizers: 9.930474487640473e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0013\tGen: 0.2997\tRec: 0.2996\tE: 0.0014\tR: 0.0011\tP: 0.5980\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][150/2225][Time 3.68]\n",
      "Unified LR across all optimizers: 9.896235007383356e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0015\tGen: 0.2427\tRec: 0.2424\tE: 0.0018\tR: 0.0012\tP: 0.4837\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][200/2225][Time 3.55]\n",
      "Unified LR across all optimizers: 9.862113582110396e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0020\tGen: 0.2350\tRec: 0.2344\tE: 0.0026\tR: 0.0015\tP: 0.4673\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][250/2225][Time 3.68]\n",
      "Unified LR across all optimizers: 9.828109804777463e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0011\tGen: 0.1986\tRec: 0.1984\tE: 0.0012\tR: 0.0009\tP: 0.3959\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][300/2225][Time 3.93]\n",
      "Unified LR across all optimizers: 9.794223269743889e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0013\tGen: 0.1888\tRec: 0.1885\tE: 0.0016\tR: 0.0010\tP: 0.3760\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][350/2225][Time 3.98]\n",
      "Unified LR across all optimizers: 9.760453572767617e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0007\tGen: 0.1830\tRec: 0.1830\tE: 0.0007\tR: 0.0007\tP: 0.3653\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][400/2225][Time 3.61]\n",
      "Unified LR across all optimizers: 9.726800311000394e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0025\tGen: 0.1786\tRec: 0.1785\tE: 0.0026\tR: 0.0023\tP: 0.3546\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][450/2225][Time 3.44]\n",
      "Unified LR across all optimizers: 9.693263082982948e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0010\tGen: 0.1680\tRec: 0.1676\tE: 0.0013\tR: 0.0006\tP: 0.3346\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][500/2225][Time 3.40]\n",
      "Unified LR across all optimizers: 9.659841488640206e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0014\tGen: 0.1757\tRec: 0.1756\tE: 0.0015\tR: 0.0013\tP: 0.3499\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][550/2225][Time 3.51]\n",
      "Unified LR across all optimizers: 9.626535129276525e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0013\tGen: 0.1641\tRec: 0.1639\tE: 0.0016\tR: 0.0010\tP: 0.3267\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][600/2225][Time 3.61]\n",
      "Unified LR across all optimizers: 9.593343607570939e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0013\tGen: 0.1549\tRec: 0.1551\tE: 0.0011\tR: 0.0016\tP: 0.3087\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][650/2225][Time 3.52]\n",
      "Unified LR across all optimizers: 9.560266527572399e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0009\tGen: 0.1542\tRec: 0.1542\tE: 0.0010\tR: 0.0009\tP: 0.3075\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][700/2225][Time 3.56]\n",
      "Unified LR across all optimizers: 9.52730349469507e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0008\tGen: 0.1538\tRec: 0.1539\tE: 0.0007\tR: 0.0010\tP: 0.3068\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][750/2225][Time 3.55]\n",
      "Unified LR across all optimizers: 9.494454115713613e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0023\tGen: 0.1591\tRec: 0.1589\tE: 0.0025\tR: 0.0021\tP: 0.3157\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][800/2225][Time 3.59]\n",
      "Unified LR across all optimizers: 9.461717998758508e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0017\tGen: 0.1590\tRec: 0.1590\tE: 0.0017\tR: 0.0017\tP: 0.3163\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][850/2225][Time 3.42]\n",
      "Unified LR across all optimizers: 9.429094753311366e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0013\tGen: 0.1443\tRec: 0.1443\tE: 0.0013\tR: 0.0014\tP: 0.2872\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][900/2225][Time 3.51]\n",
      "Unified LR across all optimizers: 9.396583990200264e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0013\tGen: 0.1444\tRec: 0.1446\tE: 0.0011\tR: 0.0015\tP: 0.2877\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][950/2225][Time 3.43]\n",
      "Unified LR across all optimizers: 9.364185321595131e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0017\tGen: 0.1564\tRec: 0.1560\tE: 0.0021\tR: 0.0012\tP: 0.3108\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][1000/2225][Time 3.56]\n",
      "Unified LR across all optimizers: 9.331898361003084e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0012\tGen: 0.1418\tRec: 0.1418\tE: 0.0012\tR: 0.0012\tP: 0.2824\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][1050/2225][Time 3.46]\n",
      "Unified LR across all optimizers: 9.299722723263858e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0028\tGen: 0.1392\tRec: 0.1390\tE: 0.0030\tR: 0.0026\tP: 0.2754\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][1100/2225][Time 3.43]\n",
      "Unified LR across all optimizers: 9.267658024545165e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0018\tGen: 0.1381\tRec: 0.1376\tE: 0.0022\tR: 0.0014\tP: 0.2739\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][1150/2225][Time 3.48]\n",
      "Unified LR across all optimizers: 9.235703882338157e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0012\tGen: 0.1316\tRec: 0.1313\tE: 0.0014\tR: 0.0009\tP: 0.2618\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][1200/2225][Time 3.35]\n",
      "Unified LR across all optimizers: 9.203859915452835e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0019\tGen: 0.1323\tRec: 0.1326\tE: 0.0016\tR: 0.0022\tP: 0.2630\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][1250/2225][Time 3.41]\n",
      "Unified LR across all optimizers: 9.172125744013532e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0017\tGen: 0.1327\tRec: 0.1329\tE: 0.0014\tR: 0.0020\tP: 0.2639\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][1300/2225][Time 3.49]\n",
      "Unified LR across all optimizers: 9.140500989454345e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0020\tGen: 0.1311\tRec: 0.1315\tE: 0.0016\tR: 0.0024\tP: 0.2606\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][1350/2225][Time 3.52]\n",
      "Unified LR across all optimizers: 9.108985274514629e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0019\tGen: 0.1286\tRec: 0.1285\tE: 0.0021\tR: 0.0017\tP: 0.2552\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][1400/2225][Time 3.47]\n",
      "Unified LR across all optimizers: 9.077578223234527e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0017\tGen: 0.1333\tRec: 0.1334\tE: 0.0016\tR: 0.0017\tP: 0.2650\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][1450/2225][Time 3.42]\n",
      "Unified LR across all optimizers: 9.046279460950436e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0014\tGen: 0.1290\tRec: 0.1287\tE: 0.0016\tR: 0.0011\tP: 0.2563\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n",
      "[0/1][1500/2225][Time 3.53]\n",
      "Unified LR across all optimizers: 9.015088614290584e-05\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0025\tGen: 0.1328\tRec: 0.1331\tE: 0.0022\tR: 0.0028\tP: 0.2635\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 1.0000\n",
      "precision: 1.0000\n",
      "recall: 1.0000\n",
      "f1_score: 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer_hub.train(trainset, testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Synthetic Data Generation\n",
    "\n",
    "In this section of the notebook, we are performing a series of operations to prepare our training data and utilize a causal cooperative network (CCNet) to generate synthetic data based on the explanations derived from the original training data. Each step is described as follows:\n",
    "\n",
    "1. **Data Loading**:\n",
    "   - `training_data, training_labels = trainset[:]`\n",
    "   This line extracts all the features and labels from `trainset`, which is presumably a pre-loaded dataset formatted for training. Here, slicing `[:]` is used to retrieve all data and labels without any modifications.\n",
    "\n",
    "2. **Device Assignment**:\n",
    "   - `training_data = training_data.to(device)`\n",
    "   - `training_labels = training_labels.to(device)`\n",
    "   These lines transfer the training data and labels to a designated computing device (`device`). This device could be a CPU or a GPU and is typically specified to optimize computational efficiency. Moving data to the device ensures that all subsequent operations that require computation can leverage hardware acceleration.\n",
    "\n",
    "3. **Data Explanation**:\n",
    "   - `explanation = trainer_hub.core_ccnet.explain(training_data)`\n",
    "   Here, the `explain` method of the `core_ccnet` module within `trainer_hub` is called with the training data. This function is expected to analyze the data and provide an \"explanation\" for each instance, which could be feature importances or another form of interpretable output that explains why certain predictions might be made from the data.\n",
    "\n",
    "4. **Synthetic Data Generation**:\n",
    "   - `recreated_data, recreated_label = trainer_hub.core_ccnet.generate(explanation)`\n",
    "   This line generates synthetic data and labels by feeding the explanations obtained from the original data into the `generate` method of `core_ccnet`. The generate method uses the explanations to create new data instances that mimic or expand upon the patterns found in the original dataset. This is particularly useful for enhancing dataset diversity, balancing classes, or improving model robustness by providing additional training samples.\n",
    "\n",
    "By the end of this process, `recreated_data` and `recreated_label` contain newly generated data and labels that can be used for further training, testing, or analysis to enhance the model's performance or robustness against various types of data inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, training_labels = trainset[:]\n",
    "training_data = training_data.to(device)\n",
    "training_labels = training_labels.to(device)\n",
    "explanation = trainer_hub.core_ccnet.explain(training_data)\n",
    "recreated_data, recreated_label = trainer_hub.core_ccnet.generate(explanation)\n",
    "recreated_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming recreated_data is a PyTorch tensor and y is the labels associated with the data\n",
    "recreated_data.squeeze_(dim=1)\n",
    "recreated_label.squeeze_()\n",
    "\n",
    "# Convert recreated_data to a NumPy array\n",
    "recreated_data_data_np = recreated_data.cpu().detach().numpy()\n",
    "recreated_label_data_np = recreated_label.cpu().detach().numpy()\n",
    "\n",
    "# Create the dataset using the converted data and labels\n",
    "ccnet_balanced_dataset = Dataset(recreated_data_data_np, recreated_label_data_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers=3, hidden_size=128):\n",
    "        super(DNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Create a list to hold all layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(torch.nn.Linear(input_size, hidden_size))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(torch.nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(torch.nn.Linear(hidden_size, output_size))\n",
    "        layers.append(torch.nn.Sigmoid())\n",
    "        \n",
    "        # Register all layers\n",
    "        self.layers = torch.nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised_model(model, trainset):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    set_random_seed(0)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "    for epoch in range(1):\n",
    "        for i, (data, label) in enumerate(trainloader):\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(data)\n",
    "            loss = torch.nn.functional.binary_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained_on_original = DNN(input_size= n_features, output_size=n_classes).to(device)\n",
    "\n",
    "train_supervised_model(model_trained_on_original, trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained_on_recreated = DNN(input_size= n_features, output_size=n_classes).to(device)\n",
    "\n",
    "train_supervised_model(model_trained_on_recreated, ccnet_balanced_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "\n",
    "def get_f1_score(model, testset):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to track gradients for inference\n",
    "        test_data, test_labels = testset[:]  # Assuming this retrieves all data and labels\n",
    "        test_data = test_data.to(device)  # Ensure the data is on the correct device\n",
    "        test_labels = test_labels.to(device)\n",
    "        outputs = model(test_data)\n",
    "        \n",
    "        # Assuming model outputs a single probability per instance, apply threshold\n",
    "        predicted = (outputs.squeeze() > 0.5).long()  # Convert probabilities to 0 or 1 based on threshold\n",
    "        \n",
    "        y_true.extend(test_labels.cpu().numpy())  # Move labels back to CPU and convert to numpy\n",
    "        y_pred.extend(predicted.cpu().numpy())  # Move predictions back to CPU and convert to numpy\n",
    "\n",
    "    # Compute F1 score, using 'binary' because this is a binary classification task\n",
    "    score = f1_score(y_true, y_pred, average='binary')\n",
    "    return score\n",
    "\n",
    "# Usage example with two models:\n",
    "f1_score_original = get_f1_score(model_trained_on_original, testset)\n",
    "f1_score_recreated = get_f1_score(model_trained_on_recreated, testset)\n",
    "\n",
    "print(\"F1 score of the supervised learning model trained on the original data: \", f1_score_original)\n",
    "print(\"F1 score of the supervised learning model trained on the recreated data: \", f1_score_recreated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
