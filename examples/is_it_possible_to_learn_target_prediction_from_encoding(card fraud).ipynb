{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author:\n",
    "        \n",
    "        PARK, JunHo, junho@ccnets.org\n",
    "\n",
    "        \n",
    "        KIM, JeongYoong, jeongyoong@ccnets.org\n",
    "        \n",
    "    COPYRIGHT (c) 2024. CCNets. All Rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is it Possible to Learn Target Prediction from Encoding? : Handling Credit Card Fraud Imbalanced Dataset with CCNet\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial explores the use of a Cooperative Encoding Network (CCNet) to address challenges associated with imbalanced datasets in the domain of credit card fraud detection. By leveraging the power of synthetic data generation, we aim to enhance the diversity and volume of training data, thereby improving the robustness and accuracy of models designed to identify fraudulent transactions.\n",
    "\n",
    "## Tutorial Goals\n",
    "\n",
    "The objectives of this tutorial are designed to guide you through the process of enhancing data quality and model performance:\n",
    "\n",
    "### Dataset Recreation with CCNet\n",
    "- **Understand Data Augmentation**: Learn how encoding techniques can be used to generate synthetic data instances that closely mimic the characteristics of real-world fraudulent and non-fraudulent transactions.\n",
    "- **Impact on Model Training**: Assess how augmenting the dataset influences the training process and subsequently, the model's ability to generalize from training to real-world scenarios.\n",
    "\n",
    "### Model Training and Evaluation\n",
    "- **Dual Model Training**: Train two distinct models to directly compare performance metrics:\n",
    "  - A model trained on the **original dataset**.\n",
    "  - A model trained on the **CCNet-augmented dataset**.\n",
    "- **Performance Metrics**: Use the F1 score, a critical measure for models operating on imbalanced datasets, to evaluate and compare the effectiveness of these models.\n",
    "\n",
    "### Testing and Validation\n",
    "- **Independent Model Testing**: Conduct a thorough evaluation of both models using a standalone test set that was not involved in the training phase.\n",
    "- **Objective Analysis**: Critically analyze the outcomes to validate whether data augmentation through CCNet offers a tangible benefit in detecting credit card fraud.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By the end of this tutorial, participants will not only grasp the theoretical underpinnings of using synthetic data to combat data imbalance but also gain hands-on experience in applying these concepts through CCNet to potentially enhance model performance in fraud detection tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path_append = \"../\"\n",
    "sys.path.append(path_append)  # Go up one directory from where you are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CCNets-team\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\CCNets-team\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://fraud-detection-handbook.github.io/fraud-detection-handbook/Chapter_7_DeepLearning/FeedForwardNeuralNetworks.html\n",
    "import pandas as pd \n",
    "\n",
    "dataroot = path_append + \"../data/credit_card_fraud_detection/creditcard.csv\"\n",
    "df = pd.read_csv(dataroot)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.83 %of the dataset\n",
      "Frauds 0.17 %of the dataset\n"
     ]
    }
   ],
   "source": [
    "print('No Frauds', round(df['Class'].value_counts()[0] / len(df) *100,2), '%of the dataset')\n",
    "print('Frauds', round(df['Class'].value_counts()[1] / len(df) *100,2), '%of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 1 31\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Initialize scalers\n",
    "standard_sc = StandardScaler()\n",
    "robust_sc = RobustScaler()\n",
    "# Scale 'Amount' and 'Time' columns separately\n",
    "df['scaled_amount'] = robust_sc.fit_transform(df['Amount'].values.reshape(-1, 1))\n",
    "df['scaled_time'] = robust_sc.fit_transform(df['Time'].values.reshape(-1, 1))\n",
    "\n",
    "# Drop original 'Time' and 'Amount' columns\n",
    "df.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
    "\n",
    "# Move the 'Class' column to the end\n",
    "class_column = df.pop('Class')\n",
    "df = pd.concat([df, class_column], axis=1)\n",
    "\n",
    "# Scale the rest of the features (excluding the class column)\n",
    "df.iloc[:, :-1] = standard_sc.fit_transform(df.iloc[:, :-1])\n",
    "\n",
    "# Calculate the number of features and classes\n",
    "n_features = len(df.iloc[:, :-1].columns)\n",
    "n_classes = len(df.iloc[:, -1:].columns)\n",
    "\n",
    "n_elements = len(df.columns)\n",
    "print(n_features, n_classes, n_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining the labeled and unlabeled dataset classes\n",
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        vals = torch.tensor(self.x[index], dtype=torch.float32)\n",
    "        label = torch.tensor(self.y[index], dtype=torch.float32)\n",
    "        return vals, label\n",
    "\n",
    "class UnlabelledDataset(Dataset):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        vals = torch.tensor(self.x[index], dtype=torch.float32)\n",
    "        return vals, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Setup and Model Configuration\n",
    "\n",
    "This section initializes the environment by setting a fixed random seed to ensure reproducibility of results. It imports necessary configurations and initializes model parameters with specific configurations. The model specified here is set to have no core model but uses a 'tabnet' encoder model for data processing, which is particularly tailored for structured or tabular data like credit card transactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabelled Dataset Shape: 142403, 31\n",
      "Labeled Trainset Shape: 142403, 30\n",
      "Labeled Testset Shape: 142404, 30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset into training and test sets for model evaluation\n",
    "df_train, df_test = train_test_split(df, test_size=0.5, shuffle=False)\n",
    "X_train, y_train = df_train.iloc[:, :-1].values, df_train.iloc[:, -1:].values\n",
    "X_test, y_test = df_test.iloc[:, :-1].values, df_test.iloc[:, -1:].values\n",
    "\n",
    "# Preparing the datasets for use in training and testing\n",
    "df_train_values = df_train.iloc[:, :].values  # Full training data for unsupervised tasks\n",
    "unlabelled_dataset = UnlabelledDataset(df_train_values)  # Renamed to reflect it includes the full train data\n",
    "\n",
    "# Labeled datasets for supervised learning tasks\n",
    "supervised_trainset = LabeledDataset(X_train, y_train)  # Corrected to include training data\n",
    "supervised_testset = LabeledDataset(X_test, y_test)     # Test set with proper labels\n",
    "\n",
    "# Printing the shapes of the datasets for verification\n",
    "print(f\"Unlabelled Dataset Shape: {len(unlabelled_dataset)}, {unlabelled_dataset.x.shape[1]}\")\n",
    "print(f\"Labeled Trainset Shape: {len(supervised_trainset)}, {supervised_trainset.x.shape[1]}\")\n",
    "print(f\"Labeled Testset Shape: {len(supervised_testset)}, {supervised_testset.x.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CCNets-team\\anaconda3\\lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility of experiments\n",
    "from nn.utils.init import set_random_seed\n",
    "set_random_seed(0)\n",
    "\n",
    "# Importing configuration setups for ML parameters and data\n",
    "import torch\n",
    "from tools.setting.ml_params import MLParameters\n",
    "from tools.setting.data_config import DataConfig\n",
    "from trainer_hub import TrainerHub\n",
    "\n",
    "# Configuration for the data handling, defining dataset specifics and the task type\n",
    "data_config = DataConfig(dataset_name='CreditCardFraudDetection', task_type='generation', obs_shape=[n_elements], label_size=None)\n",
    "\n",
    "# Initializing ML parameters without a core model and setting the encoder model to 'tabnet' with specific configurations\n",
    "ml_params = MLParameters(ccnet_network='none', encoder_network='tabnet')\n",
    "\n",
    "# Setting training parameters and device configuration\n",
    "ml_params.training.num_epoch = 4\n",
    "ml_params.model.encoder_config.num_layers = 4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Create a TrainerHub instance to manage training and data processing\n",
    "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Splitting for Training and Testing\n",
    "\n",
    "The original dataset is split into training and testing parts to evaluate the model's performance accurately. This step is crucial for validating the effectiveness of the training on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50870488bc6411281db2c8eb0a682ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30ae11793084767bbdb292d07b5509c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/2225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/4][50/2225][Time 7.96]\n",
      "Unified LR across all optimizers: 0.0001995308238189185\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(encoder):  Three Tabnet\n",
      "Inf: 0.7856\tGen: 0.8684\tRec: 0.8101\tE: 0.8439\tR: 0.7274\tP: 0.8928\n",
      "[0/4][100/2225][Time 7.19]\n",
      "Unified LR across all optimizers: 0.00019907191565870155\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(encoder):  Three Tabnet\n",
      "Inf: 0.5893\tGen: 0.6982\tRec: 0.4991\tE: 0.7883\tR: 0.3902\tP: 0.6080\n",
      "[0/4][150/2225][Time 7.25]\n",
      "Unified LR across all optimizers: 0.00019861406295796434\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(encoder):  Three Tabnet\n",
      "Inf: 0.5261\tGen: 0.6425\tRec: 0.4214\tE: 0.7473\tR: 0.3050\tP: 0.5378\n",
      "[0/4][200/2225][Time 7.09]\n",
      "Unified LR across all optimizers: 0.00019815726328921765\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(encoder):  Three Tabnet\n",
      "Inf: 0.4783\tGen: 0.5957\tRec: 0.3668\tE: 0.7072\tR: 0.2494\tP: 0.4841\n",
      "[0/4][250/2225][Time 7.13]\n",
      "Unified LR across all optimizers: 0.00019770151423055492\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(encoder):  Three Tabnet\n",
      "Inf: 0.4412\tGen: 0.5593\tRec: 0.3315\tE: 0.6689\tR: 0.2135\tP: 0.4496\n",
      "[0/4][300/2225][Time 7.15]\n",
      "Unified LR across all optimizers: 0.00019724681336564005\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(encoder):  Three Tabnet\n",
      "Inf: 0.4230\tGen: 0.5459\tRec: 0.3210\tE: 0.6479\tR: 0.1980\tP: 0.4440\n",
      "[0/4][350/2225][Time 7.23]\n",
      "Unified LR across all optimizers: 0.00019679315828369438\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(encoder):  Three Tabnet\n",
      "Inf: 0.3879\tGen: 0.5068\tRec: 0.2856\tE: 0.6091\tR: 0.1667\tP: 0.4045\n"
     ]
    }
   ],
   "source": [
    "trainer_hub.train(unlabelled_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading and Synthetic Data Generation\n",
    "\n",
    "This section deals with loading the unlabelled dataset, processing it through the trained model to create synthetic data. This data augmentation step is crucial for models that benefit from larger datasets, such as in fraud detection scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreated Dataset Shape: torch.Size([142400, 31])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tools.loader import collate_fn\n",
    "batch_size = 64\n",
    "len_unlabelled_trainset = len(unlabelled_dataset)\n",
    "unlabelled_train_loader = torch.utils.data.DataLoader(dataset=unlabelled_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "encoder_ccnet = trainer_hub.encoder\n",
    "# Initialize the recreated dataset container\n",
    "recreated_dataset = None\n",
    "\n",
    "# Generate synthetic data through the model to augment the training dataset\n",
    "for data, _ in unlabelled_train_loader:\n",
    "    data = data.to(device)\n",
    "    # Generate a large amount of synthetic data\n",
    "    batch_recreated_data = encoder_ccnet.synthesize(data, output_multiplier = 10)\n",
    "\n",
    "    # Filter out entries where the last feature is not in the range (0, 1)\n",
    "    valid_entries = (batch_recreated_data[:, -1] >= 0) & (batch_recreated_data[:, -1] <= 1)\n",
    "    batch_recreated_data = batch_recreated_data[valid_entries]\n",
    "\n",
    "    # Check if any valid data remains after filtering; if none, skip this batch\n",
    "    if batch_recreated_data.shape[0] == 0:\n",
    "        continue\n",
    "    \n",
    "    # Sort the filtered data by the last feature (class label)\n",
    "    sorted_batch = batch_recreated_data[torch.argsort(batch_recreated_data[:, -1])]\n",
    "\n",
    "    # Select front half and rear half from the sorted batch\n",
    "    # Since you want exactly batch_size items, take the first and last quarter\n",
    "    select_size = batch_size // 2  # Selecting half of the batch size for each tail\n",
    "    balanced_batch = torch.cat([sorted_batch[:select_size], sorted_batch[-select_size:]], dim=0)\n",
    "\n",
    "    # Shuffle the selected batch\n",
    "    indices = torch.randperm(len(balanced_batch))\n",
    "    balanced_batch = balanced_batch[indices]\n",
    "\n",
    "    # Concatenate the processed batch to the main dataset\n",
    "    if recreated_dataset is None:\n",
    "        recreated_dataset = balanced_batch\n",
    "    else:\n",
    "        recreated_dataset = torch.cat([recreated_dataset, balanced_batch], dim=0)\n",
    "    \n",
    "    # Ensure we do not exceed the original unlabelled dataset's size\n",
    "    if len(recreated_dataset) >= len_unlabelled_trainset:\n",
    "        recreated_dataset = recreated_dataset[:len_unlabelled_trainset]\n",
    "        break\n",
    "\n",
    "print(f\"Recreated Dataset Shape: {recreated_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation for Model Training\n",
    "\n",
    "After synthetic data generation, this section separates the data and labels for training purposes, preparing them for use in machine learning models to ensure proper supervision and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled Original Trainset Shape: 142403, 30\n",
      "CCNet Recreated Dataset Shape: 142400, 30\n",
      "Labeled Original Testset Shape: 142404, 30\n",
      "Number of Features: 30\n",
      "Number of Classes: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming recreated_dataset is a PyTorch tensor already available in your context\n",
    "\n",
    "# Separate the recreated data into features and labels for training\n",
    "recreated_training_data = recreated_dataset[:, :-1].detach().cpu().numpy()  # Features from recreated data\n",
    "recreated_labels = recreated_dataset[:, -1:].detach().cpu().numpy()  # Labels from recreated data\n",
    "ccnet_recreated_dataset = LabeledDataset(recreated_training_data, recreated_labels)\n",
    "\n",
    "# Print the shapes of the datasets for verification\n",
    "print(f\"Labeled Original Trainset Shape: {len(supervised_trainset)}, {supervised_trainset.x.shape[1]}\")\n",
    "print(f\"CCNet Recreated Dataset Shape: {len(ccnet_recreated_dataset)}, {ccnet_recreated_dataset.x.shape[1]}\")\n",
    "\n",
    "print(f\"Labeled Original Testset Shape: {len(supervised_testset)}, {supervised_testset.x.shape[1]}\")\n",
    "\n",
    "# Retrieve number of features and classes from the recreated dataset\n",
    "num_features = recreated_training_data.shape[1]\n",
    "num_classes = recreated_labels.shape[1]\n",
    "\n",
    "print(f\"Number of Features: {num_features}\")\n",
    "print(f\"Number of Classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers=4, hidden_size=256):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Create a list to hold all layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(torch.nn.Linear(input_size, hidden_size))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(torch.nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(torch.nn.Linear(hidden_size, output_size))\n",
    "        \n",
    "        # Register all layers\n",
    "        self.layers = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Supervised Models\n",
    "\n",
    "This section outlines the process of training supervised learning models using both original and synthetic datasets. The `train_supervised_model` function is designed to iterate through the dataset, perform forward passes, compute loss, and update model weights using backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised_model(model, dataset, num_epoch=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    # Initialize the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # Ensure reproducibility by resetting the random seed\n",
    "    # Create DataLoader for batch processing\n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    # Training loop\n",
    "    for epoch in range(num_epoch):  # Train for 2 epochs as an example\n",
    "        for i, (data, label) in enumerate(trainloader):\n",
    "            data = data.to(device).clone().detach()\n",
    "            label = label.to(device).float()\n",
    "            # Perform forward pass\n",
    "            output = model(data)\n",
    "            # Compute loss\n",
    "            loss = torch.nn.functional.binary_cross_entropy(output, label)\n",
    "            # Backward pass to compute gradients\n",
    "            loss.backward()\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training Using Recreated and Original Datasets\n",
    "\n",
    "Models are trained using both datasets generated through the Data Augmentation process and the original dataset. This comparison helps to determine the effectiveness of the synthetic data in improving model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train a model on the recreated dataset\n",
    "model_trained_on_recreated = MLP(input_size=num_features, output_size=num_classes).to(device)\n",
    "train_supervised_model(model_trained_on_recreated, ccnet_recreated_dataset)\n",
    "\n",
    "# Initialize and train a model on the original dataset\n",
    "model_trained_on_original = MLP(input_size=num_features, output_size=num_classes).to(device)\n",
    "train_supervised_model(model_trained_on_original, supervised_trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Model Performance\n",
    "\n",
    "After training, the models are evaluated using the F1 score, a harmonic mean of precision and recall, which is particularly useful in the context of imbalanced datasets like fraud detection. This step is critical for assessing the quality of the models trained on different types of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the supervised learning model trained on the original data:  0.697495183044316\n",
      "F1 score of the supervised learning model trained on the recreated data:  0.7989690721649485\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_f1_score(model, testset, batch_size=256):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    # DataLoader for testing\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # No gradient computation needed during inference\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(data)\n",
    "            # Process output for binary classification\n",
    "            predicted = (output.squeeze() > 0.5).long()\n",
    "            y_true.extend(label.squeeze().long().cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Compute and return the F1 score\n",
    "    score = f1_score(y_true, y_pred, average='binary')\n",
    "    return score\n",
    "\n",
    "# Calculate F1 scores for both models\n",
    "f1_score_original = get_f1_score(model_trained_on_original, supervised_testset)\n",
    "f1_score_recreated = get_f1_score(model_trained_on_recreated, supervised_testset)\n",
    "\n",
    "# Output the results\n",
    "print(\"F1 score of the supervised learning model trained on the original data: \", f1_score_original)\n",
    "print(\"F1 score of the supervised learning model trained on the recreated data: \", f1_score_recreated)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
