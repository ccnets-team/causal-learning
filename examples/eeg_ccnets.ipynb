{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author:\n",
    "        \n",
    "        PARK, JunHo, junho@ccnets.org\n",
    "\n",
    "        \n",
    "        KIM, JeongYoong, jeongyoong@ccnets.org\n",
    "        \n",
    "    COPYRIGHT (c) 2024. CCNets. All Rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path_append = \"../\"\n",
    "sys.path.append(path_append)  # Go up one directory from where you are.\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.setting.ml_params import MLParameters\n",
    "from tools.setting.data_config import DataConfig\n",
    "from nn.utils.init import set_random_seed\n",
    "set_random_seed(0)\n",
    "\n",
    "from trainer_hub import TrainerHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>...</th>\n",
       "      <th>D24</th>\n",
       "      <th>D25</th>\n",
       "      <th>D26</th>\n",
       "      <th>D27</th>\n",
       "      <th>D28</th>\n",
       "      <th>D29</th>\n",
       "      <th>D30</th>\n",
       "      <th>D31</th>\n",
       "      <th>D32</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3549.790315</td>\n",
       "      <td>4533.538497</td>\n",
       "      <td>3619.665186</td>\n",
       "      <td>3077.291188</td>\n",
       "      <td>-1380.325575</td>\n",
       "      <td>6120.066816</td>\n",
       "      <td>-4072.820600</td>\n",
       "      <td>-2256.511456</td>\n",
       "      <td>1820.012261</td>\n",
       "      <td>-2815.635423</td>\n",
       "      <td>...</td>\n",
       "      <td>-7240.845997</td>\n",
       "      <td>7034.252627</td>\n",
       "      <td>8458.062496</td>\n",
       "      <td>5905.223463</td>\n",
       "      <td>6147.660515</td>\n",
       "      <td>2458.073582</td>\n",
       "      <td>-7465.876831</td>\n",
       "      <td>-3604.133966</td>\n",
       "      <td>-5445.224315</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3551.227812</td>\n",
       "      <td>4534.850995</td>\n",
       "      <td>3622.540181</td>\n",
       "      <td>3077.322438</td>\n",
       "      <td>-1377.575581</td>\n",
       "      <td>6123.066810</td>\n",
       "      <td>-4069.851856</td>\n",
       "      <td>-2252.167714</td>\n",
       "      <td>1825.168502</td>\n",
       "      <td>-2803.072947</td>\n",
       "      <td>...</td>\n",
       "      <td>-7227.283522</td>\n",
       "      <td>7039.627617</td>\n",
       "      <td>8463.874985</td>\n",
       "      <td>5911.598451</td>\n",
       "      <td>6153.504254</td>\n",
       "      <td>2463.354822</td>\n",
       "      <td>-7461.033090</td>\n",
       "      <td>-3594.258985</td>\n",
       "      <td>-5435.693082</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3556.727802</td>\n",
       "      <td>4539.850986</td>\n",
       "      <td>3629.040169</td>\n",
       "      <td>3081.978679</td>\n",
       "      <td>-1370.419344</td>\n",
       "      <td>6130.348047</td>\n",
       "      <td>-4063.508118</td>\n",
       "      <td>-2249.292720</td>\n",
       "      <td>1828.074746</td>\n",
       "      <td>-2804.041695</td>\n",
       "      <td>...</td>\n",
       "      <td>-7227.158522</td>\n",
       "      <td>7048.502600</td>\n",
       "      <td>8473.562467</td>\n",
       "      <td>5921.348433</td>\n",
       "      <td>6163.004236</td>\n",
       "      <td>2469.854810</td>\n",
       "      <td>-7460.470591</td>\n",
       "      <td>-3591.540240</td>\n",
       "      <td>-5433.568086</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3557.915300</td>\n",
       "      <td>4541.225983</td>\n",
       "      <td>3628.540169</td>\n",
       "      <td>3083.197427</td>\n",
       "      <td>-1372.263090</td>\n",
       "      <td>6130.410547</td>\n",
       "      <td>-4062.070620</td>\n",
       "      <td>-2251.667715</td>\n",
       "      <td>1825.856000</td>\n",
       "      <td>-2803.572946</td>\n",
       "      <td>...</td>\n",
       "      <td>-7224.189777</td>\n",
       "      <td>7042.346362</td>\n",
       "      <td>8464.593734</td>\n",
       "      <td>5917.660940</td>\n",
       "      <td>6160.972990</td>\n",
       "      <td>2467.011066</td>\n",
       "      <td>-7458.158095</td>\n",
       "      <td>-3597.008980</td>\n",
       "      <td>-5437.474329</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3553.352808</td>\n",
       "      <td>4535.757243</td>\n",
       "      <td>3622.477681</td>\n",
       "      <td>3079.572434</td>\n",
       "      <td>-1377.763080</td>\n",
       "      <td>6125.598056</td>\n",
       "      <td>-4066.570612</td>\n",
       "      <td>-2255.136459</td>\n",
       "      <td>1821.981008</td>\n",
       "      <td>-2808.041687</td>\n",
       "      <td>...</td>\n",
       "      <td>-7219.971035</td>\n",
       "      <td>7044.658857</td>\n",
       "      <td>8466.843729</td>\n",
       "      <td>5914.848445</td>\n",
       "      <td>6156.785498</td>\n",
       "      <td>2466.948566</td>\n",
       "      <td>-7457.501846</td>\n",
       "      <td>-3585.821500</td>\n",
       "      <td>-5428.630595</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588018</th>\n",
       "      <td>-623.326974</td>\n",
       "      <td>2269.261431</td>\n",
       "      <td>2575.479615</td>\n",
       "      <td>285.733846</td>\n",
       "      <td>907.388947</td>\n",
       "      <td>-491.014719</td>\n",
       "      <td>-2998.447586</td>\n",
       "      <td>1886.043389</td>\n",
       "      <td>1659.637557</td>\n",
       "      <td>416.296105</td>\n",
       "      <td>...</td>\n",
       "      <td>-7176.689865</td>\n",
       "      <td>2116.667963</td>\n",
       "      <td>-901.138961</td>\n",
       "      <td>-227.327706</td>\n",
       "      <td>-657.170662</td>\n",
       "      <td>3025.322534</td>\n",
       "      <td>-12313.149124</td>\n",
       "      <td>-3810.071086</td>\n",
       "      <td>-5620.505241</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588019</th>\n",
       "      <td>-627.420717</td>\n",
       "      <td>2264.448940</td>\n",
       "      <td>2570.323375</td>\n",
       "      <td>281.077605</td>\n",
       "      <td>903.482705</td>\n",
       "      <td>-490.702219</td>\n",
       "      <td>-3001.260080</td>\n",
       "      <td>1884.387142</td>\n",
       "      <td>1657.012562</td>\n",
       "      <td>414.702358</td>\n",
       "      <td>...</td>\n",
       "      <td>-7179.502360</td>\n",
       "      <td>2118.074210</td>\n",
       "      <td>-900.607712</td>\n",
       "      <td>-227.046456</td>\n",
       "      <td>-659.389408</td>\n",
       "      <td>3027.760030</td>\n",
       "      <td>-12307.211635</td>\n",
       "      <td>-3809.946086</td>\n",
       "      <td>-5621.098990</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588020</th>\n",
       "      <td>-631.764459</td>\n",
       "      <td>2260.730197</td>\n",
       "      <td>2566.917131</td>\n",
       "      <td>275.546365</td>\n",
       "      <td>902.045207</td>\n",
       "      <td>-493.545964</td>\n",
       "      <td>-3006.103821</td>\n",
       "      <td>1886.199639</td>\n",
       "      <td>1658.512560</td>\n",
       "      <td>424.202340</td>\n",
       "      <td>...</td>\n",
       "      <td>-7177.439864</td>\n",
       "      <td>2118.199210</td>\n",
       "      <td>-900.920211</td>\n",
       "      <td>-226.140208</td>\n",
       "      <td>-659.764407</td>\n",
       "      <td>3027.103781</td>\n",
       "      <td>-12305.774138</td>\n",
       "      <td>-3805.633594</td>\n",
       "      <td>-5614.880251</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588021</th>\n",
       "      <td>-625.076971</td>\n",
       "      <td>2265.605188</td>\n",
       "      <td>2573.354619</td>\n",
       "      <td>281.702604</td>\n",
       "      <td>904.982702</td>\n",
       "      <td>-490.795969</td>\n",
       "      <td>-3001.416330</td>\n",
       "      <td>1888.387135</td>\n",
       "      <td>1659.418808</td>\n",
       "      <td>420.077348</td>\n",
       "      <td>...</td>\n",
       "      <td>-7172.002374</td>\n",
       "      <td>2119.730457</td>\n",
       "      <td>-898.170216</td>\n",
       "      <td>-224.515211</td>\n",
       "      <td>-656.576913</td>\n",
       "      <td>3032.822520</td>\n",
       "      <td>-12303.742892</td>\n",
       "      <td>-3804.133597</td>\n",
       "      <td>-5614.192752</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588022</th>\n",
       "      <td>-623.639474</td>\n",
       "      <td>2268.636432</td>\n",
       "      <td>2576.229614</td>\n",
       "      <td>286.515095</td>\n",
       "      <td>909.795193</td>\n",
       "      <td>-484.358481</td>\n",
       "      <td>-2996.353839</td>\n",
       "      <td>1895.730871</td>\n",
       "      <td>1668.950040</td>\n",
       "      <td>431.983576</td>\n",
       "      <td>...</td>\n",
       "      <td>-7171.908624</td>\n",
       "      <td>2129.230440</td>\n",
       "      <td>-889.357733</td>\n",
       "      <td>-216.577726</td>\n",
       "      <td>-649.358176</td>\n",
       "      <td>3039.572508</td>\n",
       "      <td>-12297.899153</td>\n",
       "      <td>-3793.383617</td>\n",
       "      <td>-5603.130273</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>588023 rows Ã— 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 A1           A2           A3           A4           A5   \n",
       "0       3549.790315  4533.538497  3619.665186  3077.291188 -1380.325575  \\\n",
       "1       3551.227812  4534.850995  3622.540181  3077.322438 -1377.575581   \n",
       "2       3556.727802  4539.850986  3629.040169  3081.978679 -1370.419344   \n",
       "3       3557.915300  4541.225983  3628.540169  3083.197427 -1372.263090   \n",
       "4       3553.352808  4535.757243  3622.477681  3079.572434 -1377.763080   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "588018  -623.326974  2269.261431  2575.479615   285.733846   907.388947   \n",
       "588019  -627.420717  2264.448940  2570.323375   281.077605   903.482705   \n",
       "588020  -631.764459  2260.730197  2566.917131   275.546365   902.045207   \n",
       "588021  -625.076971  2265.605188  2573.354619   281.702604   904.982702   \n",
       "588022  -623.639474  2268.636432  2576.229614   286.515095   909.795193   \n",
       "\n",
       "                 A6           A7           A8           A9          A10  ...   \n",
       "0       6120.066816 -4072.820600 -2256.511456  1820.012261 -2815.635423  ...  \\\n",
       "1       6123.066810 -4069.851856 -2252.167714  1825.168502 -2803.072947  ...   \n",
       "2       6130.348047 -4063.508118 -2249.292720  1828.074746 -2804.041695  ...   \n",
       "3       6130.410547 -4062.070620 -2251.667715  1825.856000 -2803.572946  ...   \n",
       "4       6125.598056 -4066.570612 -2255.136459  1821.981008 -2808.041687  ...   \n",
       "...             ...          ...          ...          ...          ...  ...   \n",
       "588018  -491.014719 -2998.447586  1886.043389  1659.637557   416.296105  ...   \n",
       "588019  -490.702219 -3001.260080  1884.387142  1657.012562   414.702358  ...   \n",
       "588020  -493.545964 -3006.103821  1886.199639  1658.512560   424.202340  ...   \n",
       "588021  -490.795969 -3001.416330  1888.387135  1659.418808   420.077348  ...   \n",
       "588022  -484.358481 -2996.353839  1895.730871  1668.950040   431.983576  ...   \n",
       "\n",
       "                D24          D25          D26          D27          D28   \n",
       "0      -7240.845997  7034.252627  8458.062496  5905.223463  6147.660515  \\\n",
       "1      -7227.283522  7039.627617  8463.874985  5911.598451  6153.504254   \n",
       "2      -7227.158522  7048.502600  8473.562467  5921.348433  6163.004236   \n",
       "3      -7224.189777  7042.346362  8464.593734  5917.660940  6160.972990   \n",
       "4      -7219.971035  7044.658857  8466.843729  5914.848445  6156.785498   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "588018 -7176.689865  2116.667963  -901.138961  -227.327706  -657.170662   \n",
       "588019 -7179.502360  2118.074210  -900.607712  -227.046456  -659.389408   \n",
       "588020 -7177.439864  2118.199210  -900.920211  -226.140208  -659.764407   \n",
       "588021 -7172.002374  2119.730457  -898.170216  -224.515211  -656.576913   \n",
       "588022 -7171.908624  2129.230440  -889.357733  -216.577726  -649.358176   \n",
       "\n",
       "                D29           D30          D31          D32  event  \n",
       "0       2458.073582  -7465.876831 -3604.133966 -5445.224315      5  \n",
       "1       2463.354822  -7461.033090 -3594.258985 -5435.693082      5  \n",
       "2       2469.854810  -7460.470591 -3591.540240 -5433.568086      5  \n",
       "3       2467.011066  -7458.158095 -3597.008980 -5437.474329      5  \n",
       "4       2466.948566  -7457.501846 -3585.821500 -5428.630595      5  \n",
       "...             ...           ...          ...          ...    ...  \n",
       "588018  3025.322534 -12313.149124 -3810.071086 -5620.505241     10  \n",
       "588019  3027.760030 -12307.211635 -3809.946086 -5621.098990     10  \n",
       "588020  3027.103781 -12305.774138 -3805.633594 -5614.880251     10  \n",
       "588021  3032.822520 -12303.742892 -3804.133597 -5614.192752     10  \n",
       "588022  3039.572508 -12297.899153 -3793.383617 -5603.130273     10  \n",
       "\n",
       "[588023 rows x 129 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/N-Nieto/Inner_Speech_Dataset\n",
    "\n",
    "# Load the Inner Speech Dataset\n",
    "# =============================\n",
    "# This dataset comprises raw EEG data collected from subject 'sub-01' during session 'ses-01'.\n",
    "# Source: https://github.com/N-Nieto/Inner_Speech_Dataset\n",
    "#\n",
    "# Overview:\n",
    "# - The dataset is part of a study on inner speech, capturing brain activity via EEG.\n",
    "# - Each row in the dataset corresponds to a timestamp of EEG readings.\n",
    "# - Columns represent various EEG channels (electrodes placed on the scalp).\n",
    "#\n",
    "# Usage:\n",
    "# - The data is primarily used for cognitive neuroscience research, focusing on the neural correlates of inner speech.\n",
    "# - Users can analyze EEG signals to investigate brain activity patterns associated with the cognitive processes of inner speech.\n",
    "#\n",
    "# File Structure:\n",
    "# - Located at '../data/RAW_EEG/sub-01/sub-01_ses-01.csv' relative to this script.\n",
    "# - It is advisable to preprocess the data (filtering, normalization) before detailed analysis.\n",
    "#\n",
    "# Example:\n",
    "# - To load this data into a DataFrame for analysis and processing, use the following code snippet.\n",
    "\n",
    "\n",
    "df = None\n",
    "for csv in [\"../data/RAW_EEG/sub-01/sub-01_ses-01.csv\", \"../data/RAW_EEG/sub-01/sub-01_ses-02.csv\", \"../data/RAW_EEG/sub-01/sub-01_ses-03.csv\"]:\n",
    "    tmp_df = pd.read_csv(path_append + csv)\n",
    "    if df is None:\n",
    "        df = tmp_df\n",
    "    else:\n",
    "        df = pd.concat([df, tmp_df])\n",
    "df = df.reset_index(drop=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of each class in the 'event' column:\n",
      "event\n",
      "1     57650\n",
      "0     57650\n",
      "3     57650\n",
      "2     57650\n",
      "13    57650\n",
      "12    57650\n",
      "11    57650\n",
      "10    57650\n",
      "6     28825\n",
      "9     28825\n",
      "8     28825\n",
      "7     28825\n",
      "5     11523\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Maximum class number:\n",
      "13\n",
      "\n",
      "Expected number of classes (from num_classes variable): 14\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Example setup, assuming df and mm are defined as DataFrame and RobustScaler respectively\n",
    "\n",
    "# Assuming df['event'] contains the class labels\n",
    "event_counts = df['event'].value_counts()\n",
    "max_class_number = df['event'].max()\n",
    "\n",
    "# Print each number of classes\n",
    "print(\"Counts of each class in the 'event' column:\")\n",
    "print(event_counts)\n",
    "\n",
    "# Print the maximum class number\n",
    "print(\"\\nMaximum class number:\")\n",
    "print(max_class_number)\n",
    "\n",
    "num_classes = max_class_number + 1\n",
    "# Additionally, verify against the num_classes variable\n",
    "print(\"\\nExpected number of classes (from num_classes variable):\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices where the 'event' label changes:\n",
      "[0, 3841, 4994, 6147, 8453, 9606, 10759, 11912, 13065, 14218, 15371, 17677, 18830, 19983, 21136, 23442, 24595, 25748, 26901, 28054, 29207, 30360, 31513, 33819, 34972, 36125, 38431, 39584, 40737, 41890, 45349, 46502, 48808, 49961, 52267, 55726, 56879, 58032, 60338, 61491, 62644, 63797, 66103, 67256, 68409, 69562, 70715, 71868, 73021, 74174, 75327, 77633, 78786, 81092, 82245, 83398, 85704, 86857, 88010, 89163, 90316, 91469, 92622, 94928, 96081, 98387, 101846, 102999, 104152, 106458, 107611, 108764, 109917, 112223, 113376, 114529, 115682, 116835, 117988, 119141, 120294, 121447, 123753, 124906, 127212, 128365, 129518, 131824, 132977, 134130, 135283, 136436, 137589, 138742, 141048, 142201, 143354, 144507, 146813, 149119, 150272, 151425, 152578, 153731, 157190, 158343, 160649, 161802, 162955, 164108, 165261, 166414, 168720, 169873, 172179, 173332, 175638, 176791, 179097, 180250, 181403, 182556, 183709, 184862, 186015, 187168, 188321, 190627, 192933, 194086, 195239, 196392, 197545, 198698, 199851, 202157, 203310, 204463, 205616, 207922, 209075, 210228, 211381, 214840, 215993, 217146, 218299, 219452, 220605, 221758, 222911, 224064, 225217, 226370, 228676, 229829, 230982, 232135, 233288, 234441, 238282, 239435, 240588, 241741, 245200, 246353, 248659, 250965, 252118, 253271, 254424, 255577, 256730, 257883, 259036, 260189, 261342, 264801, 265954, 268260, 269413, 270566, 271719, 274025, 275178, 276331, 277484, 278637, 279790, 280943, 282096, 284402, 286708, 289014, 290167, 292473, 293626, 295932, 297085, 298238, 299391, 300544, 301697, 305156, 306309, 309768, 310921, 312074, 313227, 314380, 315533, 316686, 317839, 318992, 320145, 322451, 323604, 325910, 327063, 328216, 330522, 332828, 335134, 336287, 338593, 339746, 342052, 343205, 344358, 345511, 346664, 347817, 351276, 352429, 355888, 357041, 358194, 359347, 360500, 361653, 362806, 363959, 365112, 366265, 368571, 369724, 372030, 373183, 374336, 376642, 377795, 378948, 380101, 381254, 382407, 383560, 384713, 385866, 387019, 388172, 389325, 390478, 391631, 392784, 393937, 395090, 396243, 398549, 399702, 402008, 403161, 404314, 405467, 406620, 407773, 408926, 411232, 412385, 414691, 415844, 416997, 418150, 419303, 420456, 421609, 422762, 423915, 425068, 426221, 427374, 428527, 429680, 430833, 433139, 434292, 435445, 436598, 437751, 438904, 441210, 442363, 443516, 444669, 445822, 446975, 448128, 449281, 450434, 452740, 453893, 455046, 456199, 457352, 458505, 459658, 460811, 461964, 464270, 466576, 467729, 468882, 472723, 473876, 475029, 476182, 479641, 480794, 481947, 484253, 485406, 486559, 487712, 488865, 491171, 492324, 493477, 494630, 495783, 498089, 499242, 500395, 502701, 505007, 506160, 507313, 508466, 509619, 510772, 511925, 516537, 518843, 521149, 522302, 523455, 525761, 528067, 529220, 530373, 531526, 532679, 533832, 534985, 539597, 541903, 543056, 544209, 545362, 546515, 549974, 551127, 552280, 553433, 554586, 555739, 556892, 558045, 559198, 560351, 561504, 562657, 563810, 564963, 568422, 569575, 570728, 571881, 574187, 575340, 576493, 577646, 578799, 579952, 581105, 582258, 583411, 584564, 585717]\n",
      "\n",
      "Lengths between changes:\n",
      "[3841, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 2306, 1153, 1153, 1153, 3459, 1153, 2306, 1153, 2306, 3459, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 3459, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 3459, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 3459, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 3841, 1153, 1153, 1153, 3459, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 3459, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 2306, 2306, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 3459, 1153, 3459, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 2306, 2306, 2306, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 3459, 1153, 3459, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 2306, 1153, 1153, 3841, 1153, 1153, 1153, 3459, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 4612, 2306, 2306, 1153, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 4612, 2306, 1153, 1153, 1153, 1153, 3459, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 3459, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153]\n",
      "\n",
      "Details of label changes at these indices:\n",
      "Initial event starts at index 0 with value 5\n",
      "Change from 5 to 6 at index 3841\n",
      "Change from 6 to 9 at index 4994\n",
      "Change from 9 to 6 at index 6147\n",
      "Change from 6 to 8 at index 8453\n",
      "Change from 8 to 7 at index 9606\n",
      "Change from 7 to 6 at index 10759\n",
      "Change from 6 to 7 at index 11912\n",
      "Change from 7 to 6 at index 13065\n",
      "Change from 6 to 7 at index 14218\n",
      "Change from 7 to 9 at index 15371\n",
      "Change from 9 to 6 at index 17677\n",
      "Change from 6 to 8 at index 18830\n",
      "Change from 8 to 7 at index 19983\n",
      "Change from 7 to 9 at index 21136\n",
      "Change from 9 to 6 at index 23442\n",
      "Change from 6 to 7 at index 24595\n",
      "Change from 7 to 6 at index 25748\n",
      "Change from 6 to 7 at index 26901\n",
      "Change from 7 to 8 at index 28054\n",
      "Change from 8 to 7 at index 29207\n",
      "Change from 7 to 9 at index 30360\n",
      "Change from 9 to 8 at index 31513\n",
      "Change from 8 to 7 at index 33819\n",
      "Change from 7 to 9 at index 34972\n",
      "Change from 9 to 8 at index 36125\n",
      "Change from 8 to 6 at index 38431\n",
      "Change from 6 to 9 at index 39584\n",
      "Change from 9 to 7 at index 40737\n",
      "Change from 7 to 8 at index 41890\n",
      "Change from 8 to 6 at index 45349\n",
      "Change from 6 to 9 at index 46502\n",
      "Change from 9 to 7 at index 48808\n",
      "Change from 7 to 1 at index 49961\n",
      "Change from 1 to 0 at index 52267\n",
      "Change from 0 to 3 at index 55726\n",
      "Change from 3 to 2 at index 56879\n",
      "Change from 2 to 0 at index 58032\n",
      "Change from 0 to 1 at index 60338\n",
      "Change from 1 to 3 at index 61491\n",
      "Change from 3 to 1 at index 62644\n",
      "Change from 1 to 0 at index 63797\n",
      "Change from 0 to 1 at index 66103\n",
      "Change from 1 to 2 at index 67256\n",
      "Change from 2 to 3 at index 68409\n",
      "Change from 3 to 2 at index 69562\n",
      "Change from 2 to 3 at index 70715\n",
      "Change from 3 to 0 at index 71868\n",
      "Change from 0 to 1 at index 73021\n",
      "Change from 1 to 3 at index 74174\n",
      "Change from 3 to 1 at index 75327\n",
      "Change from 1 to 0 at index 77633\n",
      "Change from 0 to 2 at index 78786\n",
      "Change from 2 to 3 at index 81092\n",
      "Change from 3 to 1 at index 82245\n",
      "Change from 1 to 2 at index 83398\n",
      "Change from 2 to 3 at index 85704\n",
      "Change from 3 to 1 at index 86857\n",
      "Change from 1 to 2 at index 88010\n",
      "Change from 2 to 0 at index 89163\n",
      "Change from 0 to 3 at index 90316\n",
      "Change from 3 to 2 at index 91469\n",
      "Change from 2 to 3 at index 92622\n",
      "Change from 3 to 2 at index 94928\n",
      "Change from 2 to 1 at index 96081\n",
      "Change from 1 to 0 at index 98387\n",
      "Change from 0 to 3 at index 101846\n",
      "Change from 3 to 2 at index 102999\n",
      "Change from 2 to 0 at index 104152\n",
      "Change from 0 to 1 at index 106458\n",
      "Change from 1 to 3 at index 107611\n",
      "Change from 3 to 1 at index 108764\n",
      "Change from 1 to 0 at index 109917\n",
      "Change from 0 to 1 at index 112223\n",
      "Change from 1 to 2 at index 113376\n",
      "Change from 2 to 3 at index 114529\n",
      "Change from 3 to 2 at index 115682\n",
      "Change from 2 to 3 at index 116835\n",
      "Change from 3 to 0 at index 117988\n",
      "Change from 0 to 1 at index 119141\n",
      "Change from 1 to 3 at index 120294\n",
      "Change from 3 to 1 at index 121447\n",
      "Change from 1 to 0 at index 123753\n",
      "Change from 0 to 2 at index 124906\n",
      "Change from 2 to 3 at index 127212\n",
      "Change from 3 to 1 at index 128365\n",
      "Change from 1 to 2 at index 129518\n",
      "Change from 2 to 3 at index 131824\n",
      "Change from 3 to 1 at index 132977\n",
      "Change from 1 to 2 at index 134130\n",
      "Change from 2 to 0 at index 135283\n",
      "Change from 0 to 3 at index 136436\n",
      "Change from 3 to 2 at index 137589\n",
      "Change from 2 to 3 at index 138742\n",
      "Change from 3 to 2 at index 141048\n",
      "Change from 2 to 13 at index 142201\n",
      "Change from 13 to 12 at index 143354\n",
      "Change from 12 to 11 at index 144507\n",
      "Change from 11 to 13 at index 146813\n",
      "Change from 13 to 12 at index 149119\n",
      "Change from 12 to 11 at index 150272\n",
      "Change from 11 to 13 at index 151425\n",
      "Change from 13 to 10 at index 152578\n",
      "Change from 10 to 12 at index 153731\n",
      "Change from 12 to 10 at index 157190\n",
      "Change from 10 to 13 at index 158343\n",
      "Change from 13 to 11 at index 160649\n",
      "Change from 11 to 12 at index 161802\n",
      "Change from 12 to 10 at index 162955\n",
      "Change from 10 to 13 at index 164108\n",
      "Change from 13 to 10 at index 165261\n",
      "Change from 10 to 11 at index 166414\n",
      "Change from 11 to 10 at index 168720\n",
      "Change from 10 to 12 at index 169873\n",
      "Change from 12 to 11 at index 172179\n",
      "Change from 11 to 10 at index 173332\n",
      "Change from 10 to 13 at index 175638\n",
      "Change from 13 to 11 at index 176791\n",
      "Change from 11 to 10 at index 179097\n",
      "Change from 10 to 13 at index 180250\n",
      "Change from 13 to 12 at index 181403\n",
      "Change from 12 to 10 at index 182556\n",
      "Change from 10 to 12 at index 183709\n",
      "Change from 12 to 13 at index 184862\n",
      "Change from 13 to 11 at index 186015\n",
      "Change from 11 to 10 at index 187168\n",
      "Change from 10 to 12 at index 188321\n",
      "Change from 12 to 10 at index 190627\n",
      "Change from 10 to 12 at index 192933\n",
      "Change from 12 to 11 at index 194086\n",
      "Change from 11 to 13 at index 195239\n",
      "Change from 13 to 10 at index 196392\n",
      "Change from 10 to 13 at index 197545\n",
      "Change from 13 to 11 at index 198698\n",
      "Change from 11 to 10 at index 199851\n",
      "Change from 10 to 13 at index 202157\n",
      "Change from 13 to 10 at index 203310\n",
      "Change from 10 to 12 at index 204463\n",
      "Change from 12 to 10 at index 205616\n",
      "Change from 10 to 13 at index 207922\n",
      "Change from 13 to 10 at index 209075\n",
      "Change from 10 to 11 at index 210228\n",
      "Change from 11 to 12 at index 211381\n",
      "Change from 12 to 13 at index 214840\n",
      "Change from 13 to 11 at index 215993\n",
      "Change from 11 to 12 at index 217146\n",
      "Change from 12 to 11 at index 218299\n",
      "Change from 11 to 12 at index 219452\n",
      "Change from 12 to 11 at index 220605\n",
      "Change from 11 to 13 at index 221758\n",
      "Change from 13 to 11 at index 222911\n",
      "Change from 11 to 13 at index 224064\n",
      "Change from 13 to 11 at index 225217\n",
      "Change from 11 to 13 at index 226370\n",
      "Change from 13 to 12 at index 228676\n",
      "Change from 12 to 11 at index 229829\n",
      "Change from 11 to 10 at index 230982\n",
      "Change from 10 to 13 at index 232135\n",
      "Change from 13 to 11 at index 233288\n",
      "Change from 11 to 5 at index 234441\n",
      "Change from 5 to 9 at index 238282\n",
      "Change from 9 to 8 at index 239435\n",
      "Change from 8 to 9 at index 240588\n",
      "Change from 9 to 6 at index 241741\n",
      "Change from 6 to 8 at index 245200\n",
      "Change from 8 to 7 at index 246353\n",
      "Change from 7 to 9 at index 248659\n",
      "Change from 9 to 7 at index 250965\n",
      "Change from 7 to 8 at index 252118\n",
      "Change from 8 to 7 at index 253271\n",
      "Change from 7 to 6 at index 254424\n",
      "Change from 6 to 9 at index 255577\n",
      "Change from 9 to 7 at index 256730\n",
      "Change from 7 to 8 at index 257883\n",
      "Change from 8 to 7 at index 259036\n",
      "Change from 7 to 8 at index 260189\n",
      "Change from 8 to 6 at index 261342\n",
      "Change from 6 to 9 at index 264801\n",
      "Change from 9 to 8 at index 265954\n",
      "Change from 8 to 7 at index 268260\n",
      "Change from 7 to 8 at index 269413\n",
      "Change from 8 to 6 at index 270566\n",
      "Change from 6 to 9 at index 271719\n",
      "Change from 9 to 7 at index 274025\n",
      "Change from 7 to 8 at index 275178\n",
      "Change from 8 to 9 at index 276331\n",
      "Change from 9 to 7 at index 277484\n",
      "Change from 7 to 9 at index 278637\n",
      "Change from 9 to 8 at index 279790\n",
      "Change from 8 to 7 at index 280943\n",
      "Change from 7 to 6 at index 282096\n",
      "Change from 6 to 1 at index 284402\n",
      "Change from 1 to 2 at index 286708\n",
      "Change from 2 to 0 at index 289014\n",
      "Change from 0 to 3 at index 290167\n",
      "Change from 3 to 2 at index 292473\n",
      "Change from 2 to 1 at index 293626\n",
      "Change from 1 to 0 at index 295932\n",
      "Change from 0 to 1 at index 297085\n",
      "Change from 1 to 2 at index 298238\n",
      "Change from 2 to 3 at index 299391\n",
      "Change from 3 to 0 at index 300544\n",
      "Change from 0 to 3 at index 301697\n",
      "Change from 3 to 2 at index 305156\n",
      "Change from 2 to 1 at index 306309\n",
      "Change from 1 to 0 at index 309768\n",
      "Change from 0 to 2 at index 310921\n",
      "Change from 2 to 1 at index 312074\n",
      "Change from 1 to 2 at index 313227\n",
      "Change from 2 to 3 at index 314380\n",
      "Change from 3 to 1 at index 315533\n",
      "Change from 1 to 2 at index 316686\n",
      "Change from 2 to 0 at index 317839\n",
      "Change from 0 to 2 at index 318992\n",
      "Change from 2 to 0 at index 320145\n",
      "Change from 0 to 3 at index 322451\n",
      "Change from 3 to 0 at index 323604\n",
      "Change from 0 to 2 at index 325910\n",
      "Change from 2 to 0 at index 327063\n",
      "Change from 0 to 3 at index 328216\n",
      "Change from 3 to 1 at index 330522\n",
      "Change from 1 to 2 at index 332828\n",
      "Change from 2 to 0 at index 335134\n",
      "Change from 0 to 3 at index 336287\n",
      "Change from 3 to 2 at index 338593\n",
      "Change from 2 to 1 at index 339746\n",
      "Change from 1 to 0 at index 342052\n",
      "Change from 0 to 1 at index 343205\n",
      "Change from 1 to 2 at index 344358\n",
      "Change from 2 to 3 at index 345511\n",
      "Change from 3 to 0 at index 346664\n",
      "Change from 0 to 3 at index 347817\n",
      "Change from 3 to 2 at index 351276\n",
      "Change from 2 to 1 at index 352429\n",
      "Change from 1 to 0 at index 355888\n",
      "Change from 0 to 2 at index 357041\n",
      "Change from 2 to 1 at index 358194\n",
      "Change from 1 to 2 at index 359347\n",
      "Change from 2 to 3 at index 360500\n",
      "Change from 3 to 1 at index 361653\n",
      "Change from 1 to 2 at index 362806\n",
      "Change from 2 to 0 at index 363959\n",
      "Change from 0 to 2 at index 365112\n",
      "Change from 2 to 0 at index 366265\n",
      "Change from 0 to 3 at index 368571\n",
      "Change from 3 to 0 at index 369724\n",
      "Change from 0 to 2 at index 372030\n",
      "Change from 2 to 0 at index 373183\n",
      "Change from 0 to 3 at index 374336\n",
      "Change from 3 to 10 at index 376642\n",
      "Change from 10 to 13 at index 377795\n",
      "Change from 13 to 11 at index 378948\n",
      "Change from 11 to 13 at index 380101\n",
      "Change from 13 to 10 at index 381254\n",
      "Change from 10 to 12 at index 382407\n",
      "Change from 12 to 13 at index 383560\n",
      "Change from 13 to 10 at index 384713\n",
      "Change from 10 to 11 at index 385866\n",
      "Change from 11 to 10 at index 387019\n",
      "Change from 10 to 13 at index 388172\n",
      "Change from 13 to 12 at index 389325\n",
      "Change from 12 to 10 at index 390478\n",
      "Change from 10 to 12 at index 391631\n",
      "Change from 12 to 10 at index 392784\n",
      "Change from 10 to 12 at index 393937\n",
      "Change from 12 to 10 at index 395090\n",
      "Change from 10 to 12 at index 396243\n",
      "Change from 12 to 11 at index 398549\n",
      "Change from 11 to 13 at index 399702\n",
      "Change from 13 to 12 at index 402008\n",
      "Change from 12 to 10 at index 403161\n",
      "Change from 10 to 12 at index 404314\n",
      "Change from 12 to 13 at index 405467\n",
      "Change from 13 to 11 at index 406620\n",
      "Change from 11 to 12 at index 407773\n",
      "Change from 12 to 11 at index 408926\n",
      "Change from 11 to 13 at index 411232\n",
      "Change from 13 to 11 at index 412385\n",
      "Change from 11 to 12 at index 414691\n",
      "Change from 12 to 10 at index 415844\n",
      "Change from 10 to 11 at index 416997\n",
      "Change from 11 to 13 at index 418150\n",
      "Change from 13 to 11 at index 419303\n",
      "Change from 11 to 13 at index 420456\n",
      "Change from 13 to 10 at index 421609\n",
      "Change from 10 to 11 at index 422762\n",
      "Change from 11 to 13 at index 423915\n",
      "Change from 13 to 12 at index 425068\n",
      "Change from 12 to 10 at index 426221\n",
      "Change from 10 to 12 at index 427374\n",
      "Change from 12 to 13 at index 428527\n",
      "Change from 13 to 11 at index 429680\n",
      "Change from 11 to 13 at index 430833\n",
      "Change from 13 to 10 at index 433139\n",
      "Change from 10 to 12 at index 434292\n",
      "Change from 12 to 10 at index 435445\n",
      "Change from 10 to 13 at index 436598\n",
      "Change from 13 to 10 at index 437751\n",
      "Change from 10 to 11 at index 438904\n",
      "Change from 11 to 13 at index 441210\n",
      "Change from 13 to 10 at index 442363\n",
      "Change from 10 to 11 at index 443516\n",
      "Change from 11 to 10 at index 444669\n",
      "Change from 10 to 12 at index 445822\n",
      "Change from 12 to 10 at index 446975\n",
      "Change from 10 to 11 at index 448128\n",
      "Change from 11 to 13 at index 449281\n",
      "Change from 13 to 12 at index 450434\n",
      "Change from 12 to 11 at index 452740\n",
      "Change from 11 to 12 at index 453893\n",
      "Change from 12 to 13 at index 455046\n",
      "Change from 13 to 10 at index 456199\n",
      "Change from 10 to 12 at index 457352\n",
      "Change from 12 to 11 at index 458505\n",
      "Change from 11 to 12 at index 459658\n",
      "Change from 12 to 13 at index 460811\n",
      "Change from 13 to 11 at index 461964\n",
      "Change from 11 to 10 at index 464270\n",
      "Change from 10 to 12 at index 466576\n",
      "Change from 12 to 13 at index 467729\n",
      "Change from 13 to 5 at index 468882\n",
      "Change from 5 to 8 at index 472723\n",
      "Change from 8 to 9 at index 473876\n",
      "Change from 9 to 7 at index 475029\n",
      "Change from 7 to 6 at index 476182\n",
      "Change from 6 to 7 at index 479641\n",
      "Change from 7 to 8 at index 480794\n",
      "Change from 8 to 9 at index 481947\n",
      "Change from 9 to 8 at index 484253\n",
      "Change from 8 to 7 at index 485406\n",
      "Change from 7 to 8 at index 486559\n",
      "Change from 8 to 6 at index 487712\n",
      "Change from 6 to 7 at index 488865\n",
      "Change from 7 to 9 at index 491171\n",
      "Change from 9 to 6 at index 492324\n",
      "Change from 6 to 8 at index 493477\n",
      "Change from 8 to 9 at index 494630\n",
      "Change from 9 to 2 at index 495783\n",
      "Change from 2 to 1 at index 498089\n",
      "Change from 1 to 2 at index 499242\n",
      "Change from 2 to 3 at index 500395\n",
      "Change from 3 to 1 at index 502701\n",
      "Change from 1 to 0 at index 505007\n",
      "Change from 0 to 2 at index 506160\n",
      "Change from 2 to 1 at index 507313\n",
      "Change from 1 to 2 at index 508466\n",
      "Change from 2 to 3 at index 509619\n",
      "Change from 3 to 1 at index 510772\n",
      "Change from 1 to 0 at index 511925\n",
      "Change from 0 to 3 at index 516537\n",
      "Change from 3 to 2 at index 518843\n",
      "Change from 2 to 1 at index 521149\n",
      "Change from 1 to 2 at index 522302\n",
      "Change from 2 to 3 at index 523455\n",
      "Change from 3 to 1 at index 525761\n",
      "Change from 1 to 0 at index 528067\n",
      "Change from 0 to 2 at index 529220\n",
      "Change from 2 to 1 at index 530373\n",
      "Change from 1 to 2 at index 531526\n",
      "Change from 2 to 3 at index 532679\n",
      "Change from 3 to 1 at index 533832\n",
      "Change from 1 to 0 at index 534985\n",
      "Change from 0 to 3 at index 539597\n",
      "Change from 3 to 10 at index 541903\n",
      "Change from 10 to 12 at index 543056\n",
      "Change from 12 to 11 at index 544209\n",
      "Change from 11 to 13 at index 545362\n",
      "Change from 13 to 11 at index 546515\n",
      "Change from 11 to 13 at index 549974\n",
      "Change from 13 to 12 at index 551127\n",
      "Change from 12 to 10 at index 552280\n",
      "Change from 10 to 12 at index 553433\n",
      "Change from 12 to 13 at index 554586\n",
      "Change from 13 to 10 at index 555739\n",
      "Change from 10 to 12 at index 556892\n",
      "Change from 12 to 13 at index 558045\n",
      "Change from 13 to 10 at index 559198\n",
      "Change from 10 to 13 at index 560351\n",
      "Change from 13 to 10 at index 561504\n",
      "Change from 10 to 12 at index 562657\n",
      "Change from 12 to 11 at index 563810\n",
      "Change from 11 to 12 at index 564963\n",
      "Change from 12 to 11 at index 568422\n",
      "Change from 11 to 12 at index 569575\n",
      "Change from 12 to 11 at index 570728\n",
      "Change from 11 to 13 at index 571881\n",
      "Change from 13 to 11 at index 574187\n",
      "Change from 11 to 10 at index 575340\n",
      "Change from 10 to 12 at index 576493\n",
      "Change from 12 to 13 at index 577646\n",
      "Change from 13 to 10 at index 578799\n",
      "Change from 10 to 13 at index 579952\n",
      "Change from 13 to 11 at index 581105\n",
      "Change from 11 to 13 at index 582258\n",
      "Change from 13 to 10 at index 583411\n",
      "Change from 10 to 11 at index 584564\n",
      "Change from 11 to 10 at index 585717\n",
      "\n",
      "Minimum cycle length: 1153\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is defined and already includes an 'event' column\n",
    "# Assuming 'event' column contains class labels\n",
    "event_changes = df['event'].diff().ne(0)\n",
    "change_indices = event_changes[event_changes].index.tolist()\n",
    "\n",
    "# Print the indices where changes occur\n",
    "print(\"Indices where the 'event' label changes:\")\n",
    "print(change_indices)\n",
    "\n",
    "# Calculate and print lengths between changes\n",
    "lengths_between_changes = [change_indices[i] - change_indices[i-1] for i in range(1, len(change_indices))]\n",
    "print(\"\\nLengths between changes:\")\n",
    "print(lengths_between_changes)\n",
    "\n",
    "# Further analysis if needed\n",
    "if change_indices:\n",
    "    print(\"\\nDetails of label changes at these indices:\")\n",
    "    for idx in change_indices:\n",
    "        # Skip the first index or when idx is 0 since there's no previous event to compare\n",
    "        if idx > 0:\n",
    "            print(f\"Change from {df.at[idx-1, 'event']} to {df.at[idx, 'event']} at index {idx}\")\n",
    "        else:\n",
    "            print(f\"Initial event starts at index {idx} with value {df.at[idx, 'event']}\")\n",
    "\n",
    "# Find the minimum cycle length where the label changes\n",
    "min_cycle_length = min(lengths_between_changes)\n",
    "print(f\"\\nMinimum cycle length: {min_cycle_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = StandardScaler()\n",
    "df.iloc[:,:-1] = mm.fit_transform(df.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class EEG_Dataset(Dataset):\n",
    "    def __init__(self, df, indices, window_size=128, **kwargs):\n",
    "        self.df = df\n",
    "        self.start_indices = indices  # Store only the start indices\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.start_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = self.start_indices[idx]\n",
    "        end_idx = start_idx + self.window_size\n",
    "\n",
    "        # Retrieve the sequence using calculated indices\n",
    "        seq = self.df.iloc[start_idx:end_idx]\n",
    "        X, y = seq.values[:, :-1], seq.values[:, -1]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        X = torch.from_numpy(X).float()\n",
    "        y = torch.from_numpy(y).long()\n",
    "        y = torch.nn.functional.one_hot(y, num_classes=num_classes)\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume 'df' is your DataFrame and 'event' is the column containing labels\n",
    "\n",
    "def generate_indices(df, window_size):\n",
    "    indices = []\n",
    "    max_index = len(df) - window_size + 1  # Calculate the maximum starting index\n",
    "    \n",
    "    for i in range(max_index):\n",
    "        # Check if all labels in the window are the same\n",
    "        if len(df['event'][i:i + window_size].unique()) == 1:\n",
    "            indices.append(i)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "# Example usage\n",
    "window_size = 128\n",
    "indices = generate_indices(df, window_size)\n",
    "shuffle(indices)  # Shuffle the indices to randomize the data order\n",
    "\n",
    "# Split the indices into training and testing sets\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Assuming you have an EEG_Dataset class defined as before\n",
    "trainset = EEG_Dataset(df, train_indices, window_size)\n",
    "testset = EEG_Dataset(df, test_indices, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = DataConfig(dataset_name = 'eeg-sub-01', task_type='multi_class_classification', obs_shape=[128], label_size=num_classes)\n",
    "\n",
    "#  Set training configuration from the AlgorithmConfig class, returning them as a Namespace object.\n",
    "ml_params = MLParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_params.core_model_name = 'gpt' \n",
    "ml_params.encoder_model_name = 'none'\n",
    "ml_params.training.max_epoch = 200\n",
    "ml_params.seq_len = window_size\n",
    "\n",
    "# Set the device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Initialize the TrainerHub class with the training configuration, data configuration, device, and use_print and use_wandb flags\n",
    "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b276f5f122f641969478b179b89a57d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10888e5e1eb4dc1a7b5e94a023184ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/6720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/200][50/6720][Time 21.56]\n",
      "Unified LR across all optimizers: 0.00019969466861371834\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.2815\tGen: 0.4642\tRec: 0.3848\tE: 0.3609\tR: 0.2020\tP: 0.5676\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.2589\n",
      "precision: 0.1538\n",
      "recall: 0.2443\n",
      "f1_score: 0.1726\n",
      "\n",
      "[0/200][100/6720][Time 20.84]\n",
      "Unified LR across all optimizers: 0.0001993957766378747\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0772\tGen: 0.1496\tRec: 0.1312\tE: 0.0956\tR: 0.0587\tP: 0.2036\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.1782\n",
      "precision: 0.1697\n",
      "recall: 0.1611\n",
      "f1_score: 0.1042\n",
      "\n",
      "[0/200][150/6720][Time 20.85]\n",
      "Unified LR across all optimizers: 0.00019909733202706992\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0508\tGen: 0.1136\tRec: 0.1042\tE: 0.0603\tR: 0.0414\tP: 0.1670\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.3586\n",
      "precision: 0.2489\n",
      "recall: 0.3436\n",
      "f1_score: 0.2489\n",
      "\n",
      "[0/200][200/6720][Time 20.87]\n",
      "Unified LR across all optimizers: 0.00019879933411171295\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0441\tGen: 0.1003\tRec: 0.0929\tE: 0.0515\tR: 0.0367\tP: 0.1490\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.3395\n",
      "precision: 0.2827\n",
      "recall: 0.2641\n",
      "f1_score: 0.2356\n",
      "\n",
      "[0/200][250/6720][Time 20.82]\n",
      "Unified LR across all optimizers: 0.00019850178222321458\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0412\tGen: 0.0909\tRec: 0.0838\tE: 0.0483\tR: 0.0341\tP: 0.1334\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.1741\n",
      "precision: 0.0829\n",
      "recall: 0.1447\n",
      "f1_score: 0.1034\n",
      "\n",
      "[0/200][300/6720][Time 21.00]\n",
      "Unified LR across all optimizers: 0.00019820467569398644\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0373\tGen: 0.0838\tRec: 0.0781\tE: 0.0430\tR: 0.0316\tP: 0.1246\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.2003\n",
      "precision: 0.1713\n",
      "recall: 0.1722\n",
      "f1_score: 0.1345\n",
      "\n",
      "[0/200][350/6720][Time 20.92]\n",
      "Unified LR across all optimizers: 0.00019790801385743923\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0335\tGen: 0.0779\tRec: 0.0735\tE: 0.0378\tR: 0.0291\tP: 0.1179\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.3125\n",
      "precision: 0.1386\n",
      "recall: 0.2955\n",
      "f1_score: 0.1743\n",
      "\n",
      "[0/200][400/6720][Time 20.95]\n",
      "Unified LR across all optimizers: 0.00019761179604798148\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0329\tGen: 0.0758\tRec: 0.0723\tE: 0.0364\tR: 0.0295\tP: 0.1151\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.2843\n",
      "precision: 0.1555\n",
      "recall: 0.2310\n",
      "f1_score: 0.1747\n",
      "\n",
      "[0/200][450/6720][Time 20.97]\n",
      "Unified LR across all optimizers: 0.00019731602160101788\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0316\tGen: 0.0727\tRec: 0.0686\tE: 0.0358\tR: 0.0274\tP: 0.1097\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.2954\n",
      "precision: 0.1414\n",
      "recall: 0.2354\n",
      "f1_score: 0.1734\n",
      "\n",
      "[0/200][500/6720][Time 20.88]\n",
      "Unified LR across all optimizers: 0.0001970206898529479\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0289\tGen: 0.0694\tRec: 0.0656\tE: 0.0326\tR: 0.0252\tP: 0.1061\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.2474\n",
      "precision: 0.1036\n",
      "recall: 0.1747\n",
      "f1_score: 0.1273\n",
      "\n",
      "[0/200][550/6720][Time 20.83]\n",
      "Unified LR across all optimizers: 0.00019672580014116413\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0277\tGen: 0.0676\tRec: 0.0643\tE: 0.0311\tR: 0.0244\tP: 0.1042\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.3347\n",
      "precision: 0.1918\n",
      "recall: 0.2559\n",
      "f1_score: 0.1971\n",
      "\n",
      "[0/200][600/6720][Time 20.87]\n",
      "Unified LR across all optimizers: 0.00019643135180405117\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0267\tGen: 0.0668\tRec: 0.0643\tE: 0.0292\tR: 0.0241\tP: 0.1044\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.2905\n",
      "precision: 0.1763\n",
      "recall: 0.1931\n",
      "f1_score: 0.1733\n",
      "\n",
      "[0/200][650/6720][Time 20.89]\n",
      "Unified LR across all optimizers: 0.00019613734418098366\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0252\tGen: 0.0648\tRec: 0.0624\tE: 0.0276\tR: 0.0228\tP: 0.1020\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.3263\n",
      "precision: 0.2236\n",
      "recall: 0.2487\n",
      "f1_score: 0.2072\n",
      "\n",
      "[0/200][700/6720][Time 20.78]\n",
      "Unified LR across all optimizers: 0.00019584377661232514\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0238\tGen: 0.0625\tRec: 0.0604\tE: 0.0259\tR: 0.0218\tP: 0.0991\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.3568\n",
      "precision: 0.2857\n",
      "recall: 0.3581\n",
      "f1_score: 0.3013\n",
      "\n",
      "[0/200][750/6720][Time 20.81]\n",
      "Unified LR across all optimizers: 0.0001955506484394265\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0230\tGen: 0.0608\tRec: 0.0588\tE: 0.0250\tR: 0.0209\tP: 0.0966\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.3138\n",
      "precision: 0.1611\n",
      "recall: 0.2524\n",
      "f1_score: 0.1842\n",
      "\n",
      "[0/200][800/6720][Time 20.82]\n",
      "Unified LR across all optimizers: 0.00019525795900462422\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0228\tGen: 0.0605\tRec: 0.0584\tE: 0.0249\tR: 0.0208\tP: 0.0961\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.2343\n",
      "precision: 0.1561\n",
      "recall: 0.2156\n",
      "f1_score: 0.1709\n",
      "\n",
      "[0/200][850/6720][Time 20.83]\n",
      "Unified LR across all optimizers: 0.0001949657076512394\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0229\tGen: 0.0601\tRec: 0.0581\tE: 0.0249\tR: 0.0209\tP: 0.0953\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.3135\n",
      "precision: 0.2222\n",
      "recall: 0.2752\n",
      "f1_score: 0.2207\n",
      "\n",
      "[0/200][900/6720][Time 20.83]\n",
      "Unified LR across all optimizers: 0.00019467389372357586\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0220\tGen: 0.0590\tRec: 0.0571\tE: 0.0239\tR: 0.0201\tP: 0.0942\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.3999\n",
      "precision: 0.2133\n",
      "recall: 0.3007\n",
      "f1_score: 0.2388\n",
      "\n",
      "[0/200][950/6720][Time 20.87]\n",
      "Unified LR across all optimizers: 0.00019438251656691888\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0214\tGen: 0.0580\tRec: 0.0559\tE: 0.0234\tR: 0.0194\tP: 0.0925\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.3218\n",
      "precision: 0.1653\n",
      "recall: 0.2532\n",
      "f1_score: 0.1881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer_hub.train(trainset, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_hub.test(testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
