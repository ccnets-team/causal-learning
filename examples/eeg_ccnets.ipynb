{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author:\n",
    "        \n",
    "        PARK, JunHo, junho@ccnets.org\n",
    "\n",
    "        \n",
    "        KIM, JeongYoong, jeongyoong@ccnets.org\n",
    "        \n",
    "    COPYRIGHT (c) 2024. CCNets. All Rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path_append = \"../\"\n",
    "sys.path.append(path_append)  # Go up one directory from where you are.\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.setting.ml_params import MLParameters\n",
    "from tools.setting.data_config import DataConfig\n",
    "from nn.utils.init import set_random_seed\n",
    "set_random_seed(0)\n",
    "\n",
    "from trainer_hub import TrainerHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>...</th>\n",
       "      <th>D24</th>\n",
       "      <th>D25</th>\n",
       "      <th>D26</th>\n",
       "      <th>D27</th>\n",
       "      <th>D28</th>\n",
       "      <th>D29</th>\n",
       "      <th>D30</th>\n",
       "      <th>D31</th>\n",
       "      <th>D32</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3549.790315</td>\n",
       "      <td>4533.538497</td>\n",
       "      <td>3619.665186</td>\n",
       "      <td>3077.291188</td>\n",
       "      <td>-1380.325575</td>\n",
       "      <td>6120.066816</td>\n",
       "      <td>-4072.820600</td>\n",
       "      <td>-2256.511456</td>\n",
       "      <td>1820.012261</td>\n",
       "      <td>-2815.635423</td>\n",
       "      <td>...</td>\n",
       "      <td>-7240.845997</td>\n",
       "      <td>7034.252627</td>\n",
       "      <td>8458.062496</td>\n",
       "      <td>5905.223463</td>\n",
       "      <td>6147.660515</td>\n",
       "      <td>2458.073582</td>\n",
       "      <td>-7465.876831</td>\n",
       "      <td>-3604.133966</td>\n",
       "      <td>-5445.224315</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3551.227812</td>\n",
       "      <td>4534.850995</td>\n",
       "      <td>3622.540181</td>\n",
       "      <td>3077.322438</td>\n",
       "      <td>-1377.575581</td>\n",
       "      <td>6123.066810</td>\n",
       "      <td>-4069.851856</td>\n",
       "      <td>-2252.167714</td>\n",
       "      <td>1825.168502</td>\n",
       "      <td>-2803.072947</td>\n",
       "      <td>...</td>\n",
       "      <td>-7227.283522</td>\n",
       "      <td>7039.627617</td>\n",
       "      <td>8463.874985</td>\n",
       "      <td>5911.598451</td>\n",
       "      <td>6153.504254</td>\n",
       "      <td>2463.354822</td>\n",
       "      <td>-7461.033090</td>\n",
       "      <td>-3594.258985</td>\n",
       "      <td>-5435.693082</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3556.727802</td>\n",
       "      <td>4539.850986</td>\n",
       "      <td>3629.040169</td>\n",
       "      <td>3081.978679</td>\n",
       "      <td>-1370.419344</td>\n",
       "      <td>6130.348047</td>\n",
       "      <td>-4063.508118</td>\n",
       "      <td>-2249.292720</td>\n",
       "      <td>1828.074746</td>\n",
       "      <td>-2804.041695</td>\n",
       "      <td>...</td>\n",
       "      <td>-7227.158522</td>\n",
       "      <td>7048.502600</td>\n",
       "      <td>8473.562467</td>\n",
       "      <td>5921.348433</td>\n",
       "      <td>6163.004236</td>\n",
       "      <td>2469.854810</td>\n",
       "      <td>-7460.470591</td>\n",
       "      <td>-3591.540240</td>\n",
       "      <td>-5433.568086</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3557.915300</td>\n",
       "      <td>4541.225983</td>\n",
       "      <td>3628.540169</td>\n",
       "      <td>3083.197427</td>\n",
       "      <td>-1372.263090</td>\n",
       "      <td>6130.410547</td>\n",
       "      <td>-4062.070620</td>\n",
       "      <td>-2251.667715</td>\n",
       "      <td>1825.856000</td>\n",
       "      <td>-2803.572946</td>\n",
       "      <td>...</td>\n",
       "      <td>-7224.189777</td>\n",
       "      <td>7042.346362</td>\n",
       "      <td>8464.593734</td>\n",
       "      <td>5917.660940</td>\n",
       "      <td>6160.972990</td>\n",
       "      <td>2467.011066</td>\n",
       "      <td>-7458.158095</td>\n",
       "      <td>-3597.008980</td>\n",
       "      <td>-5437.474329</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3553.352808</td>\n",
       "      <td>4535.757243</td>\n",
       "      <td>3622.477681</td>\n",
       "      <td>3079.572434</td>\n",
       "      <td>-1377.763080</td>\n",
       "      <td>6125.598056</td>\n",
       "      <td>-4066.570612</td>\n",
       "      <td>-2255.136459</td>\n",
       "      <td>1821.981008</td>\n",
       "      <td>-2808.041687</td>\n",
       "      <td>...</td>\n",
       "      <td>-7219.971035</td>\n",
       "      <td>7044.658857</td>\n",
       "      <td>8466.843729</td>\n",
       "      <td>5914.848445</td>\n",
       "      <td>6156.785498</td>\n",
       "      <td>2466.948566</td>\n",
       "      <td>-7457.501846</td>\n",
       "      <td>-3585.821500</td>\n",
       "      <td>-5428.630595</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588018</th>\n",
       "      <td>-623.326974</td>\n",
       "      <td>2269.261431</td>\n",
       "      <td>2575.479615</td>\n",
       "      <td>285.733846</td>\n",
       "      <td>907.388947</td>\n",
       "      <td>-491.014719</td>\n",
       "      <td>-2998.447586</td>\n",
       "      <td>1886.043389</td>\n",
       "      <td>1659.637557</td>\n",
       "      <td>416.296105</td>\n",
       "      <td>...</td>\n",
       "      <td>-7176.689865</td>\n",
       "      <td>2116.667963</td>\n",
       "      <td>-901.138961</td>\n",
       "      <td>-227.327706</td>\n",
       "      <td>-657.170662</td>\n",
       "      <td>3025.322534</td>\n",
       "      <td>-12313.149124</td>\n",
       "      <td>-3810.071086</td>\n",
       "      <td>-5620.505241</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588019</th>\n",
       "      <td>-627.420717</td>\n",
       "      <td>2264.448940</td>\n",
       "      <td>2570.323375</td>\n",
       "      <td>281.077605</td>\n",
       "      <td>903.482705</td>\n",
       "      <td>-490.702219</td>\n",
       "      <td>-3001.260080</td>\n",
       "      <td>1884.387142</td>\n",
       "      <td>1657.012562</td>\n",
       "      <td>414.702358</td>\n",
       "      <td>...</td>\n",
       "      <td>-7179.502360</td>\n",
       "      <td>2118.074210</td>\n",
       "      <td>-900.607712</td>\n",
       "      <td>-227.046456</td>\n",
       "      <td>-659.389408</td>\n",
       "      <td>3027.760030</td>\n",
       "      <td>-12307.211635</td>\n",
       "      <td>-3809.946086</td>\n",
       "      <td>-5621.098990</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588020</th>\n",
       "      <td>-631.764459</td>\n",
       "      <td>2260.730197</td>\n",
       "      <td>2566.917131</td>\n",
       "      <td>275.546365</td>\n",
       "      <td>902.045207</td>\n",
       "      <td>-493.545964</td>\n",
       "      <td>-3006.103821</td>\n",
       "      <td>1886.199639</td>\n",
       "      <td>1658.512560</td>\n",
       "      <td>424.202340</td>\n",
       "      <td>...</td>\n",
       "      <td>-7177.439864</td>\n",
       "      <td>2118.199210</td>\n",
       "      <td>-900.920211</td>\n",
       "      <td>-226.140208</td>\n",
       "      <td>-659.764407</td>\n",
       "      <td>3027.103781</td>\n",
       "      <td>-12305.774138</td>\n",
       "      <td>-3805.633594</td>\n",
       "      <td>-5614.880251</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588021</th>\n",
       "      <td>-625.076971</td>\n",
       "      <td>2265.605188</td>\n",
       "      <td>2573.354619</td>\n",
       "      <td>281.702604</td>\n",
       "      <td>904.982702</td>\n",
       "      <td>-490.795969</td>\n",
       "      <td>-3001.416330</td>\n",
       "      <td>1888.387135</td>\n",
       "      <td>1659.418808</td>\n",
       "      <td>420.077348</td>\n",
       "      <td>...</td>\n",
       "      <td>-7172.002374</td>\n",
       "      <td>2119.730457</td>\n",
       "      <td>-898.170216</td>\n",
       "      <td>-224.515211</td>\n",
       "      <td>-656.576913</td>\n",
       "      <td>3032.822520</td>\n",
       "      <td>-12303.742892</td>\n",
       "      <td>-3804.133597</td>\n",
       "      <td>-5614.192752</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588022</th>\n",
       "      <td>-623.639474</td>\n",
       "      <td>2268.636432</td>\n",
       "      <td>2576.229614</td>\n",
       "      <td>286.515095</td>\n",
       "      <td>909.795193</td>\n",
       "      <td>-484.358481</td>\n",
       "      <td>-2996.353839</td>\n",
       "      <td>1895.730871</td>\n",
       "      <td>1668.950040</td>\n",
       "      <td>431.983576</td>\n",
       "      <td>...</td>\n",
       "      <td>-7171.908624</td>\n",
       "      <td>2129.230440</td>\n",
       "      <td>-889.357733</td>\n",
       "      <td>-216.577726</td>\n",
       "      <td>-649.358176</td>\n",
       "      <td>3039.572508</td>\n",
       "      <td>-12297.899153</td>\n",
       "      <td>-3793.383617</td>\n",
       "      <td>-5603.130273</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>588023 rows Ã— 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 A1           A2           A3           A4           A5   \n",
       "0       3549.790315  4533.538497  3619.665186  3077.291188 -1380.325575  \\\n",
       "1       3551.227812  4534.850995  3622.540181  3077.322438 -1377.575581   \n",
       "2       3556.727802  4539.850986  3629.040169  3081.978679 -1370.419344   \n",
       "3       3557.915300  4541.225983  3628.540169  3083.197427 -1372.263090   \n",
       "4       3553.352808  4535.757243  3622.477681  3079.572434 -1377.763080   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "588018  -623.326974  2269.261431  2575.479615   285.733846   907.388947   \n",
       "588019  -627.420717  2264.448940  2570.323375   281.077605   903.482705   \n",
       "588020  -631.764459  2260.730197  2566.917131   275.546365   902.045207   \n",
       "588021  -625.076971  2265.605188  2573.354619   281.702604   904.982702   \n",
       "588022  -623.639474  2268.636432  2576.229614   286.515095   909.795193   \n",
       "\n",
       "                 A6           A7           A8           A9          A10  ...   \n",
       "0       6120.066816 -4072.820600 -2256.511456  1820.012261 -2815.635423  ...  \\\n",
       "1       6123.066810 -4069.851856 -2252.167714  1825.168502 -2803.072947  ...   \n",
       "2       6130.348047 -4063.508118 -2249.292720  1828.074746 -2804.041695  ...   \n",
       "3       6130.410547 -4062.070620 -2251.667715  1825.856000 -2803.572946  ...   \n",
       "4       6125.598056 -4066.570612 -2255.136459  1821.981008 -2808.041687  ...   \n",
       "...             ...          ...          ...          ...          ...  ...   \n",
       "588018  -491.014719 -2998.447586  1886.043389  1659.637557   416.296105  ...   \n",
       "588019  -490.702219 -3001.260080  1884.387142  1657.012562   414.702358  ...   \n",
       "588020  -493.545964 -3006.103821  1886.199639  1658.512560   424.202340  ...   \n",
       "588021  -490.795969 -3001.416330  1888.387135  1659.418808   420.077348  ...   \n",
       "588022  -484.358481 -2996.353839  1895.730871  1668.950040   431.983576  ...   \n",
       "\n",
       "                D24          D25          D26          D27          D28   \n",
       "0      -7240.845997  7034.252627  8458.062496  5905.223463  6147.660515  \\\n",
       "1      -7227.283522  7039.627617  8463.874985  5911.598451  6153.504254   \n",
       "2      -7227.158522  7048.502600  8473.562467  5921.348433  6163.004236   \n",
       "3      -7224.189777  7042.346362  8464.593734  5917.660940  6160.972990   \n",
       "4      -7219.971035  7044.658857  8466.843729  5914.848445  6156.785498   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "588018 -7176.689865  2116.667963  -901.138961  -227.327706  -657.170662   \n",
       "588019 -7179.502360  2118.074210  -900.607712  -227.046456  -659.389408   \n",
       "588020 -7177.439864  2118.199210  -900.920211  -226.140208  -659.764407   \n",
       "588021 -7172.002374  2119.730457  -898.170216  -224.515211  -656.576913   \n",
       "588022 -7171.908624  2129.230440  -889.357733  -216.577726  -649.358176   \n",
       "\n",
       "                D29           D30          D31          D32  event  \n",
       "0       2458.073582  -7465.876831 -3604.133966 -5445.224315      5  \n",
       "1       2463.354822  -7461.033090 -3594.258985 -5435.693082      5  \n",
       "2       2469.854810  -7460.470591 -3591.540240 -5433.568086      5  \n",
       "3       2467.011066  -7458.158095 -3597.008980 -5437.474329      5  \n",
       "4       2466.948566  -7457.501846 -3585.821500 -5428.630595      5  \n",
       "...             ...           ...          ...          ...    ...  \n",
       "588018  3025.322534 -12313.149124 -3810.071086 -5620.505241     10  \n",
       "588019  3027.760030 -12307.211635 -3809.946086 -5621.098990     10  \n",
       "588020  3027.103781 -12305.774138 -3805.633594 -5614.880251     10  \n",
       "588021  3032.822520 -12303.742892 -3804.133597 -5614.192752     10  \n",
       "588022  3039.572508 -12297.899153 -3793.383617 -5603.130273     10  \n",
       "\n",
       "[588023 rows x 129 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/N-Nieto/Inner_Speech_Dataset\n",
    "\n",
    "# Load the Inner Speech Dataset\n",
    "# =============================\n",
    "# This dataset comprises raw EEG data collected from subject 'sub-01' during session 'ses-01'.\n",
    "# Source: https://github.com/N-Nieto/Inner_Speech_Dataset\n",
    "#\n",
    "# Overview:\n",
    "# - The dataset is part of a study on inner speech, capturing brain activity via EEG.\n",
    "# - Each row in the dataset corresponds to a timestamp of EEG readings.\n",
    "# - Columns represent various EEG channels (electrodes placed on the scalp).\n",
    "#\n",
    "# Usage:\n",
    "# - The data is primarily used for cognitive neuroscience research, focusing on the neural correlates of inner speech.\n",
    "# - Users can analyze EEG signals to investigate brain activity patterns associated with the cognitive processes of inner speech.\n",
    "#\n",
    "# File Structure:\n",
    "# - Located at '../data/RAW_EEG/sub-01/sub-01_ses-01.csv' relative to this script.\n",
    "# - It is advisable to preprocess the data (filtering, normalization) before detailed analysis.\n",
    "#\n",
    "# Example:\n",
    "# - To load this data into a DataFrame for analysis and processing, use the following code snippet.\n",
    "\n",
    "\n",
    "df = None\n",
    "for csv in [\"../data/RAW_EEG/sub-01/sub-01_ses-01.csv\", \"../data/RAW_EEG/sub-01/sub-01_ses-02.csv\", \"../data/RAW_EEG/sub-01/sub-01_ses-03.csv\"]:\n",
    "    tmp_df = pd.read_csv(path_append + csv)\n",
    "    if df is None:\n",
    "        df = tmp_df\n",
    "    else:\n",
    "        df = pd.concat([df, tmp_df])\n",
    "df = df.reset_index(drop=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of each class in the 'event' column:\n",
      "event\n",
      "1     57650\n",
      "0     57650\n",
      "3     57650\n",
      "2     57650\n",
      "13    57650\n",
      "12    57650\n",
      "11    57650\n",
      "10    57650\n",
      "6     28825\n",
      "9     28825\n",
      "8     28825\n",
      "7     28825\n",
      "5     11523\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Maximum class number:\n",
      "13\n",
      "\n",
      "Expected number of classes (from num_classes variable): 14\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Example setup, assuming df and mm are defined as DataFrame and RobustScaler respectively\n",
    "\n",
    "# Assuming df['event'] contains the class labels\n",
    "event_counts = df['event'].value_counts()\n",
    "max_class_number = df['event'].max()\n",
    "\n",
    "# Print each number of classes\n",
    "print(\"Counts of each class in the 'event' column:\")\n",
    "print(event_counts)\n",
    "\n",
    "# Print the maximum class number\n",
    "print(\"\\nMaximum class number:\")\n",
    "print(max_class_number)\n",
    "\n",
    "num_classes = max_class_number + 1\n",
    "# Additionally, verify against the num_classes variable\n",
    "print(\"\\nExpected number of classes (from num_classes variable):\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices where the 'event' label changes: [0, 3841, 4994, 6147, 8453, 9606, 10759, 11912, 13065, 14218, 15371, 17677, 18830, 19983, 21136, 23442, 24595, 25748, 26901, 28054, 29207, 30360, 31513, 33819, 34972, 36125, 38431, 39584, 40737, 41890, 45349, 46502, 48808, 49961, 52267, 55726, 56879, 58032, 60338, 61491, 62644, 63797, 66103, 67256, 68409, 69562, 70715, 71868, 73021, 74174, 75327, 77633, 78786, 81092, 82245, 83398, 85704, 86857, 88010, 89163, 90316, 91469, 92622, 94928, 96081, 98387, 101846, 102999, 104152, 106458, 107611, 108764, 109917, 112223, 113376, 114529, 115682, 116835, 117988, 119141, 120294, 121447, 123753, 124906, 127212, 128365, 129518, 131824, 132977, 134130, 135283, 136436, 137589, 138742, 141048, 142201, 143354, 144507, 146813, 149119, 150272, 151425, 152578, 153731, 157190, 158343, 160649, 161802, 162955, 164108, 165261, 166414, 168720, 169873, 172179, 173332, 175638, 176791, 179097, 180250, 181403, 182556, 183709, 184862, 186015, 187168, 188321, 190627, 192933, 194086, 195239, 196392, 197545, 198698, 199851, 202157, 203310, 204463, 205616, 207922, 209075, 210228, 211381, 214840, 215993, 217146, 218299, 219452, 220605, 221758, 222911, 224064, 225217, 226370, 228676, 229829, 230982, 232135, 233288, 234441, 238282, 239435, 240588, 241741, 245200, 246353, 248659, 250965, 252118, 253271, 254424, 255577, 256730, 257883, 259036, 260189, 261342, 264801, 265954, 268260, 269413, 270566, 271719, 274025, 275178, 276331, 277484, 278637, 279790, 280943, 282096, 284402, 286708, 289014, 290167, 292473, 293626, 295932, 297085, 298238, 299391, 300544, 301697, 305156, 306309, 309768, 310921, 312074, 313227, 314380, 315533, 316686, 317839, 318992, 320145, 322451, 323604, 325910, 327063, 328216, 330522, 332828, 335134, 336287, 338593, 339746, 342052, 343205, 344358, 345511, 346664, 347817, 351276, 352429, 355888, 357041, 358194, 359347, 360500, 361653, 362806, 363959, 365112, 366265, 368571, 369724, 372030, 373183, 374336, 376642, 377795, 378948, 380101, 381254, 382407, 383560, 384713, 385866, 387019, 388172, 389325, 390478, 391631, 392784, 393937, 395090, 396243, 398549, 399702, 402008, 403161, 404314, 405467, 406620, 407773, 408926, 411232, 412385, 414691, 415844, 416997, 418150, 419303, 420456, 421609, 422762, 423915, 425068, 426221, 427374, 428527, 429680, 430833, 433139, 434292, 435445, 436598, 437751, 438904, 441210, 442363, 443516, 444669, 445822, 446975, 448128, 449281, 450434, 452740, 453893, 455046, 456199, 457352, 458505, 459658, 460811, 461964, 464270, 466576, 467729, 468882, 472723, 473876, 475029, 476182, 479641, 480794, 481947, 484253, 485406, 486559, 487712, 488865, 491171, 492324, 493477, 494630, 495783, 498089, 499242, 500395, 502701, 505007, 506160, 507313, 508466, 509619, 510772, 511925, 516537, 518843, 521149, 522302, 523455, 525761, 528067, 529220, 530373, 531526, 532679, 533832, 534985, 539597, 541903, 543056, 544209, 545362, 546515, 549974, 551127, 552280, 553433, 554586, 555739, 556892, 558045, 559198, 560351, 561504, 562657, 563810, 564963, 568422, 569575, 570728, 571881, 574187, 575340, 576493, 577646, 578799, 579952, 581105, 582258, 583411, 584564, 585717]\n",
      "Lengths between changes: [3841, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 2306, 1153, 1153, 1153, 3459, 1153, 2306, 1153, 2306, 3459, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 3459, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 3459, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 3459, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 3841, 1153, 1153, 1153, 3459, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 3459, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 2306, 2306, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 3459, 1153, 3459, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 2306, 2306, 2306, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 3459, 1153, 3459, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 2306, 1153, 1153, 3841, 1153, 1153, 1153, 3459, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 4612, 2306, 2306, 1153, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 4612, 2306, 1153, 1153, 1153, 1153, 3459, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 3459, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153]\n",
      "Minimum cycle length: 1153\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is defined and already includes an 'event' column\n",
    "# Assuming 'event' column contains class labels\n",
    "event_changes = df['event'].diff().ne(0)\n",
    "change_indices = event_changes[event_changes].index.tolist()\n",
    "\n",
    "# Calculate and print lengths between changes\n",
    "lengths_between_changes = [change_indices[i] - change_indices[i-1] for i in range(1, len(change_indices))]\n",
    "\n",
    "# Find the minimum cycle length where the label changes\n",
    "min_cycle_length = min(lengths_between_changes)\n",
    "\n",
    "print(\"Indices where the 'event' label changes:\", change_indices)\n",
    "print(\"Lengths between changes:\", lengths_between_changes)\n",
    "print(f\"Minimum cycle length: {min_cycle_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_tensor shape: torch.Size([588023, 128])\n",
      "         A1        A2        A3        A4        A5        A6        A7   \n",
      "0 -2.488196 -2.276049 -2.513326 -2.187655 -0.665328 -1.345729 -0.058758  \\\n",
      "1 -2.275978 -2.101950 -2.131188 -2.183079 -0.334046 -0.958461  0.322920   \n",
      "2 -1.464012 -1.438715 -1.267226 -1.501220  0.528039 -0.018531  1.138506   \n",
      "3 -1.288701 -1.256325 -1.333684 -1.322747  0.305930 -0.010463  1.323319   \n",
      "4 -1.962264 -1.981739 -2.139496 -1.853590 -0.356633 -0.631705  0.744775   \n",
      "\n",
      "         A8        A9       A10  ...       D24       D25       D26       D27   \n",
      "0  0.063271  0.040405 -1.022892  ... -1.909194 -1.004350 -0.560398 -1.681318  \\\n",
      "1  0.508570  0.570278 -0.405529  ... -1.421690 -0.624266  0.064576 -0.879298   \n",
      "2  0.803300  0.868933 -0.453136  ... -1.417197  0.003315  1.106200  0.347321   \n",
      "3  0.559827  0.640927 -0.430100  ... -1.310485 -0.432014  0.141858 -0.116593   \n",
      "4  0.204229  0.242720 -0.649710  ... -1.158842 -0.268490  0.383784 -0.470425   \n",
      "\n",
      "        D28       D29       D30       D31       D32  event  \n",
      "0 -1.691767 -1.766298 -0.931525 -1.651438 -1.624403      5  \n",
      "1 -0.971922 -1.347446 -0.658663 -1.326984 -1.307278      5  \n",
      "2  0.198306 -0.831935 -0.626976 -1.237656 -1.236575      5  \n",
      "3 -0.051907 -1.057471 -0.496707 -1.417338 -1.366544      5  \n",
      "4 -0.567732 -1.062428 -0.459739 -1.049760 -1.072294      5  \n",
      "\n",
      "[5 rows x 129 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Correctly select only the numerical columns (exclude the 'event' column) and convert to a PyTorch tensor\n",
    "df_tensor = torch.tensor(df.iloc[:, :-1].values).float().cuda()  # Using .iloc and .values to correctly handle DataFrame slicing\n",
    "print(\"df_tensor shape:\", df_tensor.shape)\n",
    "# Define a function to perform robust scaling using PyTorch\n",
    "def robust_scale_gpu(data):\n",
    "    median = torch.median(data, dim=0, keepdim=True).values\n",
    "    q75, q25 = torch.quantile(data, torch.tensor([0.75, 0.25], device=data.device), dim=0, keepdim=True)\n",
    "    iqr = q75 - q25\n",
    "\n",
    "    return (data - median) / iqr\n",
    "\n",
    "def standard_scale_gpu(data):\n",
    "    mean = torch.mean(data, dim=0, keepdim=True)\n",
    "    std = torch.std(data, dim=0, keepdim=True)\n",
    "\n",
    "    return (data - mean) / (std + 1e-8)\n",
    "\n",
    "for start, end in zip(change_indices[:-1], change_indices[1:]):\n",
    "    segment_length = end - start\n",
    "    if segment_length >= min_cycle_length and segment_length % min_cycle_length == 0:\n",
    "        # Normalize each sub-segment within the main segment\n",
    "        for offset in range(0, segment_length, min_cycle_length):\n",
    "            sub_start = start + offset\n",
    "            sub_end = sub_start + min_cycle_length\n",
    "            segment = df_tensor[sub_start:sub_end, :]\n",
    "            scaled_segment = standard_scale_gpu(segment)\n",
    "            df_tensor[sub_start:sub_end, :] = scaled_segment  # Correctly place the scaled data back into the DataFrame\n",
    "    else:\n",
    "        irregular_num = segment_length//min_cycle_length\n",
    "        # Normalize each sub-segment within the main segment\n",
    "        for i in range(irregular_num):\n",
    "            sub_start = start + i * min_cycle_length\n",
    "            if i == irregular_num - 1:\n",
    "                sub_end = end\n",
    "            else:\n",
    "                sub_end = sub_start + min_cycle_length\n",
    "            segment = df_tensor[sub_start:sub_end, :]\n",
    "            scaled_segment = standard_scale_gpu(segment)\n",
    "            df_tensor[sub_start:sub_end, :] = scaled_segment  # Correctly place the scaled data back into the DataFrame\n",
    "\n",
    "# Optionally, convert back to DataFrame if needed for further processing\n",
    "scaled_df = pd.DataFrame(df_tensor.cpu().numpy(), columns=df.columns[:-1])\n",
    "scaled_df['event'] = df['event']\n",
    "num_features = len(scaled_df.columns) - 1\n",
    "print(scaled_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class EEG_Dataset(Dataset):\n",
    "    def __init__(self, df, indices, max_window_size):\n",
    "        self.df = df\n",
    "        self.indices = indices  # List of start indices\n",
    "        self.max_window_size = max_window_size\n",
    "        self.min_window_size = max_window_size // 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = self.indices[idx]\n",
    "        # Randomly choose a window size between min_window_size and max_window_size\n",
    "        window_size = random.randint(self.min_window_size, self.max_window_size)\n",
    "        \n",
    "        end_idx = start_idx + window_size\n",
    "        # Make sure the end index does not go out of the bounds of the DataFrame\n",
    "        end_idx = min(end_idx, len(self.df))\n",
    "\n",
    "        # Retrieve the sequence using the calculated indices\n",
    "        seq = self.df.iloc[start_idx:end_idx]\n",
    "        X, y = seq.values[:, :-1], seq.values[:, -1]\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.long)  # ensure y is a tensor of type long\n",
    "        y = torch.nn.functional.one_hot(y, num_classes=num_classes)  # correct use\n",
    "        return X, y\n",
    "\n",
    "# Assuming 'df' is your DataFrame, 'indices' are the start indices, and 'max_window_size' is defined\n",
    "# trainset = EEG_Dataset(df, indices, max_window_size)\n",
    "# DataLoader code would follow initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from random import shuffle\n",
    "\n",
    "# Assume 'df' is your DataFrame and 'event' is the column containing labels\n",
    "\n",
    "def generate_indices(df, max_window_size):\n",
    "    indices = []\n",
    "    max_index = len(df) - max_window_size + 1  # Calculate the maximum starting index\n",
    "    \n",
    "    for i in range(max_index):\n",
    "        # Check if all labels in the window are the same\n",
    "        if len(df['event'][i:i + max_window_size].unique()) == 1:\n",
    "            indices.append(i)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "# Example usage\n",
    "max_window_size = 128\n",
    "indices = generate_indices(df, max_window_size)\n",
    "shuffle(indices)  # Shuffle the indices to randomize the data order\n",
    "\n",
    "# Split the indices into training and testing sets\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Assuming you have an EEG_Dataset class defined as before\n",
    "trainset = EEG_Dataset(df=scaled_df, indices=train_indices, max_window_size=max_window_size)\n",
    "testset = EEG_Dataset(df=scaled_df, indices=test_indices, max_window_size=max_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = DataConfig(dataset_name = 'eeg-sub-01', task_type='multi_class_classification', obs_shape=[num_features], label_size=num_classes)\n",
    "\n",
    "#  Set training configuration from the AlgorithmConfig class, returning them as a Namespace object.\n",
    "ml_params = MLParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_params.core_model_name = 'gpt' \n",
    "ml_params.encoder_model_name = 'none'\n",
    "ml_params.training.max_epoch = 200\n",
    "\n",
    "# Set the device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Initialize the TrainerHub class with the training configuration, data configuration, device, and use_print and use_wandb flags\n",
    "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9f74d0e615431c8cc46dafb6a4e17b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a76a6836c7e4ed68332749d1d24b97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/6720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/200][50/6720][Time 21.82]\n",
      "Unified LR across all optimizers: 0.00019969466861371834\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.3227\tGen: 17.0459\tRec: 16.9905\tE: 21.4960\tR: 15.1877\tP: 1948.8082\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.1542\n",
      "precision: 0.0902\n",
      "recall: 0.0955\n",
      "f1_score: 0.0684\n",
      "\n",
      "[0/200][100/6720][Time 20.87]\n",
      "Unified LR across all optimizers: 0.0001993957766378747\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1598\tGen: 11.3946\tRec: 11.3696\tE: 10.4920\tR: 7.6522\tP: 1292.2542\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.1536\n",
      "precision: 0.0985\n",
      "recall: 0.1245\n",
      "f1_score: 0.0883\n",
      "\n",
      "[0/200][150/6720][Time 20.87]\n",
      "Unified LR across all optimizers: 0.00019909733202706992\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1364\tGen: 12.4864\tRec: 12.4670\tE: 8.8670\tR: 6.6616\tP: 1414.6276\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.1946\n",
      "precision: 0.1508\n",
      "recall: 0.1715\n",
      "f1_score: 0.1179\n",
      "\n",
      "[0/200][200/6720][Time 20.84]\n",
      "Unified LR across all optimizers: 0.00019879933411171295\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1341\tGen: 13.4551\tRec: 13.4352\tE: 8.7495\tR: 6.4874\tP: 1557.4952\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.3048\n",
      "precision: 0.2574\n",
      "recall: 0.2241\n",
      "f1_score: 0.2146\n",
      "\n",
      "[0/200][250/6720][Time 20.90]\n",
      "Unified LR across all optimizers: 0.00019850178222321458\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1286\tGen: 8.8751\tRec: 8.8582\tE: 8.2663\tR: 6.3496\tP: 988.4402\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.3762\n",
      "precision: 0.2342\n",
      "recall: 0.2936\n",
      "f1_score: 0.2315\n",
      "\n",
      "[0/200][300/6720][Time 21.08]\n",
      "Unified LR across all optimizers: 0.00019820467569398644\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1139\tGen: 13.5784\tRec: 13.5637\tE: 7.3350\tR: 5.6633\tP: 1587.1015\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.4424\n",
      "precision: 0.3641\n",
      "recall: 0.3798\n",
      "f1_score: 0.3059\n",
      "\n",
      "[0/200][350/6720][Time 20.79]\n",
      "Unified LR across all optimizers: 0.00019790801385743923\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1022\tGen: 11.6050\tRec: 11.5924\tE: 6.5472\tR: 5.1149\tP: 1239.0663\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.3804\n",
      "precision: 0.3225\n",
      "recall: 0.3114\n",
      "f1_score: 0.2471\n",
      "\n",
      "[0/200][400/6720][Time 20.74]\n",
      "Unified LR across all optimizers: 0.00019761179604798148\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0953\tGen: 16.2671\tRec: 16.2553\tE: 6.1228\tR: 4.7832\tP: 1856.4372\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.5594\n",
      "precision: 0.3992\n",
      "recall: 0.3637\n",
      "f1_score: 0.3434\n",
      "\n",
      "[0/200][450/6720][Time 20.92]\n",
      "Unified LR across all optimizers: 0.00019731602160101788\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0871\tGen: 7.8082\tRec: 7.7978\tE: 5.5966\tR: 4.4083\tP: 868.8412\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.5878\n",
      "precision: 0.4147\n",
      "recall: 0.4074\n",
      "f1_score: 0.3824\n",
      "\n",
      "[0/200][500/6720][Time 20.81]\n",
      "Unified LR across all optimizers: 0.0001970206898529479\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0825\tGen: 11.7034\tRec: 11.6935\tE: 5.3127\tR: 4.1812\tP: 1355.4316\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.5057\n",
      "precision: 0.4437\n",
      "recall: 0.4047\n",
      "f1_score: 0.3680\n",
      "\n",
      "[0/200][550/6720][Time 20.87]\n",
      "Unified LR across all optimizers: 0.00019672580014116413\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0754\tGen: 9.4184\tRec: 9.4095\tE: 4.8132\tR: 3.8142\tP: 1054.1355\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.5896\n",
      "precision: 0.4134\n",
      "recall: 0.4624\n",
      "f1_score: 0.4140\n",
      "\n",
      "[0/200][600/6720][Time 20.94]\n",
      "Unified LR across all optimizers: 0.00019643135180405117\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0712\tGen: 10.3483\tRec: 10.3399\tE: 4.5535\tR: 3.6042\tP: 1184.6971\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.5607\n",
      "precision: 0.4542\n",
      "recall: 0.4239\n",
      "f1_score: 0.4063\n",
      "\n",
      "[0/200][650/6720][Time 20.92]\n",
      "Unified LR across all optimizers: 0.00019613734418098366\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0670\tGen: 13.4791\tRec: 13.4719\tE: 4.2321\tR: 3.4321\tP: 1519.3000\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.7161\n",
      "precision: 0.6026\n",
      "recall: 0.5508\n",
      "f1_score: 0.5381\n",
      "\n",
      "[0/200][700/6720][Time 20.53]\n",
      "Unified LR across all optimizers: 0.00019584377661232514\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0627\tGen: 12.4132\tRec: 12.4066\tE: 3.9884\tR: 3.2373\tP: 1448.6358\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6156\n",
      "precision: 0.5489\n",
      "recall: 0.5127\n",
      "f1_score: 0.4857\n",
      "\n",
      "[0/200][750/6720][Time 20.18]\n",
      "Unified LR across all optimizers: 0.0001955506484394265\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0597\tGen: 20.8821\tRec: 20.8759\tE: 3.7990\tR: 3.1013\tP: 2410.0951\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6198\n",
      "precision: 0.5387\n",
      "recall: 0.5119\n",
      "f1_score: 0.4707\n",
      "\n",
      "[0/200][800/6720][Time 20.18]\n",
      "Unified LR across all optimizers: 0.00019525795900462422\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0552\tGen: 12.2614\tRec: 12.2560\tE: 3.4986\tR: 2.8834\tP: 1356.2682\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6415\n",
      "precision: 0.5059\n",
      "recall: 0.5131\n",
      "f1_score: 0.4690\n",
      "\n",
      "[0/200][850/6720][Time 20.26]\n",
      "Unified LR across all optimizers: 0.0001949657076512394\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0520\tGen: 15.4449\tRec: 15.4396\tE: 3.3167\tR: 2.7199\tP: 1757.7215\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6978\n",
      "precision: 0.5818\n",
      "recall: 0.5794\n",
      "f1_score: 0.5458\n",
      "\n",
      "[0/200][900/6720][Time 20.21]\n",
      "Unified LR across all optimizers: 0.00019467389372357586\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0469\tGen: 8.2532\tRec: 8.2490\tE: 2.9585\tR: 2.4783\tP: 970.8853\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.7379\n",
      "precision: 0.5772\n",
      "recall: 0.5672\n",
      "f1_score: 0.5491\n",
      "\n",
      "[0/200][950/6720][Time 20.25]\n",
      "Unified LR across all optimizers: 0.00019438251656691888\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0456\tGen: 11.3846\tRec: 11.3804\tE: 2.8866\tR: 2.4134\tP: 1240.0956\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.7092\n",
      "precision: 0.6189\n",
      "recall: 0.5843\n",
      "f1_score: 0.5417\n",
      "\n",
      "[0/200][1000/6720][Time 20.37]\n",
      "Unified LR across all optimizers: 0.00019409157552753375\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0428\tGen: 12.5501\tRec: 12.5464\tE: 2.7078\tR: 2.2905\tP: 1415.3850\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8099\n",
      "precision: 0.7141\n",
      "recall: 0.6884\n",
      "f1_score: 0.6895\n",
      "\n",
      "[0/200][1050/6720][Time 20.92]\n",
      "Unified LR across all optimizers: 0.00019380106995266398\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0411\tGen: 12.7692\tRec: 12.7657\tE: 2.5973\tR: 2.1980\tP: 1483.8563\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6894\n",
      "precision: 0.5088\n",
      "recall: 0.5390\n",
      "f1_score: 0.5067\n",
      "\n",
      "[0/200][1100/6720][Time 20.50]\n",
      "Unified LR across all optimizers: 0.00019351099919053054\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0396\tGen: 7.0894\tRec: 7.0864\tE: 2.4835\tR: 2.1454\tP: 821.1201\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8162\n",
      "precision: 0.6456\n",
      "recall: 0.7074\n",
      "f1_score: 0.6676\n",
      "\n",
      "[0/200][1150/6720][Time 20.68]\n",
      "Unified LR across all optimizers: 0.00019322136259032944\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0373\tGen: 16.0293\tRec: 16.0266\tE: 2.3366\tR: 2.0270\tP: 1807.1785\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8177\n",
      "precision: 0.6556\n",
      "recall: 0.6379\n",
      "f1_score: 0.6263\n",
      "\n",
      "[0/200][1200/6720][Time 20.84]\n",
      "Unified LR across all optimizers: 0.00019293215950223126\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0367\tGen: 14.7585\tRec: 14.7563\tE: 2.2646\tR: 2.0090\tP: 1705.7077\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8324\n",
      "precision: 0.6282\n",
      "recall: 0.6805\n",
      "f1_score: 0.6379\n",
      "\n",
      "[0/200][1250/6720][Time 20.50]\n",
      "Unified LR across all optimizers: 0.00019264338927737882\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0319\tGen: 15.8630\tRec: 15.8609\tE: 1.9953\tR: 1.7522\tP: 1771.7390\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8125\n",
      "precision: 0.6355\n",
      "recall: 0.6718\n",
      "f1_score: 0.6433\n",
      "\n",
      "[0/200][1300/6720][Time 20.18]\n",
      "Unified LR across all optimizers: 0.00019235505126788632\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0304\tGen: 11.2648\tRec: 11.2630\tE: 1.9019\tR: 1.6940\tP: 1284.5394\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8175\n",
      "precision: 0.6628\n",
      "recall: 0.6982\n",
      "f1_score: 0.6649\n",
      "\n",
      "[0/200][1350/6720][Time 21.12]\n",
      "Unified LR across all optimizers: 0.0001920671448268376\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0295\tGen: 10.5170\tRec: 10.5153\tE: 1.8385\tR: 1.6433\tP: 1191.6871\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.7937\n",
      "precision: 0.7023\n",
      "recall: 0.6965\n",
      "f1_score: 0.6860\n",
      "\n",
      "[0/200][1400/6720][Time 20.61]\n",
      "Unified LR across all optimizers: 0.0001917796693082847\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0282\tGen: 10.4761\tRec: 10.4745\tE: 1.7485\tR: 1.5742\tP: 1234.8931\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.7737\n",
      "precision: 0.6117\n",
      "recall: 0.6682\n",
      "f1_score: 0.6362\n",
      "\n",
      "[0/200][1450/6720][Time 20.16]\n",
      "Unified LR across all optimizers: 0.00019149262406724683\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0276\tGen: 11.7329\tRec: 11.7316\tE: 1.7042\tR: 1.5502\tP: 1373.8732\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9230\n",
      "precision: 0.7878\n",
      "recall: 0.7830\n",
      "f1_score: 0.7815\n",
      "\n",
      "[0/200][1500/6720][Time 20.18]\n",
      "Unified LR across all optimizers: 0.00019120600845970806\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0297\tGen: 10.4746\tRec: 10.4730\tE: 1.8364\tR: 1.6603\tP: 1175.5191\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8644\n",
      "precision: 0.6548\n",
      "recall: 0.6690\n",
      "f1_score: 0.6501\n",
      "\n",
      "[0/200][1550/6720][Time 20.22]\n",
      "Unified LR across all optimizers: 0.00019091982184261694\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0256\tGen: 13.3363\tRec: 13.3351\tE: 1.5802\tR: 1.4453\tP: 1514.8358\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8503\n",
      "precision: 0.6590\n",
      "recall: 0.7003\n",
      "f1_score: 0.6737\n",
      "\n",
      "[0/200][1600/6720][Time 20.25]\n",
      "Unified LR across all optimizers: 0.0001906340635738838\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0231\tGen: 15.9407\tRec: 15.9397\tE: 1.4304\tR: 1.3111\tP: 1822.3385\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.7715\n",
      "precision: 0.7045\n",
      "recall: 0.7423\n",
      "f1_score: 0.6820\n",
      "\n",
      "[0/200][1650/6720][Time 20.81]\n",
      "Unified LR across all optimizers: 0.0001903487330123808\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0225\tGen: 14.4068\tRec: 14.4056\tE: 1.3931\tR: 1.2647\tP: 1645.2668\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8606\n",
      "precision: 0.7271\n",
      "recall: 0.7660\n",
      "f1_score: 0.7314\n",
      "\n",
      "[0/200][1700/6720][Time 20.41]\n",
      "Unified LR across all optimizers: 0.000190063829517939\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0210\tGen: 14.7258\tRec: 14.7247\tE: 1.3116\tR: 1.1932\tP: 1602.8979\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8225\n",
      "precision: 0.6830\n",
      "recall: 0.6635\n",
      "f1_score: 0.6500\n",
      "\n",
      "[0/200][1750/6720][Time 20.25]\n",
      "Unified LR across all optimizers: 0.00018977935245134824\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0213\tGen: 19.5465\tRec: 19.5456\tE: 1.3089\tR: 1.2163\tP: 2234.7360\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8903\n",
      "precision: 0.8045\n",
      "recall: 0.8127\n",
      "f1_score: 0.7755\n",
      "\n",
      "[0/200][1800/6720][Time 20.58]\n",
      "Unified LR across all optimizers: 0.00018949530117435472\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0210\tGen: 19.7887\tRec: 19.7879\tE: 1.2998\tR: 1.2118\tP: 2226.4433\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8544\n",
      "precision: 0.7228\n",
      "recall: 0.7192\n",
      "f1_score: 0.7154\n",
      "\n",
      "[0/200][1850/6720][Time 20.80]\n",
      "Unified LR across all optimizers: 0.00018921167504965984\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0186\tGen: 11.3506\tRec: 11.3501\tE: 1.1424\tR: 1.0848\tP: 1332.9885\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8664\n",
      "precision: 0.7943\n",
      "recall: 0.8135\n",
      "f1_score: 0.7871\n",
      "\n",
      "[0/200][1900/6720][Time 20.44]\n",
      "Unified LR across all optimizers: 0.00018892847344091938\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0178\tGen: 8.7079\tRec: 8.7075\tE: 1.0914\tR: 1.0500\tP: 1014.8305\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9015\n",
      "precision: 0.8167\n",
      "recall: 0.8258\n",
      "f1_score: 0.8089\n",
      "\n",
      "[0/200][1950/6720][Time 20.19]\n",
      "Unified LR across all optimizers: 0.0001886456957127411\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0175\tGen: 8.0874\tRec: 8.0870\tE: 1.0746\tR: 1.0377\tP: 974.5414\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8680\n",
      "precision: 0.7512\n",
      "recall: 0.7106\n",
      "f1_score: 0.7251\n",
      "\n",
      "[0/200][2000/6720][Time 20.14]\n",
      "Unified LR across all optimizers: 0.00018836334123068405\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0170\tGen: 8.4678\tRec: 8.4676\tE: 1.0439\tR: 1.0210\tP: 949.3310\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9059\n",
      "precision: 0.8165\n",
      "recall: 0.8400\n",
      "f1_score: 0.8234\n",
      "\n",
      "[0/200][2050/6720][Time 20.18]\n",
      "Unified LR across all optimizers: 0.0001880814093612565\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0160\tGen: 11.2919\tRec: 11.2918\tE: 0.9691\tR: 0.9612\tP: 1333.4094\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9022\n",
      "precision: 0.7418\n",
      "recall: 0.7626\n",
      "f1_score: 0.7415\n",
      "\n",
      "[0/200][2100/6720][Time 20.21]\n",
      "Unified LR across all optimizers: 0.0001877998994719154\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0134\tGen: 10.5436\tRec: 10.5435\tE: 0.8251\tR: 0.8175\tP: 1244.2498\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9405\n",
      "precision: 0.8132\n",
      "recall: 0.8071\n",
      "f1_score: 0.8089\n",
      "\n",
      "[0/200][2150/6720][Time 20.18]\n",
      "Unified LR across all optimizers: 0.00018751881093106415\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0140\tGen: 16.9085\tRec: 16.9083\tE: 0.8646\tR: 0.8479\tP: 1899.2262\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9285\n",
      "precision: 0.7980\n",
      "recall: 0.7763\n",
      "f1_score: 0.7807\n",
      "\n",
      "[0/200][2200/6720][Time 20.17]\n",
      "Unified LR across all optimizers: 0.00018723814310805145\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0127\tGen: 8.8354\tRec: 8.8354\tE: 0.7791\tR: 0.7817\tP: 1038.7664\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9238\n",
      "precision: 0.8390\n",
      "recall: 0.8550\n",
      "f1_score: 0.8431\n",
      "\n",
      "[0/200][2250/6720][Time 20.16]\n",
      "Unified LR across all optimizers: 0.00018695789537317\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0128\tGen: 14.6450\tRec: 14.6450\tE: 0.7804\tR: 0.7904\tP: 1646.8699\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9322\n",
      "precision: 0.8451\n",
      "recall: 0.8389\n",
      "f1_score: 0.8299\n",
      "\n",
      "[0/200][2300/6720][Time 20.17]\n",
      "Unified LR across all optimizers: 0.00018667806709765522\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0125\tGen: 9.5168\tRec: 9.5168\tE: 0.7566\tR: 0.7680\tP: 1066.0433\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9362\n",
      "precision: 0.8120\n",
      "recall: 0.7678\n",
      "f1_score: 0.7817\n",
      "\n",
      "[0/200][2350/6720][Time 20.17]\n",
      "Unified LR across all optimizers: 0.00018639865765368338\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0117\tGen: 9.4642\tRec: 9.4643\tE: 0.7157\tR: 0.7335\tP: 1139.7832\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9261\n",
      "precision: 0.7908\n",
      "recall: 0.7583\n",
      "f1_score: 0.7591\n",
      "\n",
      "[0/200][2400/6720][Time 20.20]\n",
      "Unified LR across all optimizers: 0.00018611966641437044\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0122\tGen: 23.5283\tRec: 23.5284\tE: 0.7404\tR: 0.7565\tP: 2653.3194\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9289\n",
      "precision: 0.8139\n",
      "recall: 0.7810\n",
      "f1_score: 0.7924\n",
      "\n",
      "[0/200][2450/6720][Time 20.18]\n",
      "Unified LR across all optimizers: 0.00018584109275377077\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0104\tGen: 9.4347\tRec: 9.4349\tE: 0.6320\tR: 0.6585\tP: 1069.8698\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9454\n",
      "precision: 0.8761\n",
      "recall: 0.8482\n",
      "f1_score: 0.8538\n",
      "\n",
      "[0/200][2500/6720][Time 20.14]\n",
      "Unified LR across all optimizers: 0.00018556293604687557\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0109\tGen: 15.8363\tRec: 15.8365\tE: 0.6584\tR: 0.6857\tP: 1837.6113\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9216\n",
      "precision: 0.8470\n",
      "recall: 0.8612\n",
      "f1_score: 0.8506\n",
      "\n",
      "[0/200][2550/6720][Time 20.21]\n",
      "Unified LR across all optimizers: 0.00018528519566961144\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0100\tGen: 15.3096\tRec: 15.3098\tE: 0.5976\tR: 0.6346\tP: 1720.1764\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9243\n",
      "precision: 0.8671\n",
      "recall: 0.8596\n",
      "f1_score: 0.8589\n",
      "\n",
      "[0/200][2600/6720][Time 20.19]\n",
      "Unified LR across all optimizers: 0.00018500787099883916\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0101\tGen: 12.6407\tRec: 12.6411\tE: 0.6063\tR: 0.6489\tP: 1401.2964\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9410\n",
      "precision: 0.8734\n",
      "recall: 0.8659\n",
      "f1_score: 0.8678\n",
      "\n",
      "[0/200][2650/6720][Time 20.17]\n",
      "Unified LR across all optimizers: 0.00018473096141235213\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0090\tGen: 8.5013\tRec: 8.5016\tE: 0.5376\tR: 0.5763\tP: 1004.7732\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9311\n",
      "precision: 0.8581\n",
      "recall: 0.8448\n",
      "f1_score: 0.8483\n",
      "\n",
      "[0/200][2700/6720][Time 20.19]\n",
      "Unified LR across all optimizers: 0.00018445446628887513\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0086\tGen: 16.2815\tRec: 16.2818\tE: 0.5143\tR: 0.5498\tP: 1898.2283\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9307\n",
      "precision: 0.8451\n",
      "recall: 0.8528\n",
      "f1_score: 0.8413\n",
      "\n",
      "[0/200][2750/6720][Time 20.18]\n",
      "Unified LR across all optimizers: 0.00018417838500806271\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0085\tGen: 4.8472\tRec: 4.8475\tE: 0.5097\tR: 0.5469\tP: 560.2333\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9478\n",
      "precision: 0.8153\n",
      "recall: 0.8055\n",
      "f1_score: 0.8075\n",
      "\n",
      "[0/200][2800/6720][Time 20.16]\n",
      "Unified LR across all optimizers: 0.00018390271695049802\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0088\tGen: 8.5242\tRec: 8.5245\tE: 0.5258\tR: 0.5680\tP: 962.1811\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9533\n",
      "precision: 0.8187\n",
      "recall: 0.8094\n",
      "f1_score: 0.8121\n",
      "\n",
      "[0/200][2850/6720][Time 20.21]\n",
      "Unified LR across all optimizers: 0.00018362746149769128\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0075\tGen: 12.7695\tRec: 12.7698\tE: 0.4479\tR: 0.4930\tP: 1437.8849\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9678\n",
      "precision: 0.8952\n",
      "recall: 0.9040\n",
      "f1_score: 0.8990\n",
      "\n",
      "[0/200][2900/6720][Time 20.19]\n",
      "Unified LR across all optimizers: 0.00018335261803207844\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0075\tGen: 17.5956\tRec: 17.5959\tE: 0.4503\tR: 0.4887\tP: 2039.9635\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9579\n",
      "precision: 0.8157\n",
      "recall: 0.8150\n",
      "f1_score: 0.8148\n",
      "\n",
      "[0/200][2950/6720][Time 20.17]\n",
      "Unified LR across all optimizers: 0.00018307818593701973\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0069\tGen: 16.6210\tRec: 16.6212\tE: 0.4133\tR: 0.4511\tP: 1904.8519\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9423\n",
      "precision: 0.8633\n",
      "recall: 0.8605\n",
      "f1_score: 0.8608\n",
      "\n",
      "[0/200][3000/6720][Time 20.16]\n",
      "Unified LR across all optimizers: 0.00018280416459679836\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0068\tGen: 10.4376\tRec: 10.4378\tE: 0.4128\tR: 0.4476\tP: 1104.4235\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9540\n",
      "precision: 0.8827\n",
      "recall: 0.8785\n",
      "f1_score: 0.8781\n",
      "\n",
      "[0/200][3050/6720][Time 20.17]\n",
      "Unified LR across all optimizers: 0.00018253055339661917\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0068\tGen: 16.8166\tRec: 16.8169\tE: 0.4089\tR: 0.4513\tP: 1881.8573\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9653\n",
      "precision: 0.8963\n",
      "recall: 0.8997\n",
      "f1_score: 0.8977\n",
      "\n",
      "[0/200][3100/6720][Time 20.19]\n",
      "Unified LR across all optimizers: 0.000182257351722607\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0068\tGen: 11.5925\tRec: 11.5928\tE: 0.4061\tR: 0.4458\tP: 1336.7950\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9521\n",
      "precision: 0.7967\n",
      "recall: 0.8201\n",
      "f1_score: 0.8032\n",
      "\n",
      "[0/200][3150/6720][Time 20.18]\n",
      "Unified LR across all optimizers: 0.00018198455896180582\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0075\tGen: 7.5794\tRec: 7.5798\tE: 0.4469\tR: 0.4989\tP: 880.3092\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9566\n",
      "precision: 0.7526\n",
      "recall: 0.7518\n",
      "f1_score: 0.7517\n",
      "\n",
      "[0/200][3200/6720][Time 20.16]\n",
      "Unified LR across all optimizers: 0.00018171217450217676\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0066\tGen: 18.2105\tRec: 18.2108\tE: 0.3885\tR: 0.4353\tP: 1955.5531\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9624\n",
      "precision: 0.8188\n",
      "recall: 0.8334\n",
      "f1_score: 0.8251\n",
      "\n",
      "[0/200][3250/6720][Time 20.14]\n",
      "Unified LR across all optimizers: 0.00018144019773259714\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0059\tGen: 10.3277\tRec: 10.3281\tE: 0.3527\tR: 0.3990\tP: 1204.0761\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9650\n",
      "precision: 0.8275\n",
      "recall: 0.8229\n",
      "f1_score: 0.8248\n",
      "\n",
      "[0/200][3300/6720][Time 20.22]\n",
      "Unified LR across all optimizers: 0.00018116862804285912\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0053\tGen: 11.2483\tRec: 11.2486\tE: 0.3172\tR: 0.3584\tP: 1334.3966\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9542\n",
      "precision: 0.8853\n",
      "recall: 0.8811\n",
      "f1_score: 0.8812\n",
      "\n",
      "[0/200][3350/6720][Time 20.22]\n",
      "Unified LR across all optimizers: 0.00018089746482366777\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0060\tGen: 9.2207\tRec: 9.2211\tE: 0.3562\tR: 0.4045\tP: 971.3498\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9678\n",
      "precision: 0.8991\n",
      "recall: 0.8957\n",
      "f1_score: 0.8970\n",
      "\n",
      "[0/200][3400/6720][Time 20.02]\n",
      "Unified LR across all optimizers: 0.0001806267074666406\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0059\tGen: 12.4704\tRec: 12.4708\tE: 0.3486\tR: 0.3960\tP: 1394.6802\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9637\n",
      "precision: 0.8924\n",
      "recall: 0.8885\n",
      "f1_score: 0.8898\n",
      "\n",
      "[0/200][3450/6720][Time 20.06]\n",
      "Unified LR across all optimizers: 0.00018035635536430543\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0058\tGen: 11.8832\tRec: 11.8835\tE: 0.3432\tR: 0.3865\tP: 1257.5910\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9309\n",
      "precision: 0.8520\n",
      "recall: 0.8777\n",
      "f1_score: 0.8631\n",
      "\n",
      "[0/200][3500/6720][Time 20.25]\n",
      "Unified LR across all optimizers: 0.00018008640791009926\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0053\tGen: 12.2119\tRec: 12.2122\tE: 0.3129\tR: 0.3536\tP: 1380.4256\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9465\n",
      "precision: 0.7267\n",
      "recall: 0.7465\n",
      "f1_score: 0.7315\n",
      "\n",
      "[0/200][3550/6720][Time 20.08]\n",
      "Unified LR across all optimizers: 0.00017981686449836722\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0050\tGen: 12.0267\tRec: 12.0271\tE: 0.2951\tR: 0.3424\tP: 1381.0219\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9709\n",
      "precision: 0.8277\n",
      "recall: 0.8304\n",
      "f1_score: 0.8285\n",
      "\n",
      "[0/200][3600/6720][Time 20.08]\n",
      "Unified LR across all optimizers: 0.0001795477245243606\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0055\tGen: 12.6436\tRec: 12.6440\tE: 0.3209\tR: 0.3686\tP: 1416.0383\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9706\n",
      "precision: 0.8980\n",
      "recall: 0.9035\n",
      "f1_score: 0.9004\n",
      "\n",
      "[0/200][3650/6720][Time 20.03]\n",
      "Unified LR across all optimizers: 0.00017927898738423638\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0054\tGen: 16.4847\tRec: 16.4851\tE: 0.3207\tR: 0.3671\tP: 1789.5039\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9604\n",
      "precision: 0.8842\n",
      "recall: 0.9039\n",
      "f1_score: 0.8902\n",
      "\n",
      "[0/200][3700/6720][Time 20.85]\n",
      "Unified LR across all optimizers: 0.00017901065247505463\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0052\tGen: 19.2139\tRec: 19.2143\tE: 0.3026\tR: 0.3543\tP: 2178.5389\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9508\n",
      "precision: 0.8819\n",
      "recall: 0.8841\n",
      "f1_score: 0.8822\n",
      "\n",
      "[0/200][3750/6720][Time 20.88]\n",
      "Unified LR across all optimizers: 0.00017874271919477843\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0048\tGen: 16.9380\tRec: 16.9384\tE: 0.2780\tR: 0.3222\tP: 1889.2599\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9455\n",
      "precision: 0.8759\n",
      "recall: 0.8731\n",
      "f1_score: 0.8708\n",
      "\n",
      "[0/200][3800/6720][Time 20.70]\n",
      "Unified LR across all optimizers: 0.0001784751869422717\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0050\tGen: 12.9028\tRec: 12.9032\tE: 0.2961\tR: 0.3406\tP: 1479.7809\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9695\n",
      "precision: 0.8972\n",
      "recall: 0.9011\n",
      "f1_score: 0.8988\n",
      "\n",
      "[0/200][3850/6720][Time 21.12]\n",
      "Unified LR across all optimizers: 0.0001782080551172982\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0046\tGen: 10.5641\tRec: 10.5645\tE: 0.2707\tR: 0.3145\tP: 1214.4753\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9674\n",
      "precision: 0.7541\n",
      "recall: 0.7644\n",
      "f1_score: 0.7586\n",
      "\n",
      "[0/200][3900/6720][Time 20.85]\n",
      "Unified LR across all optimizers: 0.0001779413231205201\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0045\tGen: 9.6227\tRec: 9.6230\tE: 0.2671\tR: 0.3081\tP: 1139.7527\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9827\n",
      "precision: 0.9105\n",
      "recall: 0.9133\n",
      "f1_score: 0.9118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer_hub.train(trainset, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_hub.test(testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
