{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Authors : Jinsu Kim, JunHo Park\n",
        "\n",
        "â“’ 2022 CCNets, Inc. All Rights Reserved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://ccnets.org"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "path_append = \"../\"\n",
        "sys.path.append(path_append)  # Go up one directory from where you are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# https://www.kaggle.com/datasets/ruslankl/mice-protein-expression\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define the base directory and CSV file name\n",
        "base_dir = path_append + \"../data/mice_protein_expression/\"  # Update this to the directory where your data folder is located\n",
        "csv_file = \"Data_Cortex_Nuclear.csv\"  # Update this to your CSV file name if different\n",
        "\n",
        "# Full path to the CSV file\n",
        "full_path = os.path.join(base_dir, csv_file)\n",
        "\n",
        "# Load the dataset\n",
        "try:\n",
        "    df = pd.read_csv(full_path)\n",
        "    print(\"Data loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Failed to load data. File not found at:\", full_path)\n",
        "\n",
        "# No need for image_size here unless it is used later in your code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TrainLoader / DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "# Preprocess dataset\n",
        "df = df.drop(\"MouseID\", axis=1)\n",
        "label_cols = [\"Genotype\", \"Treatment\", \"Behavior\", \"class\"]\n",
        "for col in label_cols:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\impute\\_iterative.py:700: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Impute missing values\n",
        "imputer = IterativeImputer(max_iter=10, random_state=0)  # max_iter was num_features; adjust as appropriate\n",
        "df[:] = imputer.fit_transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(80, 8)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Scale features\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "feature_cols = df.columns[df.columns != 'class']\n",
        "df[feature_cols] = StandardScaler().fit_transform(df[feature_cols])\n",
        "\n",
        "# Determine number of features and classes\n",
        "num_features = len(feature_cols)\n",
        "num_classes = len(df['class'].unique())\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[feature_cols], df['class'], test_size=0.2, random_state=1)\n",
        "num_features, num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom dataset class\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.x = torch.tensor(features, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        y_one_hot = torch.nn.functional.one_hot(self.y[index], num_classes=num_classes)\n",
        "        return self.x[index], y_one_hot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Dataset instances\n",
        "train_dataset = CustomDataset(X_train.values, y_train.values)\n",
        "test_dataset = CustomDataset(X_test.values, y_test.values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([864, 80]),\n",
              " torch.Size([864]),\n",
              " torch.Size([216, 80]),\n",
              " torch.Size([216]))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset.x.shape, train_dataset.y.shape, test_dataset.x.shape, test_dataset.y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 0.8567,  0.8996,  0.5220,  0.2666,  0.3052,  0.1107, -0.1130,  1.1694,\n",
            "        -0.1509,  0.2293,  0.8203,  0.1649,  0.4288,  0.2231, -0.3929, -0.5361,\n",
            "        -0.6118,  0.9344,  1.0031, -0.1890,  0.9307, -0.1968, -0.0757, -0.0926,\n",
            "         0.3065,  1.3649,  0.3003,  0.0717,  0.7352,  0.1070,  0.1126,  0.4942,\n",
            "        -0.8181, -0.3349, -0.6556, -0.3706, -0.7622, -0.0408, -0.2814,  0.3589,\n",
            "        -0.4635, -0.9334,  0.1160, -0.1771, -0.1465,  0.0760,  0.5365,  0.7723,\n",
            "         0.4190,  1.1157, -0.2373, -0.3129, -0.1937, -0.0316,  0.5051,  0.3011,\n",
            "        -0.1636, -0.1046, -0.4791, -0.2392, -0.3477, -0.3318, -0.3252, -0.1085,\n",
            "        -1.0837, -0.3432,  1.1123,  1.0492, -0.5837, -1.4140, -0.0316, -1.4158,\n",
            "         0.1160, -0.8185, -1.3089, -1.0408,  0.6297, -0.9459, -0.9459, -1.0282]) tensor([1, 0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "for features, labels in train_dataset:\n",
        "    print(features, labels)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tools.setting.ml_params import MLParameters\n",
        "from tools.setting.data_config import DataConfig\n",
        "\n",
        "data_config = DataConfig(dataset_name = 'mice_protein_expression', task_type='multi_class_classification', obs_shape=[num_features], label_size=num_classes)\n",
        "#  Set training configuration from the AlgorithmConfig class, returning them as a Namespace object.\n",
        "ml_params = MLParameters(ccnet_network = 'gpt', encoder_network = 'none')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trainer_hub import TrainerHub\n",
        "\n",
        "# Set the device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "\n",
        "# Initialize the TrainerHub class with the training configuration, data configuration, device, and use_print and use_wandb flags\n",
        "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91f59ddd57d745fe9d3c92a4e0073636",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3/100][11/13][Time 3.26]\n",
            "Unified LR across all optimizers: 0.0001995308238189185\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.1355\tGen: 0.5654\tRec: 0.5560\tE: 0.1448\tR: 0.1261\tP: 0.9860\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 0.9688\n",
            "precision: 0.9665\n",
            "recall: 0.9808\n",
            "f1_score: 0.9716\n",
            "\n",
            "[7/100][9/13][Time 3.05]\n",
            "Unified LR across all optimizers: 0.00019907191565870155\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0099\tGen: 0.3267\tRec: 0.3263\tE: 0.0103\tR: 0.0095\tP: 0.6432\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 0.9844\n",
            "precision: 0.9896\n",
            "recall: 0.9750\n",
            "f1_score: 0.9807\n",
            "\n",
            "[11/100][7/13][Time 3.05]\n",
            "Unified LR across all optimizers: 0.00019861406295796434\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0028\tGen: 0.2672\tRec: 0.2671\tE: 0.0029\tR: 0.0027\tP: 0.5315\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 1.0000\n",
            "precision: 1.0000\n",
            "recall: 1.0000\n",
            "f1_score: 1.0000\n",
            "\n",
            "[15/100][5/13][Time 3.16]\n",
            "Unified LR across all optimizers: 0.00019815726328921765\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0009\tGen: 0.2318\tRec: 0.2318\tE: 0.0009\tR: 0.0008\tP: 0.4627\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 1.0000\n",
            "precision: 1.0000\n",
            "recall: 1.0000\n",
            "f1_score: 1.0000\n",
            "\n",
            "[19/100][3/13][Time 3.18]\n",
            "Unified LR across all optimizers: 0.00019770151423055492\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0005\tGen: 0.2135\tRec: 0.2134\tE: 0.0005\tR: 0.0005\tP: 0.4264\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 1.0000\n",
            "precision: 1.0000\n",
            "recall: 1.0000\n",
            "f1_score: 1.0000\n",
            "\n",
            "[23/100][1/13][Time 3.11]\n",
            "Unified LR across all optimizers: 0.00019724681336564005\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0004\tGen: 0.1954\tRec: 0.1954\tE: 0.0004\tR: 0.0004\tP: 0.3904\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 1.0000\n",
            "precision: 1.0000\n",
            "recall: 1.0000\n",
            "f1_score: 1.0000\n",
            "\n",
            "[26/100][12/13][Time 3.09]\n",
            "Unified LR across all optimizers: 0.00019679315828369438\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0003\tGen: 0.1843\tRec: 0.1843\tE: 0.0003\tR: 0.0003\tP: 0.3683\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 1.0000\n",
            "precision: 1.0000\n",
            "recall: 1.0000\n",
            "f1_score: 1.0000\n",
            "\n",
            "[30/100][10/13][Time 3.14]\n",
            "Unified LR across all optimizers: 0.00019634054657948372\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0003\tGen: 0.1752\tRec: 0.1752\tE: 0.0003\tR: 0.0003\tP: 0.3501\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 1.0000\n",
            "precision: 1.0000\n",
            "recall: 1.0000\n",
            "f1_score: 1.0000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainer_hub.train(train_dataset, test_dataset)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "metadata": {
      "interpreter": {
        "hash": "a7e81af88087f1f4bdc1f0426df14b24fa2673362c5daa7f7f9146748f40b3b1"
      }
    },
    "vscode": {
      "interpreter": {
        "hash": "aa62b0cd3048c1f98ba81d64726d0b99961f9a4c88743680dc76385f8ef59940"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
