{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Author:\n",
    "        \n",
    "        PARK, JunHo, junho@ccnets.org\n",
    "\n",
    "        \n",
    "        KIM, JoengYoong, jeongyoong@ccnets.org\n",
    "        \n",
    "    COPYRIGHT (c) 2024. CCNets. All Rights reserved.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is it Possible to Learn Target Prediction from Encoding? \n",
    "## Recyclable and Household Waste Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates the use of a cooperative encoding\n",
    "network for causal encoding. The pipeline involves an encoding\n",
    "model (cooperative encoding network with three ResNets) and\n",
    "a core model (three GPTs). It showcases how the encoding\n",
    "process and prediction learning happen internally in the API,\n",
    "as illustrated in the example file.\n",
    "\n",
    "## Problem Definition\n",
    "\n",
    "In many machine learning tasks, effectively encoding data so that\n",
    "the predictive model can learn the underlying patterns and\n",
    "relationships is challenging. Traditional encoding methods, such\n",
    "as autoencoders, often fail to capture the causal relationships\n",
    "and inherent attributes of the data effectively. These methods\n",
    "typically compress the data stochastically, requiring a decoding\n",
    "step with the paired decoder used in training to retrieve the\n",
    "original data before further prediction learning can occur.\n",
    "\n",
    "### Key Issues with Traditional Encoding:\n",
    "1. **Need for Decoding:** Further prediction learning from\n",
    "   compressed data (e.g., large language model learning)\n",
    "   necessitates a decoder.\n",
    "2. **Paired Decoding:** The decoder must be trained together\n",
    "   with the encoder to effectively reconstruct the original data.\n",
    "\n",
    "In contrast, the causal encoding framework directly addresses these\n",
    "issues by uncovering and manipulating independent causal factors\n",
    "and common attributes in dataset observations. This allows for\n",
    "a more structured and meaningful representation of the data,\n",
    "facilitating accurate prediction learning without the need\n",
    "for intermediate decoding steps.\n",
    "\n",
    "### Advantages of Causal Encoding Framework:\n",
    "- **Independent of Prediction Model:** The cooperative encoding\n",
    "  network does not require training or interaction with the\n",
    "  prediction model.\n",
    "- **Structured Representation:** Utilizes both stochastic and\n",
    "  deterministic variables to capture all causal factors in the\n",
    "  latent representation, which is a concatenation of the Explainer\n",
    "  and Reasoner outputs in the encoding network.\n",
    "\n",
    "This structured approach allows the model to learn from the encoded\n",
    "data and make accurate predictions more efficiently.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "[Recyclable and Household Waste Classification](https://www.kaggle.com/datasets/alistairking/recyclable-and-household-waste-classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "path_append = \"../\" # Go up one directory from where you are.\n",
    "sys.path.append(path_append) \n",
    "\n",
    "from tools.setting.ml_params import MLParameters\n",
    "from tools.setting.data_config import DataConfig\n",
    "from nn.utils.init import set_random_seed\n",
    "set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../data/Recyclable and Household Waste Classification/images/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class WasteDataset(Dataset):\n",
    "    def __init__(self, root_dir, split, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for i, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for subfolder in ['default', 'real_world']:\n",
    "                subfolder_dir = os.path.join(class_dir, subfolder)\n",
    "                image_names = os.listdir(subfolder_dir)\n",
    "                random.shuffle(image_names)\n",
    "                \n",
    "                if split == 'train':\n",
    "                    image_names = image_names[:int(0.6 * len(image_names))]\n",
    "                elif split == 'val':\n",
    "                    image_names = image_names[int(0.6 * len(image_names)):int(0.8 * len(image_names))]\n",
    "                else:  # split == 'test'\n",
    "                    image_names = image_names[int(0.8 * len(image_names)):]\n",
    "                \n",
    "                for image_name in image_names:\n",
    "                    self.image_paths.append(os.path.join(subfolder_dir, image_name))\n",
    "                    self.labels.append(i)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        label = self.labels[index]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        label = F.one_hot(label, num_classes=30)\n",
    "           \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Create the datasets and data loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "train_dataset = WasteDataset(path_append + dataset_path, split='train', transform=transform)\n",
    "val_dataset = WasteDataset(path_append + dataset_path, split='val', transform=transform)\n",
    "test_dataset = WasteDataset(path_append + dataset_path, split='test', transform=transform)\n",
    "\n",
    "X, y = train_dataset[0]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_config = DataConfig(dataset_name = 'recycle_image', task_type='multi_label_classification', obs_shape=[3, 224, 224], label_size=30)\n",
    "\n",
    "#  Set training configuration from the AlgorithmConfig class, returning them as a Namespace object.\n",
    "ml_params = MLParameters(core_model = 'gpt', encoder_model = 'resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer_hub import TrainerHub\n",
    "\n",
    "# Set the device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Initialize the TrainerHub class with the training configuration, data configuration, device, and use_print and use_wandb flags\n",
    "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da6c3947f6042c681cc13dc7521aa49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7476696714cc4c23b208909ced445da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ccnets-team\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100][50/140][Time 43.10]\n",
      "Unified LR across all optimizers: 0.0001995308238189185\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  resnet\n",
      "Inf: 1.3029\tGen: 1.2072\tRec: 1.3920\tE: 1.1181\tR: 1.4877\tP: 1.2964\n",
      "Trainer:  gpt\n",
      "Inf: 0.2419\tGen: 0.5568\tRec: 0.5008\tE: 0.2979\tR: 0.1858\tP: 0.8157\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.4781\n",
      "\n",
      "[0/100][100/140][Time 41.53]\n",
      "Unified LR across all optimizers: 0.00019907191565870155\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  resnet\n",
      "Inf: 1.1712\tGen: 1.0934\tRec: 1.2027\tE: 1.0620\tR: 1.2804\tP: 1.1249\n",
      "Trainer:  gpt\n",
      "Inf: 0.0374\tGen: 0.3859\tRec: 0.3840\tE: 0.0393\tR: 0.0355\tP: 0.7325\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6176\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a57332f9fc4527bd2d56663e1b82e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100][10/140][Time 41.87]\n",
      "Unified LR across all optimizers: 0.00019861406295796434\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  resnet\n",
      "Inf: 1.1155\tGen: 1.0590\tRec: 1.1533\tE: 1.0212\tR: 1.2097\tP: 1.0968\n",
      "Trainer:  gpt\n",
      "Inf: 0.0431\tGen: 0.4393\tRec: 0.4349\tE: 0.0475\tR: 0.0387\tP: 0.8312\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.5712\n",
      "\n",
      "[1/100][60/140][Time 41.53]\n",
      "Unified LR across all optimizers: 0.00019815726328921765\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  resnet\n",
      "Inf: 1.0765\tGen: 1.0427\tRec: 1.1326\tE: 0.9866\tR: 1.1665\tP: 1.0988\n",
      "Trainer:  gpt\n",
      "Inf: 0.1032\tGen: 0.4885\tRec: 0.4720\tE: 0.1196\tR: 0.0868\tP: 0.8573\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8053\n",
      "\n",
      "[1/100][110/140][Time 41.56]\n",
      "Unified LR across all optimizers: 0.00019770151423055492\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  resnet\n",
      "Inf: 1.0512\tGen: 1.0274\tRec: 1.1129\tE: 0.9658\tR: 1.1367\tP: 1.0890\n",
      "Trainer:  gpt\n",
      "Inf: 0.0744\tGen: 0.3769\tRec: 0.3699\tE: 0.0813\tR: 0.0674\tP: 0.6724\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8909\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5f5eb7e1664d7c938dfab98aa5b984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/100][20/140][Time 41.74]\n",
      "Unified LR across all optimizers: 0.00019724681336564005\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  resnet\n",
      "Inf: 1.0484\tGen: 1.0242\tRec: 1.1056\tE: 0.9670\tR: 1.1298\tP: 1.0814\n",
      "Trainer:  gpt\n",
      "Inf: 0.0564\tGen: 0.3590\tRec: 0.3547\tE: 0.0607\tR: 0.0522\tP: 0.6573\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8870\n",
      "\n",
      "[2/100][70/140][Time 41.89]\n",
      "Unified LR across all optimizers: 0.00019679315828369438\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  resnet\n",
      "Inf: 1.0522\tGen: 1.0226\tRec: 1.1039\tE: 0.9709\tR: 1.1335\tP: 1.0742\n",
      "Trainer:  gpt\n",
      "Inf: 0.0598\tGen: 0.3738\tRec: 0.3690\tE: 0.0646\tR: 0.0550\tP: 0.6830\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8979\n",
      "\n",
      "[2/100][120/140][Time 41.89]\n",
      "Unified LR across all optimizers: 0.00019634054657948372\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  resnet\n",
      "Inf: 1.0422\tGen: 1.0110\tRec: 1.0946\tE: 0.9585\tR: 1.1258\tP: 1.0634\n",
      "Trainer:  gpt\n",
      "Inf: 0.0540\tGen: 0.3614\tRec: 0.3580\tE: 0.0575\tR: 0.0506\tP: 0.6654\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9098\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630e075cfa6641f8a4db6c91e5c33288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/100][30/140][Time 41.65]\n",
      "Unified LR across all optimizers: 0.00019588897585330582\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  resnet\n",
      "Inf: 1.0356\tGen: 1.0089\tRec: 1.0859\tE: 0.9586\tR: 1.1125\tP: 1.0592\n",
      "Trainer:  gpt\n",
      "Inf: 0.0484\tGen: 0.3556\tRec: 0.3531\tE: 0.0510\tR: 0.0459\tP: 0.6603\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8931\n",
      "\n",
      "[3/100][80/140][Time 41.56]\n",
      "Unified LR across all optimizers: 0.00019543844371097777\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  resnet\n",
      "Inf: 1.0405\tGen: 1.0130\tRec: 1.0888\tE: 0.9647\tR: 1.1162\tP: 1.0613\n",
      "Trainer:  gpt\n",
      "Inf: 0.0445\tGen: 0.3630\tRec: 0.3611\tE: 0.0464\tR: 0.0426\tP: 0.6797\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8988\n",
      "\n",
      "[3/100][130/140][Time 41.97]\n",
      "Unified LR across all optimizers: 0.00019498894776382288\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  resnet\n",
      "Inf: 1.0437\tGen: 1.0142\tRec: 1.0926\tE: 0.9653\tR: 1.1220\tP: 1.0631\n",
      "Trainer:  gpt\n",
      "Inf: 0.0434\tGen: 0.3682\tRec: 0.3669\tE: 0.0447\tR: 0.0420\tP: 0.6917\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9125\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f37b446d0b4e999fb50bf9ecd2fe9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/100][40/140][Time 41.82]\n",
      "Unified LR across all optimizers: 0.00019454048562865856\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  resnet\n",
      "Inf: 1.0342\tGen: 1.0093\tRec: 1.0854\tE: 0.9581\tR: 1.1104\tP: 1.0605\n",
      "Trainer:  gpt\n",
      "Inf: 0.0403\tGen: 0.3789\tRec: 0.3780\tE: 0.0413\tR: 0.0393\tP: 0.7166\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9116\n",
      "\n",
      "[4/100][90/140][Time 41.83]\n",
      "Unified LR across all optimizers: 0.00019409305492778308\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  resnet\n",
      "Inf: 1.0344\tGen: 1.0054\tRec: 1.0815\tE: 0.9583\tR: 1.1106\tP: 1.0525\n",
      "Trainer:  gpt\n",
      "Inf: 0.0377\tGen: 0.4065\tRec: 0.4053\tE: 0.0389\tR: 0.0365\tP: 0.7740\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9267\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8ce2b0e70e40d2b1fc1bae9e5f65b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/100][0/140][Time 41.91]\n",
      "Unified LR across all optimizers: 0.00019364665328896346\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  resnet\n",
      "Inf: 1.0452\tGen: 1.0080\tRec: 1.0835\tE: 0.9696\tR: 1.1207\tP: 1.0463\n",
      "Trainer:  gpt\n",
      "Inf: 0.0379\tGen: 0.4135\tRec: 0.4122\tE: 0.0392\tR: 0.0366\tP: 0.7878\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9199\n",
      "\n",
      "[5/100][50/140][Time 41.81]\n",
      "Unified LR across all optimizers: 0.00019320127834542263\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  resnet\n",
      "Inf: 1.0423\tGen: 1.0059\tRec: 1.0812\tE: 0.9669\tR: 1.1176\tP: 1.0448\n",
      "Trainer:  gpt\n",
      "Inf: 0.0406\tGen: 0.4170\tRec: 0.4155\tE: 0.0420\tR: 0.0391\tP: 0.7920\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer_hub.train(train_dataset, val_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
