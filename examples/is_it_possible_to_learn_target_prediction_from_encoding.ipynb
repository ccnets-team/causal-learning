{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Author:\n",
    "        \n",
    "        PARK, JunHo, junho@ccnets.org\n",
    "\n",
    "        \n",
    "        KIM, JoengYoong, jeongyoong@ccnets.org\n",
    "        \n",
    "    COPYRIGHT (c) 2024. CCNets. All Rights reserved.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is it Possible to Learn Target Prediction from Encoding? \n",
    "## Recyclable and Household Waste Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates the use of a cooperative encoding\n",
    "network for causal encoding. The pipeline involves an encoding\n",
    "model (cooperative encoding network with three ResNets) and\n",
    "a core model (three GPTs). It showcases how the encoding\n",
    "process and prediction learning happen internally in the API,\n",
    "as illustrated in the example file.\n",
    "\n",
    "## Problem Definition\n",
    "\n",
    "In many machine learning tasks, effectively encoding data so that\n",
    "the predictive model can learn the underlying patterns and\n",
    "relationships is challenging. Traditional encoding methods, such\n",
    "as autoencoders, often fail to capture the causal relationships\n",
    "and inherent attributes of the data effectively. These methods\n",
    "typically compress the data stochastically, requiring a decoding\n",
    "step with the paired decoder used in training to retrieve the\n",
    "original data before further prediction learning can occur.\n",
    "\n",
    "### Key Issues with Traditional Encoding:\n",
    "1. **Need for Decoding:** Further prediction learning from\n",
    "   compressed data (e.g., large language model learning)\n",
    "   necessitates a decoder.\n",
    "2. **Paired Decoding:** The decoder must be trained together\n",
    "   with the encoder to effectively reconstruct the original data.\n",
    "\n",
    "In contrast, the causal encoding framework directly addresses these\n",
    "issues by uncovering and manipulating independent causal factors\n",
    "and common attributes in dataset observations. This allows for\n",
    "a more structured and meaningful representation of the data,\n",
    "facilitating accurate prediction learning without the need\n",
    "for intermediate decoding steps.\n",
    "\n",
    "### Advantages of Causal Encoding Framework:\n",
    "- **Independent of Prediction Model:** The cooperative encoding\n",
    "  network does not require training or interaction with the\n",
    "  prediction model.\n",
    "- **Structured Representation:** Utilizes both stochastic and\n",
    "  deterministic variables to capture all causal factors in the\n",
    "  latent representation, which is a concatenation of the Explainer\n",
    "  and Reasoner outputs in the encoding network.\n",
    "\n",
    "This structured approach allows the model to learn from the encoded\n",
    "data and make accurate predictions more efficiently.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "[Recyclable and Household Waste Classification](https://www.kaggle.com/datasets/alistairking/recyclable-and-household-waste-classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "path_append = \"../\" # Go up one directory from where you are.\n",
    "sys.path.append(path_append) \n",
    "\n",
    "from tools.setting.ml_params import MLParameters\n",
    "from tools.setting.data_config import DataConfig\n",
    "from nn.utils.init import set_random_seed\n",
    "set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../data/Recyclable and Household Waste Classification/images/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class WasteDataset_for_encoder(Dataset):\n",
    "    def __init__(self, root_dir, split, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        for i, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for subfolder in ['default', 'real_world']:\n",
    "                subfolder_dir = os.path.join(class_dir, subfolder)\n",
    "                if os.path.exists(subfolder_dir):\n",
    "                    image_names = os.listdir(subfolder_dir)\n",
    "                    random.shuffle(image_names)\n",
    "                    for image_name in image_names:\n",
    "                        self.image_paths.append(os.path.join(subfolder_dir, image_name))\n",
    "                        self.labels.append(i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, None\n",
    "    \n",
    "    \n",
    "class WasteDataset_for_core(Dataset):\n",
    "    def __init__(self, trainer_hub1, root_dir, split, transform=None, device='cuda', precompute_batches=64):\n",
    "        self.encoder = trainer_hub1.helper\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.device = device\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.precompute_batches = precompute_batches\n",
    "        self.dataset_length = 0\n",
    "        self.X_cache = None\n",
    "        self.y_cache = None\n",
    "        self.total_iters = 0\n",
    "        self.batch_indices = []\n",
    "\n",
    "        # Loop through each class and subfolder to load image paths and labels\n",
    "        for i, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for subfolder in ['default', 'real_world']:\n",
    "                subfolder_dir = os.path.join(class_dir, subfolder)\n",
    "                if os.path.exists(subfolder_dir):\n",
    "                    image_names = os.listdir(subfolder_dir)\n",
    "                    random.shuffle(image_names)\n",
    "                    image_names = self._split_images(image_names, split)\n",
    "                    for image_name in image_names:\n",
    "                        self.image_paths.append(os.path.join(subfolder_dir, image_name))\n",
    "                        self.labels.append(i)\n",
    "        \n",
    "        self.dataset_length = len(self.image_paths)\n",
    "        self._shuffle_indices()\n",
    "        self._precompute_batches(0)\n",
    "\n",
    "    # Helper function to split images into train, validation, or test sets\n",
    "    def _split_images(self, image_names, split):\n",
    "        if split == 'train':\n",
    "            return image_names[:int(0.6 * len(image_names))]\n",
    "        elif split == 'val':\n",
    "            return image_names[int(0.6 * len(image_names)):int(0.8 * len(image_names))]\n",
    "        else:\n",
    "            return image_names[int(0.8 * len(image_names)):]\n",
    "        \n",
    "    # Shuffle the indices of the images to randomize batch creation\n",
    "    def _shuffle_indices(self):\n",
    "        self.batch_indices = torch.randperm(self.dataset_length)\n",
    "\n",
    "    # Precompute batches by encoding a set of images\n",
    "    def _precompute_batches(self, start_idx):\n",
    "        end_idx = min(start_idx + self.precompute_batches, self.dataset_length)\n",
    "        batch_indices = self.batch_indices[start_idx:end_idx].tolist()\n",
    "        \n",
    "        # Load and transform images\n",
    "        images = [Image.open(self.image_paths[i]).convert('RGB') for i in batch_indices]\n",
    "        if self.transform:\n",
    "            images = [self.transform(img) for img in images]\n",
    "\n",
    "        images = torch.stack(images).to(self.device)\n",
    "        \n",
    "        # Convert labels to tensors and one-hot encode\n",
    "        labels = torch.tensor([self.labels[i] for i in batch_indices], dtype=torch.long)\n",
    "        labels = F.one_hot(labels, num_classes=len(self.classes)).to(self.device)\n",
    "        \n",
    "        # Encode images using the encoder and store results in cache\n",
    "        with torch.no_grad():\n",
    "            codes, labels = self.encoder.encode_inputs(images, labels)  # Assuming the encoder method processes batched inputs\n",
    "        \n",
    "        # Store precomputed batch in cache\n",
    "        self.X_cache = codes\n",
    "        self.y_cache = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_idx = idx // self.precompute_batches\n",
    "        batch_start_idx = batch_idx * self.precompute_batches\n",
    "        cur_idx = idx % self.precompute_batches\n",
    "        \n",
    "        if self.total_iters % self.dataset_length == 0:\n",
    "            self._shuffle_indices()\n",
    "        if self.total_iters % self.precompute_batches == 0:\n",
    "            self._precompute_batches(batch_start_idx)\n",
    "        \n",
    "        self.total_iters += 1\n",
    "        \n",
    "        if cur_idx >= len(self.X_cache):\n",
    "            cur_idx = idx % len(self.X_cache)\n",
    "\n",
    "        return self.X_cache[cur_idx], self.y_cache[cur_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Create the datasets and data loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset for encoder\n",
    "train_dataset_for_encoder = WasteDataset_for_encoder(path_append + dataset_path, split='train', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = DataConfig(dataset_name = 'recycle_image', task_type='multi_label_classification', obs_shape=[3, 224, 224], label_size=30)\n",
    "\n",
    "#  Set training configuration from the MLParameters class, returning them as a Namespace object.\n",
    "\n",
    "# Set the training parameters for the `encoder model`\n",
    "ml_params1 = MLParameters(core_model = 'none', encoder_model = 'resnet')\n",
    "\n",
    "# Set the training parameters for the `core model`\n",
    "ml_params2 = MLParameters(core_model = 'gpt', encoder_model = 'none')\n",
    "\n",
    "ml_params1.training.num_epoch = 1\n",
    "ml_params2.training.num_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer_hub import TrainerHub\n",
    "\n",
    "# Set the device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Initialize 2 TrainerHub class with the training configuration, data configuration, device, and use_print and use_wandb flags\n",
    "\n",
    "# trainer_hub1 is for the encoder model\n",
    "trainer_hub1 = TrainerHub(ml_params1, data_config, device, use_print=True, use_wandb=False, print_interval=20)\n",
    "# trainer_hub2 is for the core model\n",
    "trainer_hub2 = TrainerHub(ml_params2, data_config, device, use_print=True, use_wandb=False, print_interval=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use function `encode_inputs` in `trainer_hub1.helper` \n",
    "encoder = trainer_hub1\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_dataset_for_core = WasteDataset_for_core(encoder, path_append + dataset_path, split='train', transform=transform)\n",
    "val_dataset_for_core = WasteDataset_for_core(encoder, path_append + dataset_path, split='val', transform=transform)\n",
    "test_dataset_for_core = WasteDataset_for_core(encoder, path_append + dataset_path, split='test', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(\"=\"*10,\"Encoder Epoch\", i,\"=\"*10)\n",
    "    # Train the encoder models with the encoder dataset\n",
    "    trainer_hub1.train(train_dataset_for_encoder)\n",
    "    \n",
    "    print(\"=\"*10,\"Core Epoch\", i,\"=\"*10)\n",
    "    # Train the core models with encoded inputs\n",
    "    trainer_hub2.train(train_dataset_for_core, val_dataset_for_core)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
