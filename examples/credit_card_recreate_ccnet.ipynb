{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author:\n",
    "        \n",
    "        PARK, JunHo, junho@ccnets.org\n",
    "\n",
    "        \n",
    "        KIM, JeongYoong, jeongyoong@ccnets.org\n",
    "        \n",
    "    COPYRIGHT (c) 2024. CCNets. All Rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection: Handling Imbalanced Dataset with CCNet\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial explores the use of a Cooperative Encoding Network (CCNet) to address challenges associated with imbalanced datasets in the domain of credit card fraud detection. By leveraging the power of synthetic data generation, we aim to enhance the diversity and volume of training data, thereby improving the robustness and accuracy of models designed to identify fraudulent transactions.\n",
    "\n",
    "## Tutorial Goals\n",
    "\n",
    "The objectives of this tutorial are designed to guide you through the process of enhancing data quality and model performance:\n",
    "\n",
    "### Dataset Recreation with CCNet\n",
    "- **Understand Data Augmentation**: Learn how encoding techniques can be used to generate synthetic data instances that closely mimic the characteristics of real-world fraudulent and non-fraudulent transactions.\n",
    "- **Impact on Model Training**: Assess how augmenting the dataset influences the training process and subsequently, the model's ability to generalize from training to real-world scenarios.\n",
    "\n",
    "### Model Training and Evaluation\n",
    "- **Dual Model Training**: Train two distinct models to directly compare performance metrics:\n",
    "  - A model trained on the **original dataset**.\n",
    "  - A model trained on the **CCNet-augmented dataset**.\n",
    "- **Performance Metrics**: Use the F1 score, a critical measure for models operating on imbalanced datasets, to evaluate and compare the effectiveness of these models.\n",
    "\n",
    "### Testing and Validation\n",
    "- **Independent Model Testing**: Conduct a thorough evaluation of both models using a standalone test set that was not involved in the training phase.\n",
    "- **Objective Analysis**: Critically analyze the outcomes to validate whether data augmentation through CCNet offers a tangible benefit in detecting credit card fraud.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By the end of this tutorial, participants will not only grasp the theoretical underpinnings of using synthetic data to combat data imbalance but also gain hands-on experience in applying these concepts through CCNet to potentially enhance model performance in fraud detection tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path_append = \"../\"\n",
    "sys.path.append(path_append)  # Go up one directory from where you are.\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataroot = path_append + \"../data/credit_card_fraud_detection/creditcard.csv\"\n",
    "df = pd.read_csv(dataroot)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.83 %of the dataset\n",
      "Frauds 0.17 %of the dataset\n"
     ]
    }
   ],
   "source": [
    "print('No Frauds', round(df['Class'].value_counts()[0] / len(df) *100,2), '%of the dataset')\n",
    "print('Frauds', round(df['Class'].value_counts()[1] / len(df) *100,2), '%of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://fraud-detection-handbook.github.io/fraud-detection-handbook/Chapter_7_DeepLearning/FeedForwardNeuralNetworks.html\n",
    "class LabeledDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        vals = torch.tensor(self.x[index], dtype = torch.float32)\n",
    "        label = torch.tensor(self.y[index], dtype = torch.float32)\n",
    "        return vals, label\n",
    "\n",
    "class UnlabelledDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        vals = torch.tensor(self.x[index], dtype = torch.float32)\n",
    "        return vals, None\n",
    "\n",
    "sc = StandardScaler()\n",
    "df.iloc[:, :-1] = sc.fit_transform(df.iloc[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features\n",
    "n_elements = df.shape[1]\n",
    "# number of label classes\n",
    "# n_classes = y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Setup and Model Configuration\n",
    "\n",
    "This section initializes the environment by setting a fixed random seed to ensure reproducibility of results. It imports necessary configurations and initializes model parameters with specific configurations. The model specified here is set to have no core model but uses a 'deepfm' encoder model for data processing, which is particularly tailored for structured or tabular data like credit card transactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\junho\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\junho\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility of experiments\n",
    "from nn.utils.init import set_random_seed\n",
    "set_random_seed(0)\n",
    "\n",
    "# Importing configuration setups for ML parameters and data\n",
    "from tools.setting.ml_params import MLParameters\n",
    "from tools.setting.data_config import DataConfig\n",
    "from trainer_hub import TrainerHub\n",
    "\n",
    "# Configuration for the data handling, defining dataset specifics and the task type\n",
    "data_config = DataConfig(dataset_name='CreditCardFraudDetection', task_type='augmentation', obs_shape=[n_elements], label_size=None)\n",
    "\n",
    "# Initializing ML parameters without a core model and setting the encoder model to 'deepfm' with specific configurations\n",
    "ml_params = MLParameters(core_model='none', encoder_model='deepfm')\n",
    "ml_params.encoder_config.num_layers = 4\n",
    "ml_params.encoder_config.d_model = 256\n",
    "ml_params.encoder_config.dropout = 0.0\n",
    "\n",
    "# Setting training parameters and device configuration\n",
    "ml_params.training.num_epoch = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Create a TrainerHub instance to manage training and data processing\n",
    "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb=False, use_full_eval=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Splitting for Training and Testing\n",
    "\n",
    "The original dataset is split into training and testing parts to evaluate the model's performance accurately. This step is crucial for validating the effectiveness of the training on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and test sets for model evaluation\n",
    "df_train, df_test = train_test_split(df, test_size=0.5, shuffle=False)\n",
    "X_train, y_train = df_train.iloc[:, :-1].values, df_train.iloc[:, -1:].values\n",
    "X_test, y_test = df_test.iloc[:, :-1].values, df_test.iloc[:, -1:].values\n",
    "\n",
    "# Preparing the unlabelled and labelled datasets for use in training and testing\n",
    "_df_train = df_train.iloc[:, :].values \n",
    "unlabelled_trainset = UnlabelledDataset(_df_train)\n",
    "trainset = LabeledDataset(X_test, y_test)\n",
    "testset = LabeledDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4061ca06514ea9b14342bf12f9e881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6019e7143b5439ebdb6de028222709b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/2225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/2][50/2225][Time 1.82]\n",
      "Unified LR across all optimizers: 0.0001995308238189185\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.5556\tGen: 0.5464\tRec: 0.5939\tE: 0.5082\tR: 0.6031\tP: 0.5847\n",
      "[0/2][100/2225][Time 1.54]\n",
      "Unified LR across all optimizers: 0.00019907191565870155\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.5095\tGen: 0.4888\tRec: 0.5415\tE: 0.4569\tR: 0.5621\tP: 0.5208\n",
      "[0/2][150/2225][Time 1.49]\n",
      "Unified LR across all optimizers: 0.00019861406295796434\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.4601\tGen: 0.4454\tRec: 0.4898\tE: 0.4156\tR: 0.5045\tP: 0.4752\n",
      "[0/2][200/2225][Time 1.54]\n",
      "Unified LR across all optimizers: 0.00019815726328921765\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.4106\tGen: 0.4098\tRec: 0.4379\tE: 0.3825\tR: 0.4387\tP: 0.4371\n",
      "[0/2][250/2225][Time 1.55]\n",
      "Unified LR across all optimizers: 0.00019770151423055492\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.3607\tGen: 0.3696\tRec: 0.3833\tE: 0.3470\tR: 0.3745\tP: 0.3922\n",
      "[0/2][300/2225][Time 1.56]\n",
      "Unified LR across all optimizers: 0.00019724681336564005\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.3291\tGen: 0.3454\tRec: 0.3478\tE: 0.3267\tR: 0.3314\tP: 0.3642\n",
      "[0/2][350/2225][Time 1.64]\n",
      "Unified LR across all optimizers: 0.00019679315828369438\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.3028\tGen: 0.3247\tRec: 0.3215\tE: 0.3061\tR: 0.2995\tP: 0.3434\n",
      "[0/2][400/2225][Time 1.61]\n",
      "Unified LR across all optimizers: 0.00019634054657948372\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.2907\tGen: 0.3134\tRec: 0.3101\tE: 0.2939\tR: 0.2874\tP: 0.3328\n",
      "[0/2][450/2225][Time 1.68]\n",
      "Unified LR across all optimizers: 0.00019588897585330582\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.2654\tGen: 0.2897\tRec: 0.2817\tE: 0.2734\tR: 0.2574\tP: 0.3060\n",
      "[0/2][500/2225][Time 1.74]\n",
      "Unified LR across all optimizers: 0.00019543844371097777\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.2596\tGen: 0.2839\tRec: 0.2755\tE: 0.2680\tR: 0.2512\tP: 0.2998\n",
      "[0/2][550/2225][Time 1.69]\n",
      "Unified LR across all optimizers: 0.00019498894776382288\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.2409\tGen: 0.2642\tRec: 0.2570\tE: 0.2482\tR: 0.2336\tP: 0.2803\n",
      "[0/2][600/2225][Time 1.69]\n",
      "Unified LR across all optimizers: 0.00019454048562865856\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.2389\tGen: 0.2616\tRec: 0.2533\tE: 0.2472\tR: 0.2306\tP: 0.2760\n",
      "[0/2][650/2225][Time 1.84]\n",
      "Unified LR across all optimizers: 0.00019409305492778308\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.2265\tGen: 0.2503\tRec: 0.2404\tE: 0.2363\tR: 0.2166\tP: 0.2643\n",
      "[0/2][700/2225][Time 1.71]\n",
      "Unified LR across all optimizers: 0.00019364665328896346\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.2148\tGen: 0.2364\tRec: 0.2287\tE: 0.2225\tR: 0.2070\tP: 0.2503\n",
      "[0/2][750/2225][Time 1.78]\n",
      "Unified LR across all optimizers: 0.00019320127834542263\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.2147\tGen: 0.2352\tRec: 0.2292\tE: 0.2207\tR: 0.2088\tP: 0.2496\n",
      "[0/2][800/2225][Time 1.66]\n",
      "Unified LR across all optimizers: 0.00019275692773582703\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1972\tGen: 0.2188\tRec: 0.2107\tE: 0.2052\tR: 0.1891\tP: 0.2323\n",
      "[0/2][850/2225][Time 1.73]\n",
      "Unified LR across all optimizers: 0.0001923135991042739\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1950\tGen: 0.2147\tRec: 0.2090\tE: 0.2008\tR: 0.1892\tP: 0.2287\n",
      "[0/2][900/2225][Time 1.84]\n",
      "Unified LR across all optimizers: 0.0001918712901002789\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1933\tGen: 0.2106\tRec: 0.2077\tE: 0.1962\tR: 0.1904\tP: 0.2251\n",
      "[0/2][950/2225][Time 1.68]\n",
      "Unified LR across all optimizers: 0.00019142999837876384\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1785\tGen: 0.1960\tRec: 0.1927\tE: 0.1818\tR: 0.1752\tP: 0.2103\n",
      "[0/2][1000/2225][Time 1.82]\n",
      "Unified LR across all optimizers: 0.00019098972160004388\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1877\tGen: 0.2027\tRec: 0.2022\tE: 0.1882\tR: 0.1872\tP: 0.2171\n",
      "[0/2][1050/2225][Time 1.75]\n",
      "Unified LR across all optimizers: 0.00019055045742981543\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1773\tGen: 0.1911\tRec: 0.1909\tE: 0.1775\tR: 0.1771\tP: 0.2048\n",
      "[0/2][1100/2225][Time 1.70]\n",
      "Unified LR across all optimizers: 0.00019011220353914353\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1634\tGen: 0.1784\tRec: 0.1766\tE: 0.1652\tR: 0.1616\tP: 0.1916\n",
      "[0/2][1150/2225][Time 1.85]\n",
      "Unified LR across all optimizers: 0.00018967495760444968\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1622\tGen: 0.1755\tRec: 0.1761\tE: 0.1617\tR: 0.1628\tP: 0.1893\n",
      "[0/2][1200/2225][Time 1.77]\n",
      "Unified LR across all optimizers: 0.00018923871730749947\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1664\tGen: 0.1769\tRec: 0.1790\tE: 0.1642\tR: 0.1685\tP: 0.1895\n",
      "[0/2][1250/2225][Time 1.79]\n",
      "Unified LR across all optimizers: 0.00018880348033539028\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1758\tGen: 0.1850\tRec: 0.1895\tE: 0.1713\tR: 0.1803\tP: 0.1987\n",
      "[0/2][1300/2225][Time 1.69]\n",
      "Unified LR across all optimizers: 0.00018836924438053897\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1462\tGen: 0.1576\tRec: 0.1583\tE: 0.1455\tR: 0.1469\tP: 0.1697\n",
      "[0/2][1350/2225][Time 1.70]\n",
      "Unified LR across all optimizers: 0.0001879360071406698\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1618\tGen: 0.1710\tRec: 0.1742\tE: 0.1586\tR: 0.1651\tP: 0.1834\n",
      "[0/2][1400/2225][Time 1.77]\n",
      "Unified LR across all optimizers: 0.000187503766318802\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1530\tGen: 0.1619\tRec: 0.1659\tE: 0.1490\tR: 0.1570\tP: 0.1748\n",
      "[0/2][1450/2225][Time 1.68]\n",
      "Unified LR across all optimizers: 0.00018707251962323787\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1426\tGen: 0.1534\tRec: 0.1545\tE: 0.1414\tR: 0.1437\tP: 0.1653\n",
      "[0/2][1500/2225][Time 1.74]\n",
      "Unified LR across all optimizers: 0.0001866422647675502\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1423\tGen: 0.1513\tRec: 0.1542\tE: 0.1394\tR: 0.1453\tP: 0.1632\n",
      "[0/2][1550/2225][Time 1.74]\n",
      "Unified LR across all optimizers: 0.00018621299947057073\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1461\tGen: 0.1541\tRec: 0.1575\tE: 0.1427\tR: 0.1496\tP: 0.1655\n",
      "[0/2][1600/2225][Time 1.72]\n",
      "Unified LR across all optimizers: 0.00018578472145637737\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1494\tGen: 0.1570\tRec: 0.1613\tE: 0.1452\tR: 0.1537\tP: 0.1688\n",
      "[0/2][1650/2225][Time 1.77]\n",
      "Unified LR across all optimizers: 0.00018535742845428288\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1369\tGen: 0.1455\tRec: 0.1480\tE: 0.1344\tR: 0.1393\tP: 0.1566\n",
      "[0/2][1700/2225][Time 1.76]\n",
      "Unified LR across all optimizers: 0.00018493111819882223\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1295\tGen: 0.1387\tRec: 0.1405\tE: 0.1277\tR: 0.1313\tP: 0.1497\n",
      "[0/2][1750/2225][Time 1.87]\n",
      "Unified LR across all optimizers: 0.00018450578842974107\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1287\tGen: 0.1368\tRec: 0.1392\tE: 0.1263\tR: 0.1311\tP: 0.1473\n",
      "[0/2][1800/2225][Time 1.93]\n",
      "Unified LR across all optimizers: 0.00018408143689198318\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1274\tGen: 0.1354\tRec: 0.1377\tE: 0.1250\tR: 0.1297\tP: 0.1457\n",
      "[0/2][1850/2225][Time 1.80]\n",
      "Unified LR across all optimizers: 0.0001836580613356789\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1287\tGen: 0.1376\tRec: 0.1396\tE: 0.1266\tR: 0.1307\tP: 0.1486\n",
      "[0/2][1900/2225][Time 1.65]\n",
      "Unified LR across all optimizers: 0.0001832356595161332\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1307\tGen: 0.1379\tRec: 0.1411\tE: 0.1276\tR: 0.1338\tP: 0.1483\n",
      "[0/2][1950/2225][Time 1.86]\n",
      "Unified LR across all optimizers: 0.00018281422919381367\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1256\tGen: 0.1333\tRec: 0.1358\tE: 0.1231\tR: 0.1282\tP: 0.1434\n",
      "[0/2][2000/2225][Time 1.87]\n",
      "Unified LR across all optimizers: 0.00018239376813433867\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1179\tGen: 0.1267\tRec: 0.1278\tE: 0.1168\tR: 0.1190\tP: 0.1365\n",
      "[0/2][2050/2225][Time 1.70]\n",
      "Unified LR across all optimizers: 0.00018197427410846564\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1228\tGen: 0.1310\tRec: 0.1328\tE: 0.1209\tR: 0.1247\tP: 0.1410\n",
      "[0/2][2100/2225][Time 1.83]\n",
      "Unified LR across all optimizers: 0.00018155574489207887\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1259\tGen: 0.1334\tRec: 0.1355\tE: 0.1238\tR: 0.1280\tP: 0.1430\n",
      "[0/2][2150/2225][Time 1.86]\n",
      "Unified LR across all optimizers: 0.00018113817826617823\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1219\tGen: 0.1289\tRec: 0.1316\tE: 0.1192\tR: 0.1246\tP: 0.1387\n",
      "[0/2][2200/2225][Time 1.88]\n",
      "Unified LR across all optimizers: 0.00018072157201686696\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1170\tGen: 0.1246\tRec: 0.1267\tE: 0.1149\tR: 0.1190\tP: 0.1343\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ded56496a364840822dcd4f696365e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/2225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2][25/2225][Time 2.73]\n",
      "Unified LR across all optimizers: 0.00018030592393534033\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1172\tGen: 0.1250\tRec: 0.1265\tE: 0.1157\tR: 0.1187\tP: 0.1342\n",
      "[1/2][75/2225][Time 5.82]\n",
      "Unified LR across all optimizers: 0.0001798912318178735\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1158\tGen: 0.1232\tRec: 0.1249\tE: 0.1141\tR: 0.1175\tP: 0.1323\n",
      "[1/2][125/2225][Time 4.54]\n",
      "Unified LR across all optimizers: 0.00017947749346581006\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1118\tGen: 0.1198\tRec: 0.1209\tE: 0.1107\tR: 0.1128\tP: 0.1289\n",
      "[1/2][175/2225][Time 2.65]\n",
      "Unified LR across all optimizers: 0.0001790647066855505\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1183\tGen: 0.1261\tRec: 0.1279\tE: 0.1165\tR: 0.1201\tP: 0.1357\n",
      "[1/2][225/2225][Time 1.77]\n",
      "Unified LR across all optimizers: 0.00017865286928854052\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1163\tGen: 0.1237\tRec: 0.1256\tE: 0.1144\tR: 0.1183\tP: 0.1329\n",
      "[1/2][275/2225][Time 1.93]\n",
      "Unified LR across all optimizers: 0.00017824197909125899\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1081\tGen: 0.1161\tRec: 0.1168\tE: 0.1073\tR: 0.1088\tP: 0.1249\n",
      "[1/2][325/2225][Time 2.03]\n",
      "Unified LR across all optimizers: 0.00017783203391520723\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1046\tGen: 0.1124\tRec: 0.1130\tE: 0.1040\tR: 0.1052\tP: 0.1208\n",
      "[1/2][375/2225][Time 2.01]\n",
      "Unified LR across all optimizers: 0.00017742303158689668\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1088\tGen: 0.1162\tRec: 0.1172\tE: 0.1078\tR: 0.1098\tP: 0.1247\n",
      "[1/2][425/2225][Time 1.88]\n",
      "Unified LR across all optimizers: 0.00017701496993783762\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1021\tGen: 0.1093\tRec: 0.1105\tE: 0.1009\tR: 0.1033\tP: 0.1178\n",
      "[1/2][475/2225][Time 6.01]\n",
      "Unified LR across all optimizers: 0.00017660784680452796\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1033\tGen: 0.1115\tRec: 0.1118\tE: 0.1031\tR: 0.1036\tP: 0.1199\n",
      "[1/2][525/2225][Time 1.88]\n",
      "Unified LR across all optimizers: 0.0001762016600284412\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1118\tGen: 0.1185\tRec: 0.1204\tE: 0.1099\tR: 0.1138\tP: 0.1271\n",
      "[1/2][575/2225][Time 1.95]\n",
      "Unified LR across all optimizers: 0.00017579640745601563\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1059\tGen: 0.1123\tRec: 0.1140\tE: 0.1042\tR: 0.1076\tP: 0.1204\n",
      "[1/2][625/2225][Time 1.95]\n",
      "Unified LR across all optimizers: 0.0001753920869386423\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1077\tGen: 0.1137\tRec: 0.1160\tE: 0.1055\tR: 0.1099\tP: 0.1220\n",
      "[1/2][675/2225][Time 1.88]\n",
      "Unified LR across all optimizers: 0.0001749886963326542\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1059\tGen: 0.1128\tRec: 0.1137\tE: 0.1050\tR: 0.1069\tP: 0.1205\n",
      "[1/2][725/2225][Time 1.99]\n",
      "Unified LR across all optimizers: 0.0001745862334993144\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1010\tGen: 0.1083\tRec: 0.1090\tE: 0.1004\tR: 0.1017\tP: 0.1163\n",
      "[1/2][775/2225][Time 1.91]\n",
      "Unified LR across all optimizers: 0.00017418469630480507\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1063\tGen: 0.1122\tRec: 0.1144\tE: 0.1041\tR: 0.1085\tP: 0.1203\n",
      "[1/2][825/2225][Time 1.67]\n",
      "Unified LR across all optimizers: 0.00017378408262021616\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1049\tGen: 0.1112\tRec: 0.1123\tE: 0.1039\tR: 0.1060\tP: 0.1185\n",
      "[1/2][875/2225][Time 1.77]\n",
      "Unified LR across all optimizers: 0.00017338439032153356\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1039\tGen: 0.1101\tRec: 0.1118\tE: 0.1022\tR: 0.1056\tP: 0.1179\n",
      "[1/2][925/2225][Time 1.77]\n",
      "Unified LR across all optimizers: 0.00017298561728962847\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.0989\tGen: 0.1048\tRec: 0.1065\tE: 0.0972\tR: 0.1005\tP: 0.1124\n",
      "[1/2][975/2225][Time 4.78]\n",
      "Unified LR across all optimizers: 0.00017258776141024598\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.0906\tGen: 0.0986\tRec: 0.0974\tE: 0.0917\tR: 0.0895\tP: 0.1054\n",
      "[1/2][1025/2225][Time 2.95]\n",
      "Unified LR across all optimizers: 0.00017219082057399394\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.0958\tGen: 0.1034\tRec: 0.1031\tE: 0.0961\tR: 0.0955\tP: 0.1107\n",
      "[1/2][1075/2225][Time 1.86]\n",
      "Unified LR across all optimizers: 0.00017179479267633146\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.1001\tGen: 0.1069\tRec: 0.1073\tE: 0.0996\tR: 0.1005\tP: 0.1141\n",
      "[1/2][1125/2225][Time 1.71]\n",
      "Unified LR across all optimizers: 0.00017139967561755819\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.0993\tGen: 0.1052\tRec: 0.1065\tE: 0.0980\tR: 0.1006\tP: 0.1125\n",
      "[1/2][1175/2225][Time 1.80]\n",
      "Unified LR across all optimizers: 0.00017100546730280274\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.0943\tGen: 0.1011\tRec: 0.1014\tE: 0.0940\tR: 0.0946\tP: 0.1082\n",
      "[1/2][1225/2225][Time 1.83]\n",
      "Unified LR across all optimizers: 0.00017061216564201177\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.0992\tGen: 0.1053\tRec: 0.1065\tE: 0.0980\tR: 0.1004\tP: 0.1127\n",
      "[1/2][1275/2225][Time 1.72]\n",
      "Unified LR across all optimizers: 0.0001702197685499392\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.0898\tGen: 0.0969\tRec: 0.0967\tE: 0.0901\tR: 0.0896\tP: 0.1037\n",
      "[1/2][1325/2225][Time 1.79]\n",
      "Unified LR across all optimizers: 0.0001698282739461345\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.0888\tGen: 0.0955\tRec: 0.0960\tE: 0.0883\tR: 0.0893\tP: 0.1027\n",
      "[1/2][1375/2225][Time 1.70]\n",
      "Unified LR across all optimizers: 0.00016943767975493242\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.0919\tGen: 0.0989\tRec: 0.0990\tE: 0.0918\tR: 0.0921\tP: 0.1059\n",
      "[1/2][1425/2225][Time 1.80]\n",
      "Unified LR across all optimizers: 0.0001690479839054414\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.0938\tGen: 0.0997\tRec: 0.1010\tE: 0.0925\tR: 0.0951\tP: 0.1069\n",
      "[1/2][1475/2225][Time 1.66]\n",
      "Unified LR across all optimizers: 0.00016865918433153277\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.0900\tGen: 0.0967\tRec: 0.0966\tE: 0.0901\tR: 0.0899\tP: 0.1033\n",
      "[1/2][1525/2225][Time 1.81]\n",
      "Unified LR across all optimizers: 0.00016827127897182985\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.0920\tGen: 0.0985\tRec: 0.0988\tE: 0.0917\tR: 0.0923\tP: 0.1052\n",
      "[1/2][1575/2225][Time 1.78]\n",
      "Unified LR across all optimizers: 0.0001678842657696972\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.0914\tGen: 0.0978\tRec: 0.0981\tE: 0.0910\tR: 0.0917\tP: 0.1046\n",
      "[1/2][1625/2225][Time 1.67]\n",
      "Unified LR across all optimizers: 0.00016749814267322938\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.0943\tGen: 0.1006\tRec: 0.1010\tE: 0.0939\tR: 0.0947\tP: 0.1072\n",
      "[1/2][1675/2225][Time 1.64]\n",
      "Unified LR across all optimizers: 0.00016711290763524007\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.0876\tGen: 0.0938\tRec: 0.0950\tE: 0.0865\tR: 0.0888\tP: 0.1012\n",
      "[1/2][1725/2225][Time 1.94]\n",
      "Unified LR across all optimizers: 0.00016672855861325146\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.0908\tGen: 0.0974\tRec: 0.0967\tE: 0.0914\tR: 0.0901\tP: 0.1033\n",
      "[1/2][1775/2225][Time 1.87]\n",
      "Unified LR across all optimizers: 0.00016634509356948314\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.0784\tGen: 0.0860\tRec: 0.0850\tE: 0.0794\tR: 0.0773\tP: 0.0927\n",
      "[1/2][1825/2225][Time 1.67]\n",
      "Unified LR across all optimizers: 0.00016596251047084197\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.0866\tGen: 0.0930\tRec: 0.0930\tE: 0.0866\tR: 0.0865\tP: 0.0994\n",
      "[1/2][1875/2225][Time 1.81]\n",
      "Unified LR across all optimizers: 0.00016558080728890993\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  deepfm\n",
      "Inf: 0.0872\tGen: 0.0937\tRec: 0.0934\tE: 0.0875\tR: 0.0868\tP: 0.0999\n"
     ]
    }
   ],
   "source": [
    "trainer_hub.train(unlabelled_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Lower than the original batch size\n",
    "# Use DataLoader to handle smaller batches\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def collate_fn(batch):\n",
    "    X, y = zip(*batch)\n",
    "    # Directly use the tensors from X if they are already tensors, else convert appropriately\n",
    "    X_padded = pad_sequence([x.clone().detach() if isinstance(x, torch.Tensor) else torch.tensor(x) for x in X], batch_first=True, padding_value=0)\n",
    "    \n",
    "    if any(label is None for label in y):\n",
    "        y_padded = None\n",
    "    else:\n",
    "        # Directly use the tensors from y if they are already tensors, else convert appropriately\n",
    "        y_padded = pad_sequence([label.clone().detach() if isinstance(label, torch.Tensor) else torch.tensor(label) for label in y], batch_first=True, padding_value=-1)\n",
    "    \n",
    "    return X_padded, y_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading and Synthetic Data Generation\n",
    "\n",
    "This section deals with loading the unlabelled dataset, processing it through the trained model to create synthetic data. This data augmentation step is crucial for models that benefit from larger datasets, such as in fraud detection scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the unlabelled data and preparing it for processing222\n",
    "train_loader = torch.utils.data.DataLoader(dataset=unlabelled_trainset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "# Generate synthetic data through the model to augment the training dataset\n",
    "recreated_dataset = None\n",
    "with torch.no_grad():\n",
    "    for data, _ in train_loader:\n",
    "        data = data.to(device)\n",
    "        batch_recreated_data = trainer_hub.encoder_ccnet.synthesize(data, output_multiplier=2)\n",
    "        recreated_dataset = torch.cat([recreated_dataset, batch_recreated_data], dim = 0) if recreated_dataset is not None else batch_recreated_data\n",
    "        \n",
    "recreated_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation for Model Training\n",
    "\n",
    "After synthetic data generation, this section separates the data and labels for training purposes, preparing them for use in machine learning models to ensure proper supervision and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the recreated data into features and labels for training\n",
    "recreated_training_data, recreated_labels = recreated_dataset[:, :-1].clone().detach().cpu().numpy(), recreated_dataset[:, -1:].clone().detach().cpu().numpy()\n",
    "ccnet_recreated_dataset = LabeledDataset(recreated_training_data, recreated_labels)\n",
    "\n",
    "num_features = recreated_training_data.shape[1]\n",
    "num_classes = recreated_labels.shape[1]\n",
    "num_features, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers=4, hidden_size=256):\n",
    "        super(DNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Create a list to hold all layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(torch.nn.Linear(input_size, hidden_size))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(torch.nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(torch.nn.Linear(hidden_size, output_size))\n",
    "        \n",
    "        # Register all layers\n",
    "        self.layers = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Supervised Models\n",
    "\n",
    "This section outlines the process of training supervised learning models using both original and synthetic datasets. The `train_supervised_model` function is designed to iterate through the dataset, perform forward passes, compute loss, and update model weights using backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised_model(model, dataset):\n",
    "    # Initialize the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # Ensure reproducibility by resetting the random seed\n",
    "    set_random_seed(0)\n",
    "    # Create DataLoader for batch processing\n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(2):  # Train for 2 epochs as an example\n",
    "        for i, (data, label) in enumerate(trainloader):\n",
    "            data = data.to(device)\n",
    "            label = label.to(device).clamp(min = 0, max = 1)\n",
    "            # Perform forward pass\n",
    "            output = model(data)\n",
    "            output = output.clamp(min = 0, max = 1)\n",
    "            # Compute loss\n",
    "            loss = torch.nn.functional.binary_cross_entropy(output, label)\n",
    "            # Backward pass to compute gradients\n",
    "            loss.backward()\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training Using Recreated and Original Datasets\n",
    "\n",
    "Models are trained using both datasets generated through the Data Augmentation process and the original dataset. This comparison helps to determine the effectiveness of the synthetic data in improving model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train a model on the recreated dataset\n",
    "model_trained_on_recreated = DNN(input_size=num_features, output_size=num_classes).to(device)\n",
    "train_supervised_model(model_trained_on_recreated, ccnet_recreated_dataset)\n",
    "\n",
    "# Initialize and train a model on the original dataset\n",
    "model_trained_on_original = DNN(input_size=num_features, output_size=num_classes).to(device)\n",
    "train_supervised_model(model_trained_on_original, trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Model Performance\n",
    "\n",
    "After training, the models are evaluated using the F1 score, a harmonic mean of precision and recall, which is particularly useful in the context of imbalanced datasets like fraud detection. This step is critical for assessing the quality of the models trained on different types of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_f1_score(model, testset, batch_size=batch_size):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    # DataLoader for testing\n",
    "    data_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # No gradient computation needed during inference\n",
    "    with torch.no_grad():\n",
    "        for data, label in data_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(data)\n",
    "            # Process output for binary classification\n",
    "            predicted = (output.squeeze() > 0.5).long()\n",
    "            y_true.extend(label.squeeze().long().cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Compute and return the F1 score\n",
    "    score = f1_score(y_true, y_pred, average='binary')\n",
    "    return score\n",
    "\n",
    "# Calculate F1 scores for both models\n",
    "f1_score_original = get_f1_score(model_trained_on_original, testset)\n",
    "f1_score_recreated = get_f1_score(model_trained_on_recreated, testset)\n",
    "\n",
    "# Output the results\n",
    "print(\"F1 score of the supervised learning model trained on the original data: \", f1_score_original)\n",
    "print(\"F1 score of the supervised learning model trained on the recreated data: \", f1_score_recreated)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
