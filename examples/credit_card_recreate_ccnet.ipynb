{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author:\n",
    "        \n",
    "        PARK, JunHo, junho@ccnets.org\n",
    "\n",
    "        \n",
    "        KIM, JeongYoong, jeongyoong@ccnets.org\n",
    "        \n",
    "    COPYRIGHT (c) 2024. CCNets. All Rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path_append = \"../\"\n",
    "sys.path.append(path_append)  # Go up one directory from where you are.\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5   \n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321  \\\n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22   \n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838  \\\n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount   \n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62  \\\n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataroot = path_append + \"../data/credit_card_fraud_detection/creditcard.csv\"\n",
    "df = pd.read_csv(dataroot)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.83 %of the dataset\n",
      "Frauds 0.17 %of the dataset\n"
     ]
    }
   ],
   "source": [
    "print('No Frauds', round(df['Class'].value_counts()[0] / len(df) *100,2), '%of the dataset')\n",
    "print('Frauds', round(df['Class'].value_counts()[1] / len(df) *100,2), '%of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://fraud-detection-handbook.github.io/fraud-detection-handbook/Chapter_7_DeepLearning/FeedForwardNeuralNetworks.html\n",
    "class LabeledDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        vals = torch.tensor(self.x[index], dtype = torch.float32)\n",
    "        label = torch.tensor(self.y[index], dtype = torch.float32)\n",
    "        return vals, label\n",
    "\n",
    "class UnlabelledDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        vals = torch.tensor(self.x[index], dtype = torch.float32)\n",
    "        return vals, None\n",
    "\n",
    "sc = StandardScaler()\n",
    "df.iloc[:, :-1] = sc.fit_transform(df.iloc[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features\n",
    "n_elements = df.shape[1]\n",
    "# number of label classes\n",
    "# n_classes = y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.setting.ml_params import MLParameters\n",
    "from tools.setting.data_config import DataConfig\n",
    "from nn.utils.init import set_random_seed\n",
    "set_random_seed(0)\n",
    "\n",
    "from trainer_hub import TrainerHub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = DataConfig(dataset_name = 'CreditCardFraudDetection', task_type='augmentation', obs_shape=[n_elements], label_size=None)\n",
    "\n",
    "#  Set training configuration from the AlgorithmConfig class, returning them as a Namespace object.\n",
    "ml_params = MLParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.setting.ml_params import TabularModelParams\n",
    "\n",
    "ml_params.core_model_name = 'none' \n",
    "ml_params.encoder_model_name = 'tabular'\n",
    "ml_params.encoding_params = TabularModelParams(dropout=0.05)\n",
    "ml_params.training.max_epoch = 4\n",
    "# Set the device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb=False, use_full_eval=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size = 0.5, shuffle= False)\n",
    "X_train, y_train = df_train.iloc[:, :-1].values, df_train.iloc[:, -1:].values\n",
    "X_test, y_test = df_test.iloc[:, :-1].values, df_test.iloc[:, -1:].values\n",
    "_df_train = df_train.iloc[:, :].values \n",
    "\n",
    "unlabelled_trainset = UnlabelledDataset(_df_train)\n",
    "trainset = LabeledDataset(X_test, y_test)\n",
    "testset = LabeledDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408442b600ca4d36a6d90b9428c9460a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd0e2e8733e4b98a492aaa14ab4884d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/2225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/4][50/2225][Time 0.92]\n",
      "Unified LR across all optimizers: 0.0001995308238189185\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.5647\tGen: 0.5245\tRec: 0.5736\tE: 0.5156\tR: 0.6138\tP: 0.5334\n",
      "[0/4][100/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00019907191565870155\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.5552\tGen: 0.4696\tRec: 0.4684\tE: 0.5564\tR: 0.5540\tP: 0.3828\n",
      "[0/4][150/2225][Time 0.82]\n",
      "Unified LR across all optimizers: 0.00019861406295796434\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.5483\tGen: 0.4573\tRec: 0.3894\tE: 0.6162\tR: 0.4805\tP: 0.2984\n",
      "[0/4][200/2225][Time 0.80]\n",
      "Unified LR across all optimizers: 0.00019815726328921765\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.4659\tGen: 0.4350\tRec: 0.3619\tE: 0.5390\tR: 0.3928\tP: 0.3311\n",
      "[0/4][250/2225][Time 0.88]\n",
      "Unified LR across all optimizers: 0.00019770151423055492\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.4143\tGen: 0.4234\tRec: 0.3708\tE: 0.4668\tR: 0.3617\tP: 0.3799\n",
      "[0/4][300/2225][Time 0.82]\n",
      "Unified LR across all optimizers: 0.00019724681336564005\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.3225\tGen: 0.3496\tRec: 0.3236\tE: 0.3485\tR: 0.2966\tP: 0.3507\n",
      "[0/4][350/2225][Time 0.82]\n",
      "Unified LR across all optimizers: 0.00019679315828369438\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.2780\tGen: 0.3009\tRec: 0.2917\tE: 0.2872\tR: 0.2687\tP: 0.3147\n",
      "[0/4][400/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00019634054657948372\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.2489\tGen: 0.2645\tRec: 0.2646\tE: 0.2488\tR: 0.2490\tP: 0.2803\n",
      "[0/4][450/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00019588897585330582\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.2371\tGen: 0.2495\tRec: 0.2506\tE: 0.2361\tR: 0.2382\tP: 0.2629\n",
      "[0/4][500/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00019543844371097777\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.2146\tGen: 0.2236\tRec: 0.2263\tE: 0.2119\tR: 0.2172\tP: 0.2354\n",
      "[0/4][550/2225][Time 0.89]\n",
      "Unified LR across all optimizers: 0.00019498894776382288\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.2084\tGen: 0.2184\tRec: 0.2187\tE: 0.2081\tR: 0.2088\tP: 0.2287\n",
      "[0/4][600/2225][Time 0.88]\n",
      "Unified LR across all optimizers: 0.00019454048562865856\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.2042\tGen: 0.2112\tRec: 0.2155\tE: 0.1999\tR: 0.2085\tP: 0.2226\n",
      "[0/4][650/2225][Time 0.98]\n",
      "Unified LR across all optimizers: 0.00019409305492778308\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1921\tGen: 0.2003\tRec: 0.2033\tE: 0.1890\tR: 0.1951\tP: 0.2115\n",
      "[0/4][700/2225][Time 0.87]\n",
      "Unified LR across all optimizers: 0.00019364665328896346\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1898\tGen: 0.1971\tRec: 0.2013\tE: 0.1857\tR: 0.1940\tP: 0.2085\n",
      "[0/4][750/2225][Time 0.87]\n",
      "Unified LR across all optimizers: 0.00019320127834542263\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1857\tGen: 0.1919\tRec: 0.1969\tE: 0.1807\tR: 0.1907\tP: 0.2031\n",
      "[0/4][800/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00019275692773582703\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1699\tGen: 0.1763\tRec: 0.1801\tE: 0.1661\tR: 0.1737\tP: 0.1865\n",
      "[0/4][850/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.0001923135991042739\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1721\tGen: 0.1796\tRec: 0.1829\tE: 0.1689\tR: 0.1754\tP: 0.1904\n",
      "[0/4][900/2225][Time 0.85]\n",
      "Unified LR across all optimizers: 0.0001918712901002789\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1613\tGen: 0.1705\tRec: 0.1720\tE: 0.1597\tR: 0.1628\tP: 0.1813\n",
      "[0/4][950/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.00019142999837876384\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1638\tGen: 0.1711\tRec: 0.1741\tE: 0.1608\tR: 0.1668\tP: 0.1815\n",
      "[0/4][1000/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.00019098972160004388\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1541\tGen: 0.1620\tRec: 0.1639\tE: 0.1521\tR: 0.1560\tP: 0.1719\n",
      "[0/4][1050/2225][Time 0.84]\n",
      "Unified LR across all optimizers: 0.00019055045742981543\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1591\tGen: 0.1641\tRec: 0.1699\tE: 0.1532\tR: 0.1649\tP: 0.1750\n",
      "[0/4][1100/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.00019011220353914353\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1486\tGen: 0.1552\tRec: 0.1588\tE: 0.1449\tR: 0.1522\tP: 0.1654\n",
      "[0/4][1150/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00018967495760444968\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1393\tGen: 0.1461\tRec: 0.1494\tE: 0.1359\tR: 0.1426\tP: 0.1562\n",
      "[0/4][1200/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.00018923871730749947\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1446\tGen: 0.1510\tRec: 0.1538\tE: 0.1418\tR: 0.1474\tP: 0.1602\n",
      "[0/4][1250/2225][Time 0.82]\n",
      "Unified LR across all optimizers: 0.00018880348033539028\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1399\tGen: 0.1466\tRec: 0.1492\tE: 0.1373\tR: 0.1425\tP: 0.1559\n",
      "[0/4][1300/2225][Time 0.80]\n",
      "Unified LR across all optimizers: 0.00018836924438053897\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1357\tGen: 0.1428\tRec: 0.1452\tE: 0.1332\tR: 0.1382\tP: 0.1523\n",
      "[0/4][1350/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.0001879360071406698\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1287\tGen: 0.1357\tRec: 0.1378\tE: 0.1266\tR: 0.1309\tP: 0.1448\n",
      "[0/4][1400/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.000187503766318802\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1273\tGen: 0.1348\tRec: 0.1362\tE: 0.1260\tR: 0.1287\tP: 0.1437\n",
      "[0/4][1450/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00018707251962323787\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1321\tGen: 0.1383\tRec: 0.1407\tE: 0.1296\tR: 0.1345\tP: 0.1469\n",
      "[0/4][1500/2225][Time 0.85]\n",
      "Unified LR across all optimizers: 0.0001866422647675502\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1225\tGen: 0.1297\tRec: 0.1311\tE: 0.1211\tR: 0.1239\tP: 0.1383\n",
      "[0/4][1550/2225][Time 0.81]\n",
      "Unified LR across all optimizers: 0.00018621299947057073\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1156\tGen: 0.1231\tRec: 0.1248\tE: 0.1139\tR: 0.1172\tP: 0.1323\n",
      "[0/4][1600/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00018578472145637737\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1166\tGen: 0.1239\tRec: 0.1247\tE: 0.1158\tR: 0.1175\tP: 0.1320\n",
      "[0/4][1650/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00018535742845428288\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1133\tGen: 0.1209\tRec: 0.1219\tE: 0.1123\tR: 0.1142\tP: 0.1296\n",
      "[0/4][1700/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.00018493111819882223\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1139\tGen: 0.1211\tRec: 0.1226\tE: 0.1124\tR: 0.1154\tP: 0.1297\n",
      "[0/4][1750/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.00018450578842974107\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1067\tGen: 0.1142\tRec: 0.1150\tE: 0.1059\tR: 0.1075\tP: 0.1225\n",
      "[0/4][1800/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.00018408143689198318\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1017\tGen: 0.1105\tRec: 0.1094\tE: 0.1028\tR: 0.1006\tP: 0.1182\n",
      "[0/4][1850/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.0001836580613356789\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1014\tGen: 0.1094\tRec: 0.1094\tE: 0.1014\tR: 0.1013\tP: 0.1174\n",
      "[0/4][1900/2225][Time 0.81]\n",
      "Unified LR across all optimizers: 0.0001832356595161332\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1106\tGen: 0.1164\tRec: 0.1187\tE: 0.1082\tR: 0.1129\tP: 0.1246\n",
      "[0/4][1950/2225][Time 0.83]\n",
      "Unified LR across all optimizers: 0.00018281422919381367\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1104\tGen: 0.1168\tRec: 0.1182\tE: 0.1090\tR: 0.1118\tP: 0.1246\n",
      "[0/4][2000/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00018239376813433867\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1081\tGen: 0.1147\tRec: 0.1160\tE: 0.1068\tR: 0.1093\tP: 0.1226\n",
      "[0/4][2050/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00018197427410846564\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1033\tGen: 0.1097\tRec: 0.1112\tE: 0.1018\tR: 0.1048\tP: 0.1177\n",
      "[0/4][2100/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.00018155574489207887\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1051\tGen: 0.1125\tRec: 0.1131\tE: 0.1045\tR: 0.1057\tP: 0.1206\n",
      "[0/4][2150/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00018113817826617823\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1029\tGen: 0.1096\tRec: 0.1101\tE: 0.1024\tR: 0.1034\tP: 0.1168\n",
      "[0/4][2200/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00018072157201686696\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0978\tGen: 0.1049\tRec: 0.1055\tE: 0.0972\tR: 0.0984\tP: 0.1126\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09870a58bb3e48f69074590ac38e4489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/2225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/4][25/2225][Time 0.82]\n",
      "Unified LR across all optimizers: 0.00018030592393534033\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0970\tGen: 0.1036\tRec: 0.1046\tE: 0.0959\tR: 0.0980\tP: 0.1112\n",
      "[1/4][75/2225][Time 0.82]\n",
      "Unified LR across all optimizers: 0.0001798912318178735\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0991\tGen: 0.1050\tRec: 0.1060\tE: 0.0981\tR: 0.1000\tP: 0.1119\n",
      "[1/4][125/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00017947749346581006\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0951\tGen: 0.1026\tRec: 0.1023\tE: 0.0954\tR: 0.0948\tP: 0.1097\n",
      "[1/4][175/2225][Time 0.80]\n",
      "Unified LR across all optimizers: 0.0001790647066855505\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1013\tGen: 0.1081\tRec: 0.1084\tE: 0.1010\tR: 0.1016\tP: 0.1151\n",
      "[1/4][225/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00017865286928854052\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.1006\tGen: 0.1070\tRec: 0.1075\tE: 0.1001\tR: 0.1012\tP: 0.1138\n",
      "[1/4][275/2225][Time 0.81]\n",
      "Unified LR across all optimizers: 0.00017824197909125899\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0931\tGen: 0.1013\tRec: 0.1000\tE: 0.0944\tR: 0.0919\tP: 0.1082\n",
      "[1/4][325/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.00017783203391520723\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0895\tGen: 0.0971\tRec: 0.0960\tE: 0.0906\tR: 0.0884\tP: 0.1036\n",
      "[1/4][375/2225][Time 0.82]\n",
      "Unified LR across all optimizers: 0.00017742303158689668\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0945\tGen: 0.1017\tRec: 0.1010\tE: 0.0951\tR: 0.0938\tP: 0.1082\n",
      "[1/4][425/2225][Time 0.83]\n",
      "Unified LR across all optimizers: 0.00017701496993783762\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0888\tGen: 0.0967\tRec: 0.0961\tE: 0.0894\tR: 0.0882\tP: 0.1039\n",
      "[1/4][475/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00017660784680452796\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0896\tGen: 0.0983\tRec: 0.0964\tE: 0.0915\tR: 0.0877\tP: 0.1050\n",
      "[1/4][525/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.0001762016600284412\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0960\tGen: 0.1031\tRec: 0.1033\tE: 0.0959\tR: 0.0962\tP: 0.1104\n",
      "[1/4][575/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.00017579640745601563\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0919\tGen: 0.0985\tRec: 0.0983\tE: 0.0920\tR: 0.0917\tP: 0.1049\n",
      "[1/4][625/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.0001753920869386423\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0962\tGen: 0.1034\tRec: 0.1034\tE: 0.0962\tR: 0.0962\tP: 0.1105\n",
      "[1/4][675/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.0001749886963326542\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0941\tGen: 0.1015\tRec: 0.1011\tE: 0.0946\tR: 0.0937\tP: 0.1085\n",
      "[1/4][725/2225][Time 0.81]\n",
      "Unified LR across all optimizers: 0.0001745862334993144\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0904\tGen: 0.0984\tRec: 0.0975\tE: 0.0913\tR: 0.0894\tP: 0.1055\n",
      "[1/4][775/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00017418469630480507\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0932\tGen: 0.1006\tRec: 0.1002\tE: 0.0936\tR: 0.0928\tP: 0.1077\n",
      "[1/4][825/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.00017378408262021616\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0935\tGen: 0.1010\tRec: 0.0999\tE: 0.0945\tR: 0.0924\tP: 0.1075\n",
      "[1/4][875/2225][Time 0.85]\n",
      "Unified LR across all optimizers: 0.00017338439032153356\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0923\tGen: 0.0996\tRec: 0.0998\tE: 0.0920\tR: 0.0926\tP: 0.1071\n",
      "[1/4][925/2225][Time 0.81]\n",
      "Unified LR across all optimizers: 0.00017298561728962847\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0889\tGen: 0.0951\tRec: 0.0958\tE: 0.0882\tR: 0.0895\tP: 0.1021\n",
      "[1/4][975/2225][Time 0.81]\n",
      "Unified LR across all optimizers: 0.00017258776141024598\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0794\tGen: 0.0885\tRec: 0.0854\tE: 0.0825\tR: 0.0763\tP: 0.0944\n",
      "[1/4][1025/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00017219082057399394\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0842\tGen: 0.0931\tRec: 0.0911\tE: 0.0863\tR: 0.0821\tP: 0.1000\n",
      "[1/4][1075/2225][Time 0.83]\n",
      "Unified LR across all optimizers: 0.00017179479267633146\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0863\tGen: 0.0940\tRec: 0.0923\tE: 0.0880\tR: 0.0846\tP: 0.1000\n",
      "[1/4][1125/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00017139967561755819\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0862\tGen: 0.0937\tRec: 0.0925\tE: 0.0874\tR: 0.0851\tP: 0.0999\n",
      "[1/4][1175/2225][Time 0.80]\n",
      "Unified LR across all optimizers: 0.00017100546730280274\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0825\tGen: 0.0902\tRec: 0.0890\tE: 0.0837\tR: 0.0812\tP: 0.0968\n",
      "[1/4][1225/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00017061216564201177\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0906\tGen: 0.0972\tRec: 0.0975\tE: 0.0903\tR: 0.0910\tP: 0.1040\n",
      "[1/4][1275/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.0001702197685499392\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0802\tGen: 0.0884\tRec: 0.0866\tE: 0.0820\tR: 0.0784\tP: 0.0949\n",
      "[1/4][1325/2225][Time 0.85]\n",
      "Unified LR across all optimizers: 0.0001698282739461345\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0787\tGen: 0.0862\tRec: 0.0853\tE: 0.0796\tR: 0.0778\tP: 0.0928\n",
      "[1/4][1375/2225][Time 0.81]\n",
      "Unified LR across all optimizers: 0.00016943767975493242\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0806\tGen: 0.0886\tRec: 0.0864\tE: 0.0828\tR: 0.0784\tP: 0.0944\n",
      "[1/4][1425/2225][Time 0.80]\n",
      "Unified LR across all optimizers: 0.0001690479839054414\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0817\tGen: 0.0893\tRec: 0.0878\tE: 0.0832\tR: 0.0802\tP: 0.0955\n",
      "[1/4][1475/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.00016865918433153277\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0819\tGen: 0.0899\tRec: 0.0880\tE: 0.0838\tR: 0.0800\tP: 0.0961\n",
      "[1/4][1525/2225][Time 0.82]\n",
      "Unified LR across all optimizers: 0.00016827127897182985\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0821\tGen: 0.0895\tRec: 0.0886\tE: 0.0829\tR: 0.0812\tP: 0.0961\n",
      "[1/4][1575/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.0001678842657696972\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0831\tGen: 0.0909\tRec: 0.0890\tE: 0.0850\tR: 0.0812\tP: 0.0968\n",
      "[1/4][1625/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00016749814267322938\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0870\tGen: 0.0948\tRec: 0.0938\tE: 0.0880\tR: 0.0860\tP: 0.1015\n",
      "[1/4][1675/2225][Time 0.80]\n",
      "Unified LR across all optimizers: 0.00016711290763524007\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0773\tGen: 0.0851\tRec: 0.0838\tE: 0.0786\tR: 0.0760\tP: 0.0916\n",
      "[1/4][1725/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00016672855861325146\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0828\tGen: 0.0906\tRec: 0.0882\tE: 0.0853\tR: 0.0803\tP: 0.0960\n",
      "[1/4][1775/2225][Time 0.83]\n",
      "Unified LR across all optimizers: 0.00016634509356948314\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0709\tGen: 0.0796\tRec: 0.0775\tE: 0.0730\tR: 0.0687\tP: 0.0862\n",
      "[1/4][1825/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.00016596251047084197\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0789\tGen: 0.0867\tRec: 0.0849\tE: 0.0806\tR: 0.0771\tP: 0.0927\n",
      "[1/4][1875/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00016558080728890993\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0788\tGen: 0.0863\tRec: 0.0848\tE: 0.0802\tR: 0.0773\tP: 0.0923\n",
      "[1/4][1925/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00016519998199993532\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0786\tGen: 0.0861\tRec: 0.0845\tE: 0.0803\tR: 0.0770\tP: 0.0919\n",
      "[1/4][1975/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.0001648200325848201\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0805\tGen: 0.0884\tRec: 0.0856\tE: 0.0834\tR: 0.0777\tP: 0.0935\n",
      "[1/4][2025/2225][Time 0.83]\n",
      "Unified LR across all optimizers: 0.00016444095702911038\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0750\tGen: 0.0827\tRec: 0.0804\tE: 0.0773\tR: 0.0726\tP: 0.0881\n",
      "[1/4][2075/2225][Time 0.80]\n",
      "Unified LR across all optimizers: 0.00016406275332298505\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0775\tGen: 0.0854\tRec: 0.0830\tE: 0.0799\tR: 0.0751\tP: 0.0908\n",
      "[1/4][2125/2225][Time 0.80]\n",
      "Unified LR across all optimizers: 0.00016368541946124596\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0766\tGen: 0.0850\tRec: 0.0824\tE: 0.0793\tR: 0.0740\tP: 0.0908\n",
      "[1/4][2175/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00016330895344330638\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0795\tGen: 0.0873\tRec: 0.0849\tE: 0.0818\tR: 0.0771\tP: 0.0928\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069aab483b214786bcba1ceb4716b64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/2225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/4][0/2225][Time 0.89]\n",
      "Unified LR across all optimizers: 0.00016293335327318117\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0800\tGen: 0.0873\tRec: 0.0854\tE: 0.0819\tR: 0.0781\tP: 0.0926\n",
      "[2/4][50/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00016255861695947546\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0750\tGen: 0.0834\tRec: 0.0796\tE: 0.0788\tR: 0.0713\tP: 0.0880\n",
      "[2/4][100/2225][Time 0.81]\n",
      "Unified LR across all optimizers: 0.00016218474251537463\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0779\tGen: 0.0863\tRec: 0.0837\tE: 0.0805\tR: 0.0752\tP: 0.0922\n",
      "[2/4][150/2225][Time 0.80]\n",
      "Unified LR across all optimizers: 0.00016181172795863357\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0739\tGen: 0.0831\tRec: 0.0788\tE: 0.0782\tR: 0.0697\tP: 0.0880\n",
      "[2/4][200/2225][Time 0.86]\n",
      "Unified LR across all optimizers: 0.0001614395713115662\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0727\tGen: 0.0812\tRec: 0.0783\tE: 0.0756\tR: 0.0699\tP: 0.0868\n",
      "[2/4][250/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00016106827060103523\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0740\tGen: 0.0823\tRec: 0.0781\tE: 0.0782\tR: 0.0698\tP: 0.0865\n",
      "[2/4][300/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00016069782385844109\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0759\tGen: 0.0839\tRec: 0.0811\tE: 0.0787\tR: 0.0732\tP: 0.0891\n",
      "[2/4][350/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00016032822911971208\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0719\tGen: 0.0811\tRec: 0.0768\tE: 0.0761\tR: 0.0676\tP: 0.0861\n",
      "[2/4][400/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.0001599594844252937\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0777\tGen: 0.0864\tRec: 0.0829\tE: 0.0812\tR: 0.0743\tP: 0.0915\n",
      "[2/4][450/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.00015959158782013816\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0783\tGen: 0.0854\tRec: 0.0827\tE: 0.0810\tR: 0.0756\tP: 0.0898\n",
      "[2/4][500/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00015922453735369438\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0762\tGen: 0.0842\tRec: 0.0807\tE: 0.0797\tR: 0.0727\tP: 0.0887\n",
      "[2/4][550/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00015885833107989733\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0746\tGen: 0.0823\tRec: 0.0792\tE: 0.0777\tR: 0.0714\tP: 0.0870\n",
      "[2/4][600/2225][Time 0.84]\n",
      "Unified LR across all optimizers: 0.00015849296705715778\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0738\tGen: 0.0830\tRec: 0.0779\tE: 0.0788\tR: 0.0687\tP: 0.0871\n",
      "[2/4][650/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.0001581284433483521\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0767\tGen: 0.0853\tRec: 0.0810\tE: 0.0810\tR: 0.0725\tP: 0.0896\n",
      "[2/4][700/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00015776475802081183\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0704\tGen: 0.0790\tRec: 0.0754\tE: 0.0740\tR: 0.0669\tP: 0.0839\n",
      "[2/4][750/2225][Time 0.80]\n",
      "Unified LR across all optimizers: 0.00015740190914631356\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0762\tGen: 0.0849\tRec: 0.0808\tE: 0.0803\tR: 0.0720\tP: 0.0895\n",
      "[2/4][800/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.0001570398948010687\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0757\tGen: 0.0847\tRec: 0.0796\tE: 0.0808\tR: 0.0707\tP: 0.0886\n",
      "[2/4][850/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00015667871306571324\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0799\tGen: 0.0890\tRec: 0.0844\tE: 0.0845\tR: 0.0752\tP: 0.0935\n",
      "[2/4][900/2225][Time 0.82]\n",
      "Unified LR across all optimizers: 0.00015631836202529756\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0697\tGen: 0.0793\tRec: 0.0746\tE: 0.0744\tR: 0.0649\tP: 0.0843\n",
      "[2/4][950/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.00015595883976927632\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0713\tGen: 0.0808\tRec: 0.0760\tE: 0.0760\tR: 0.0665\tP: 0.0856\n",
      "[2/4][1000/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.0001556001443914984\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0730\tGen: 0.0827\tRec: 0.0772\tE: 0.0784\tR: 0.0675\tP: 0.0869\n",
      "[2/4][1050/2225][Time 0.88]\n",
      "Unified LR across all optimizers: 0.0001552422739901963\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0651\tGen: 0.0757\tRec: 0.0697\tE: 0.0711\tR: 0.0591\tP: 0.0802\n",
      "[2/4][1100/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00015488522666797712\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0722\tGen: 0.0813\tRec: 0.0768\tE: 0.0766\tR: 0.0677\tP: 0.0860\n",
      "[2/4][1150/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00015452900053181137\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0695\tGen: 0.0796\tRec: 0.0741\tE: 0.0750\tR: 0.0640\tP: 0.0843\n",
      "[2/4][1200/2225][Time 0.81]\n",
      "Unified LR across all optimizers: 0.00015417359369302347\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0715\tGen: 0.0814\tRec: 0.0760\tE: 0.0770\tR: 0.0661\tP: 0.0859\n",
      "[2/4][1250/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00015381900426728195\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0672\tGen: 0.0771\tRec: 0.0720\tE: 0.0723\tR: 0.0622\tP: 0.0818\n",
      "[2/4][1300/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00015346523037458877\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0650\tGen: 0.0744\tRec: 0.0694\tE: 0.0700\tR: 0.0600\tP: 0.0788\n",
      "[2/4][1350/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00015311227013926996\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0724\tGen: 0.0819\tRec: 0.0764\tE: 0.0779\tR: 0.0669\tP: 0.0859\n",
      "[2/4][1400/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00015276012168996563\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0723\tGen: 0.0821\tRec: 0.0759\tE: 0.0786\tR: 0.0661\tP: 0.0857\n",
      "[2/4][1450/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00015240878315961963\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0642\tGen: 0.0747\tRec: 0.0683\tE: 0.0705\tR: 0.0578\tP: 0.0788\n",
      "[2/4][1500/2225][Time 0.83]\n",
      "Unified LR across all optimizers: 0.00015205825268547007\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0753\tGen: 0.0860\tRec: 0.0783\tE: 0.0829\tR: 0.0677\tP: 0.0890\n",
      "[2/4][1550/2225][Time 0.83]\n",
      "Unified LR across all optimizers: 0.0001517085284090394\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0722\tGen: 0.0824\tRec: 0.0752\tE: 0.0794\tR: 0.0651\tP: 0.0854\n",
      "[2/4][1600/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00015135960847612417\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0701\tGen: 0.0809\tRec: 0.0741\tE: 0.0769\tR: 0.0634\tP: 0.0849\n",
      "[2/4][1650/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.0001510114910367858\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0724\tGen: 0.0832\tRec: 0.0754\tE: 0.0802\tR: 0.0646\tP: 0.0862\n",
      "[2/4][1700/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00015066417424534014\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0725\tGen: 0.0827\tRec: 0.0763\tE: 0.0789\tR: 0.0661\tP: 0.0864\n",
      "[2/4][1750/2225][Time 0.80]\n",
      "Unified LR across all optimizers: 0.00015031765626034814\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0670\tGen: 0.0771\tRec: 0.0707\tE: 0.0734\tR: 0.0606\tP: 0.0808\n",
      "[2/4][1800/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.00014997193524460595\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0769\tGen: 0.0871\tRec: 0.0806\tE: 0.0834\tR: 0.0704\tP: 0.0908\n",
      "[2/4][1850/2225][Time 0.81]\n",
      "Unified LR across all optimizers: 0.00014962700936513516\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0609\tGen: 0.0717\tRec: 0.0651\tE: 0.0676\tR: 0.0543\tP: 0.0759\n",
      "[2/4][1900/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00014928287679317315\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0739\tGen: 0.0838\tRec: 0.0766\tE: 0.0811\tR: 0.0667\tP: 0.0866\n",
      "[2/4][1950/2225][Time 0.85]\n",
      "Unified LR across all optimizers: 0.0001489395357041632\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0702\tGen: 0.0812\tRec: 0.0741\tE: 0.0773\tR: 0.0631\tP: 0.0851\n",
      "[2/4][2000/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.000148596984277745\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0692\tGen: 0.0802\tRec: 0.0726\tE: 0.0768\tR: 0.0616\tP: 0.0835\n",
      "[2/4][2050/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.0001482552206977451\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0617\tGen: 0.0730\tRec: 0.0654\tE: 0.0693\tR: 0.0541\tP: 0.0766\n",
      "[2/4][2100/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00014791424315216693\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0656\tGen: 0.0768\tRec: 0.0692\tE: 0.0732\tR: 0.0580\tP: 0.0804\n",
      "[2/4][2150/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00014757404983318153\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0644\tGen: 0.0754\tRec: 0.0669\tE: 0.0729\tR: 0.0559\tP: 0.0778\n",
      "[2/4][2200/2225][Time 0.80]\n",
      "Unified LR across all optimizers: 0.00014723463893711783\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0682\tGen: 0.0789\tRec: 0.0709\tE: 0.0762\tR: 0.0602\tP: 0.0816\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f0883c2a5d4b1b900b1b7f08cedbc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/2225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/4][25/2225][Time 0.91]\n",
      "Unified LR across all optimizers: 0.00014689600866445298\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0695\tGen: 0.0815\tRec: 0.0716\tE: 0.0794\tR: 0.0596\tP: 0.0836\n",
      "[3/4][75/2225][Time 0.80]\n",
      "Unified LR across all optimizers: 0.00014655815721980301\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0639\tGen: 0.0758\tRec: 0.0666\tE: 0.0731\tR: 0.0547\tP: 0.0785\n",
      "[3/4][125/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00014622108281191326\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0684\tGen: 0.0802\tRec: 0.0706\tE: 0.0780\tR: 0.0589\tP: 0.0824\n",
      "[3/4][175/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.00014588478365364866\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0706\tGen: 0.0823\tRec: 0.0732\tE: 0.0796\tR: 0.0615\tP: 0.0850\n",
      "[3/4][225/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.0001455492579619846\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0620\tGen: 0.0741\tRec: 0.0649\tE: 0.0712\tR: 0.0527\tP: 0.0770\n",
      "[3/4][275/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00014521450395799725\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0674\tGen: 0.0796\tRec: 0.0695\tE: 0.0776\tR: 0.0573\tP: 0.0817\n",
      "[3/4][325/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00014488051986685427\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0635\tGen: 0.0760\tRec: 0.0661\tE: 0.0733\tR: 0.0536\tP: 0.0787\n",
      "[3/4][375/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.00014454730391780514\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0655\tGen: 0.0767\tRec: 0.0677\tE: 0.0746\tR: 0.0565\tP: 0.0789\n",
      "[3/4][425/2225][Time 0.86]\n",
      "Unified LR across all optimizers: 0.0001442148543441721\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0662\tGen: 0.0786\tRec: 0.0667\tE: 0.0780\tR: 0.0543\tP: 0.0791\n",
      "[3/4][475/2225][Time 0.82]\n",
      "Unified LR across all optimizers: 0.00014388316938334075\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0614\tGen: 0.0723\tRec: 0.0642\tE: 0.0695\tR: 0.0533\tP: 0.0751\n",
      "[3/4][525/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.0001435522472767503\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0689\tGen: 0.0809\tRec: 0.0711\tE: 0.0787\tR: 0.0591\tP: 0.0831\n",
      "[3/4][575/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00014322208626988471\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0662\tGen: 0.0773\tRec: 0.0681\tE: 0.0754\tR: 0.0570\tP: 0.0792\n",
      "[3/4][625/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00014289268461226322\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0665\tGen: 0.0775\tRec: 0.0680\tE: 0.0760\tR: 0.0569\tP: 0.0790\n",
      "[3/4][675/2225][Time 0.82]\n",
      "Unified LR across all optimizers: 0.00014256404055743098\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0623\tGen: 0.0739\tRec: 0.0646\tE: 0.0716\tR: 0.0530\tP: 0.0762\n",
      "[3/4][725/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00014223615236295\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0584\tGen: 0.0688\tRec: 0.0609\tE: 0.0664\tR: 0.0505\tP: 0.0713\n",
      "[3/4][775/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.0001419090182903897\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0622\tGen: 0.0726\tRec: 0.0637\tE: 0.0711\tR: 0.0534\tP: 0.0741\n",
      "[3/4][825/2225][Time 0.85]\n",
      "Unified LR across all optimizers: 0.00014158263660531755\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0677\tGen: 0.0778\tRec: 0.0674\tE: 0.0781\tR: 0.0573\tP: 0.0774\n",
      "[3/4][875/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00014125700557729063\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0646\tGen: 0.0749\tRec: 0.0666\tE: 0.0729\tR: 0.0564\tP: 0.0769\n",
      "[3/4][925/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00014093212347984557\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0602\tGen: 0.0703\tRec: 0.0625\tE: 0.0680\tR: 0.0524\tP: 0.0726\n",
      "[3/4][975/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.00014060798859048983\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0597\tGen: 0.0700\tRec: 0.0621\tE: 0.0676\tR: 0.0518\tP: 0.0723\n",
      "[3/4][1025/2225][Time 0.80]\n",
      "Unified LR across all optimizers: 0.00014028459919069257\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0585\tGen: 0.0692\tRec: 0.0610\tE: 0.0667\tR: 0.0503\tP: 0.0718\n",
      "[3/4][1075/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00013996195356587524\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0670\tGen: 0.0779\tRec: 0.0680\tE: 0.0769\tR: 0.0570\tP: 0.0790\n",
      "[3/4][1125/2225][Time 0.80]\n",
      "Unified LR across all optimizers: 0.0001396400500054029\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0640\tGen: 0.0741\tRec: 0.0656\tE: 0.0724\tR: 0.0556\tP: 0.0757\n",
      "[3/4][1175/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00013931888680257493\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0638\tGen: 0.0751\tRec: 0.0652\tE: 0.0736\tR: 0.0539\tP: 0.0765\n",
      "[3/4][1225/2225][Time 0.77]\n",
      "Unified LR across all optimizers: 0.000138998462254616\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0605\tGen: 0.0718\tRec: 0.0622\tE: 0.0701\tR: 0.0509\tP: 0.0736\n",
      "[3/4][1275/2225][Time 0.82]\n",
      "Unified LR across all optimizers: 0.00013867877466266706\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0578\tGen: 0.0692\tRec: 0.0598\tE: 0.0671\tR: 0.0484\tP: 0.0712\n",
      "[3/4][1325/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00013835982233177638\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0620\tGen: 0.0722\tRec: 0.0634\tE: 0.0708\tR: 0.0531\tP: 0.0737\n",
      "[3/4][1375/2225][Time 0.83]\n",
      "Unified LR across all optimizers: 0.0001380416035708904\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0660\tGen: 0.0769\tRec: 0.0657\tE: 0.0772\tR: 0.0548\tP: 0.0766\n",
      "[3/4][1425/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00013772411669284496\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0629\tGen: 0.0736\tRec: 0.0633\tE: 0.0733\tR: 0.0526\tP: 0.0740\n",
      "[3/4][1475/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00013740736001435612\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0609\tGen: 0.0709\tRec: 0.0629\tE: 0.0688\tR: 0.0529\tP: 0.0730\n",
      "[3/4][1525/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.0001370913318560116\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0626\tGen: 0.0730\tRec: 0.0649\tE: 0.0707\tR: 0.0545\tP: 0.0754\n",
      "[3/4][1575/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00013677603054226164\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0664\tGen: 0.0767\tRec: 0.0662\tE: 0.0769\tR: 0.0558\tP: 0.0766\n",
      "[3/4][1625/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00013646145440140986\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0571\tGen: 0.0672\tRec: 0.0595\tE: 0.0648\tR: 0.0494\tP: 0.0696\n",
      "[3/4][1675/2225][Time 0.80]\n",
      "Unified LR across all optimizers: 0.000136147601765605\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0592\tGen: 0.0700\tRec: 0.0600\tE: 0.0692\tR: 0.0491\tP: 0.0709\n",
      "[3/4][1725/2225][Time 0.85]\n",
      "Unified LR across all optimizers: 0.00013583447097083163\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0595\tGen: 0.0698\tRec: 0.0601\tE: 0.0693\tR: 0.0498\tP: 0.0704\n",
      "[3/4][1775/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00013552206035690145\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0583\tGen: 0.0684\tRec: 0.0606\tE: 0.0661\tR: 0.0505\tP: 0.0706\n",
      "[3/4][1825/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00013521036826744445\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0572\tGen: 0.0674\tRec: 0.0593\tE: 0.0653\tR: 0.0491\tP: 0.0696\n",
      "[3/4][1875/2225][Time 0.80]\n",
      "Unified LR across all optimizers: 0.00013489939304990025\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0571\tGen: 0.0681\tRec: 0.0588\tE: 0.0664\tR: 0.0478\tP: 0.0697\n",
      "[3/4][1925/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00013458913305550914\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0568\tGen: 0.0676\tRec: 0.0587\tE: 0.0658\tR: 0.0479\tP: 0.0694\n",
      "[3/4][1975/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00013427958663930363\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0548\tGen: 0.0651\tRec: 0.0575\tE: 0.0624\tR: 0.0473\tP: 0.0678\n",
      "[3/4][2025/2225][Time 0.81]\n",
      "Unified LR across all optimizers: 0.00013397075216009928\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0641\tGen: 0.0748\tRec: 0.0637\tE: 0.0751\tR: 0.0530\tP: 0.0744\n",
      "[3/4][2075/2225][Time 0.79]\n",
      "Unified LR across all optimizers: 0.00013366262798048648\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0560\tGen: 0.0661\tRec: 0.0572\tE: 0.0648\tR: 0.0472\tP: 0.0673\n",
      "[3/4][2125/2225][Time 0.78]\n",
      "Unified LR across all optimizers: 0.00013335521246682152\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0566\tGen: 0.0672\tRec: 0.0576\tE: 0.0662\tR: 0.0470\tP: 0.0681\n",
      "[3/4][2175/2225][Time 0.86]\n",
      "Unified LR across all optimizers: 0.00013304850398921782\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  tabular\n",
      "Inf: 0.0553\tGen: 0.0657\tRec: 0.0573\tE: 0.0637\tR: 0.0469\tP: 0.0677\n"
     ]
    }
   ],
   "source": [
    "trainer_hub.train(unlabelled_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Lower than the original batch size\n",
    "# Use DataLoader to handle smaller batches\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def collate_fn(batch):\n",
    "    X, y = zip(*batch)\n",
    "    # Directly use the tensors from X if they are already tensors, else convert appropriately\n",
    "    X_padded = pad_sequence([x.clone().detach() if isinstance(x, torch.Tensor) else torch.tensor(x) for x in X], batch_first=True, padding_value=0)\n",
    "    \n",
    "    if any(label is None for label in y):\n",
    "        y_padded = None\n",
    "    else:\n",
    "        # Directly use the tensors from y if they are already tensors, else convert appropriately\n",
    "        y_padded = pad_sequence([label.clone().detach() if isinstance(label, torch.Tensor) else torch.tensor(label) for label in y], batch_first=True, padding_value=-1)\n",
    "    \n",
    "    return X_padded, y_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9901e+00, -6.9761e-01,  4.3261e-02,  ..., -7.6110e-02,\n",
       "          1.8244e-01,  2.3873e-03],\n",
       "        [-1.9154e+00,  5.8628e-01,  1.8738e-01,  ...,  7.0969e-02,\n",
       "         -3.2018e-01,  8.0240e-03],\n",
       "        [-1.7592e+00, -7.6837e-01, -8.2039e-01,  ..., -1.2940e-01,\n",
       "          1.1841e+00, -3.4368e-04],\n",
       "        ...,\n",
       "        [-2.4652e-01, -1.1312e+00,  6.7068e-01,  ..., -4.5710e-01,\n",
       "         -2.5605e-01,  4.6941e-04],\n",
       "        [-1.8864e-01, -1.1131e+00,  1.3040e+00,  ..., -6.7787e-02,\n",
       "         -3.3815e-01, -7.8926e-03],\n",
       "        [-2.8286e-01, -5.0602e-01,  3.3722e-01,  ..., -1.1421e+00,\n",
       "         -2.6214e-01,  3.7897e-03]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=unlabelled_trainset, batch_size=batch_size, collate_fn = collate_fn, shuffle=False)\n",
    "# Example: Reduce batch size\n",
    "recreated_dataset = None\n",
    "for data, _ in train_loader:\n",
    "    data = data.to(device)\n",
    "    batch_recreated_data = trainer_hub.encoder_ccnet.synthesize(data, output_multiplier=2)\n",
    "    recreated_dataset = torch.cat([recreated_dataset, batch_recreated_data]) if recreated_dataset is not None else batch_recreated_data\n",
    "recreated_dataset.squeeze_(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate the data and labels\n",
    "recreated_training_data, recreated_labels = recreated_dataset[:, :-1].clone().detach().cpu().numpy(), recreated_dataset[:, -1:].clone().detach().cpu().numpy()\n",
    "ccnet_recreated_dataset = LabeledDataset(recreated_training_data, recreated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = recreated_training_data.shape[1]\n",
    "num_classes = recreated_labels.shape[1]\n",
    "num_features, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers=4, hidden_size=128):\n",
    "        super(DNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Create a list to hold all layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(torch.nn.Linear(input_size, hidden_size))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(torch.nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(torch.nn.Linear(hidden_size, output_size))\n",
    "        layers.append(torch.nn.Sigmoid())\n",
    "        \n",
    "        # Register all layers\n",
    "        self.layers = torch.nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised_model(model, trainset):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    set_random_seed(0)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(2):\n",
    "        for i, (data, label) in enumerate(trainloader):\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(data)\n",
    "            loss = torch.nn.functional.binary_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained_on_original = DNN(input_size= num_features, output_size=num_classes).to(device)\n",
    "\n",
    "train_supervised_model(model_trained_on_original, trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained_on_recreated = DNN(input_size= num_features, output_size=num_classes).to(device)\n",
    "\n",
    "train_supervised_model(model_trained_on_recreated, ccnet_recreated_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the supervised learning model trained on the original data:  0.8620689655172413\n",
      "F1 score of the supervised learning model trained on the recreated data:  0.7634408602150538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_f1_score(model, testset, batch_size=batch_size):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    data_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients for inference\n",
    "        for data, label in data_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(data)\n",
    "            # Ensure output is squeezed, thresholded, and converted to long for binary classification\n",
    "            predicted = (output.squeeze() > 0.5).long()\n",
    "            # Make sure label is also in the correct format (long type)\n",
    "            y_true.extend(label.squeeze().long().cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Compute F1 score, using 'binary' because this is a binary classification task\n",
    "    score = f1_score(y_true, y_pred, average='binary')\n",
    "    return score\n",
    "\n",
    "# Usage example with two models:\n",
    "f1_score_original = get_f1_score(model_trained_on_original, testset)\n",
    "f1_score_recreated = get_f1_score(model_trained_on_recreated, testset)\n",
    "\n",
    "print(\"F1 score of the supervised learning model trained on the original data: \", f1_score_original)\n",
    "print(\"F1 score of the supervised learning model trained on the recreated data: \", f1_score_recreated)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
