{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Author:\n",
    "        \n",
    "        PARK, JunHo, junho@ccnets.org\n",
    "\n",
    "        \n",
    "        KIM, JoengYoong, jeongyoong@ccnets.org\n",
    "        \n",
    "    COPYRIGHT (c) 2024. CCNets. All Rights reserved.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheat Your Decision-Making Model: Manipulating Loan Approval Data with Causal Inference\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this tutorial, we explore how to manipulate decision-making models for loan approval prediction. Using a Cooperative Network (CCNet), we investigate the impact of subtle manipulations in non-fraudulent data to test and expose the vulnerabilities of machine learning models. This demonstration not only highlights the strengths and weaknesses of current fraud detection systems but also outlines the ethical considerations and potential risks associated with data manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Goals\n",
    "This tutorial offers guidance on applying data manipulation to machine learning models for predicting loan approval outcomes and evaluating the impacts. We will address the following:\n",
    "\n",
    "1. **Training a Binary Classifier**: We'll train a model using actual loan data to differentiate between approved and denied loan applications.\n",
    "2. **Generating Manipulated Data with CCNet**: We'll employ the Explainer, Reasoner, and Producer networks within CCNet to create data that mimics genuine loan applications but is engineered to mislead the classifier by altering decision label factors.\n",
    "3. **Evaluating Classifier Performance**: We'll test the classifier on both the original and the manipulated datasets to assess the model’s robustness. The classifier should perform well on both the original test set and the CCNet-recreated test set, demonstrating that CCNet has effectively learned how the classifier maps decisions.\n",
    "4. **Comparative Dataset Analysis**:\n",
    "   - Testset A: Real Test Data - Actual loan approval data used as a benchmark to evaluate the classifier’s performance.\n",
    "   - Testset B: CCNet-generated data with random labels but maintaining the same frequency distribution as the original. The classifier's performance on this dataset should mirror that of Testset A, indicating that CCNet comprehends the classifier's decision-making process.\n",
    "   - Testset C: CCNet-generated data with original labels. While Testset B proves that CCNet can generate a genuine data distribution for any scenario of decision label, most of the data's information resides in the explanation vector, making it easy to manipulate by changing the decision factor of the data. In short, the decision-making process can be easily manipulated through causal inference.\n",
    "5. **Ethical Considerations**: We will discuss the ethical implications of manipulating data in machine learning, emphasizing the necessity for robustness in model training and the risks of misuse in critical applications like loan approval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path_append = \"../../\"\n",
    "sys.path.append(path_append)  # Go up one directory from where you are.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../data/LoanApprovalPredictionDataset/loan_approval_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \n\u001b[0;32m      5\u001b[0m dataroot \u001b[38;5;241m=\u001b[39m path_append \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/LoanApprovalPredictionDataset/loan_approval_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m df\n",
      "File \u001b[1;32mc:\\Users\\junho\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\junho\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\junho\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\junho\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\junho\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../data/LoanApprovalPredictionDataset/loan_approval_dataset.csv'"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/datasets/architsharma01/loan-approval-prediction-dataset\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "dataroot = path_append + \"../data/LoanApprovalPredictionDataset/loan_approval_dataset.csv\"\n",
    "df = pd.read_csv(dataroot)\n",
    "df.columns = df.columns.str.replace(' ', '')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approved 62.22 %of the dataset\n",
      "Rejected 37.78 %of the dataset\n"
     ]
    }
   ],
   "source": [
    "print('Approved', round(df['loan_status'].value_counts()[0] / len(df) *100,2), '%of the dataset')\n",
    "print('Rejected', round(df['loan_status'].value_counts()[1] / len(df) *100,2), '%of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "one_hot_columns = ['education', 'self_employed']\n",
    "\n",
    "df = pd.get_dummies(df, columns=one_hot_columns)\n",
    "\n",
    "one_hot_columns = ['education_ Graduate', 'education_ Not Graduate', 'self_employed_ No', 'self_employed_ Yes']\n",
    "df[one_hot_columns] = df[one_hot_columns].astype(int)\n",
    "\n",
    "scale_cols = ['loan_id', 'no_of_dependents', 'income_annum', 'loan_amount',\n",
    "       'loan_term', 'cibil_score', 'residential_assets_value',\n",
    "       'commercial_assets_value', 'luxury_assets_value', 'bank_asset_value']\n",
    "\n",
    "# Initialize scalers\n",
    "sc = RobustScaler()\n",
    "\n",
    "# Scale all columns except the last one (which is the class column)\n",
    "df[scale_cols] = sc.fit_transform(df[scale_cols])\n",
    "\n",
    "# Map the target column to 0 and 1\n",
    "df['loan_status'] = df['loan_status'].map({' Approved': 1, ' Rejected': 0})\n",
    "\n",
    "# Drop the loan_id column\n",
    "df.drop('loan_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the original loan_status column to change its position\n",
    "loan_status = df.pop('loan_status')\n",
    "\n",
    "df['loan_status'] = loan_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_dependents</th>\n",
       "      <th>income_annum</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>loan_term</th>\n",
       "      <th>cibil_score</th>\n",
       "      <th>residential_assets_value</th>\n",
       "      <th>commercial_assets_value</th>\n",
       "      <th>luxury_assets_value</th>\n",
       "      <th>bank_asset_value</th>\n",
       "      <th>education_ Graduate</th>\n",
       "      <th>education_ Not Graduate</th>\n",
       "      <th>self_employed_ No</th>\n",
       "      <th>self_employed_ Yes</th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>1.115942</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.603390</td>\n",
       "      <td>-0.351648</td>\n",
       "      <td>2.206349</td>\n",
       "      <td>0.570423</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.208333</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.620339</td>\n",
       "      <td>-0.318681</td>\n",
       "      <td>-0.238095</td>\n",
       "      <td>-0.408451</td>\n",
       "      <td>-0.270833</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.101449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.318644</td>\n",
       "      <td>0.164835</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>1.316901</td>\n",
       "      <td>1.708333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>1.173913</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.450847</td>\n",
       "      <td>1.384615</td>\n",
       "      <td>-0.063492</td>\n",
       "      <td>0.612676</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.702899</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.738983</td>\n",
       "      <td>0.747253</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.042254</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_of_dependents  income_annum  loan_amount  loan_term  cibil_score  \\\n",
       "0         -0.333333      0.937500     1.115942        0.2     0.603390   \n",
       "1         -1.000000     -0.208333    -0.166667       -0.2    -0.620339   \n",
       "2          0.000000      0.833333     1.101449        1.0    -0.318644   \n",
       "3          0.000000      0.645833     1.173913       -0.2    -0.450847   \n",
       "4          0.666667      0.979167     0.702899        1.0    -0.738983   \n",
       "\n",
       "   residential_assets_value  commercial_assets_value  luxury_assets_value  \\\n",
       "0                 -0.351648                 2.206349             0.570423   \n",
       "1                 -0.318681                -0.238095            -0.408451   \n",
       "2                  0.164835                 0.126984             1.316901   \n",
       "3                  1.384615                -0.063492             0.612676   \n",
       "4                  0.747253                 0.714286             1.042254   \n",
       "\n",
       "   bank_asset_value  education_ Graduate  education_ Not Graduate  \\\n",
       "0          0.708333                    1                        0   \n",
       "1         -0.270833                    0                        1   \n",
       "2          1.708333                    1                        0   \n",
       "3          0.687500                    1                        0   \n",
       "4          0.083333                    0                        1   \n",
       "\n",
       "   self_employed_ No  self_employed_ Yes  loan_status  \n",
       "0                  1                   0            1  \n",
       "1                  0                   1            0  \n",
       "2                  1                   0            0  \n",
       "3                  1                   0            0  \n",
       "4                  0                   1            0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the labeled and unlabeled dataset classes\n",
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        vals = torch.tensor(self.x[index], dtype=torch.float32)\n",
    "        label = torch.tensor(self.y[index], dtype=torch.long)\n",
    "        # label = torch.nn.functional.one_hot(label, num_classes=2).float().squeeze(0)\n",
    "        return vals, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Splitting for Training and Testing\n",
    "\n",
    "The original dataset is split into training and testing parts to evaluate the model's performance accurately. This step is crucial for validating the effectiveness of the training on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled Trainset Shape: 1280, 13\n",
      "Labeled Testset Shape: 2989, 13\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset into training and test sets for model evaluation\n",
    "df_train, df_test = train_test_split(df, test_size=0.7, shuffle=True, random_state=42)\n",
    "X_train, y_train = df_train.iloc[:, :-1].values, df_train.iloc[:, -1:].values\n",
    "X_test, y_test = df_test.iloc[:, :-1].values, df_test.iloc[:, -1:].values\n",
    "\n",
    "# Labeled datasets for supervised learning tasks\n",
    "trainset = LabeledDataset(X_train, y_train)  # Corrected to include training data\n",
    "testset = LabeledDataset(X_test, y_test)     # Test set with proper labels\n",
    "\n",
    "# Printing the shapes of the datasets for verification\n",
    "print(f\"Labeled Trainset Shape: {len(trainset)}, {trainset.x.shape[1]}\")\n",
    "print(f\"Labeled Testset Shape: {len(testset)}, {testset.x.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 1\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of features and classes\n",
    "n_features = trainset[0][0].shape[0]\n",
    "n_classes = trainset[0][1].shape[0]\n",
    "\n",
    "print(n_features, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Setup and Model Configuration\n",
    "\n",
    "This section initializes the environment by setting a fixed random seed to ensure reproducibility of results. It imports necessary configurations and initializes model parameters with specific configurations. The model specified here is set to have no core model but uses a 'tabnet' encoder model for data processing, which is particularly tailored for structured or tabular data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility of experiments\n",
    "from nn.utils.init_layer import set_random_seed\n",
    "set_random_seed(0)\n",
    "\n",
    "# Importing configuration setups for ML parameters and data\n",
    "import torch\n",
    "from tools.setting.ml_params import MLParameters\n",
    "from tools.setting.data_config import DataConfig\n",
    "from trainer_hub import TrainerHub\n",
    "\n",
    "# Configuration for the data handling, defining dataset specifics and the task type\n",
    "data_config = DataConfig(dataset_name='LoanApproval', task_type='binary_classification', obs_shape=[n_features], label_size=n_classes, explain_size=4)\n",
    "\n",
    "# Initializing ML parameters without a core model and setting the encoder model to 'tabnet' with specific configurations\n",
    "ml_params = MLParameters(model_name='tabnet', encoder_network='none')\n",
    "\n",
    "# Setting training parameters and device configuration\n",
    "ml_params.training.num_epoch = 50\n",
    "ml_params.training.error_function = 'mse'\n",
    "ml_params.model.ccnet_config.num_layers = 3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Create a TrainerHub instance to manage training and data processing\n",
    "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb=False, print_interval=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d040a979244479db08cc489e26becf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ea2f2c28b440aca5a840d1d33d3d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dacba4c5fa3549f1bac18bd4d63f087d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833fb8cd6f4a44499ef51bf1fdb2ebd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd5d7bf9bde4c148fe45605a882e3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf5a164c8184a8499c17a91137fcaf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539d304576e64f5fa9fcfbddb318cf7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e697f4d5f67448e90293d3ec2bf44fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32176443894c4af5a5737d2664ba5a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277d17d4e3874e65824c26d0f5d44b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4e91cbad8e4bdcaf8cc3e9e62eef79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2da66dece2433cac0e4cb63d8c773a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/50][0/20][Time 25.66]\n",
      "Unified LR across all optimizers: 0.00019815726328921765\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.1212\tGen: 0.3599\tRec: 0.3548\tE: 0.0461\tR: 0.0385\tP: 0.3983\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8828\n",
      "precision: 0.8200\n",
      "recall: 0.8723\n",
      "f1_score: 0.8454\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb80dc54a909431796b4a01abbbe6a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19bb8f0d4484c0185586c095b12038f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef4c5b9f5dd4173af8bcb50c07dac37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e45229491745d48829617d027e80ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2fa83716c949ff851bbd71ee0ed8aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8ca4e3ecf54e54b0590e0b8d9bcc77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab2d515eac240abb81f6748802566d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ab484bb9f5423babf9712fe72a938c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd24e4f383440199aac3dbb809d8ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7fc4bd451a64f32aeb7b3ea5468e0ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/50][0/20][Time 24.75]\n",
      "Unified LR across all optimizers: 0.00019634054657948372\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.0505\tGen: 0.2745\tRec: 0.2712\tE: 0.0102\tR: 0.0069\tP: 0.2745\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8906\n",
      "precision: 0.8247\n",
      "recall: 0.8791\n",
      "f1_score: 0.8511\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c5ebbea1434aef99782873abecf625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118d0271f9914eb38b22fb60d197c52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87cec62fc5e74330852eda7f2060712c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d159356c984123be456e3dce10805e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45bfeacfb04c4543bafa1c4f418d5aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb71df69fd3f4e948541f253f599ece4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa560602e89f42b7b7dfdac58be48427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd88173b0f5e486fa6eb280e61db8549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3e0bf3921c4815aa12a8d0fc7765a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d53fad853a46df977072c4ce6e9968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30/50][0/20][Time 25.56]\n",
      "Unified LR across all optimizers: 0.00019454048562865856\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.0368\tGen: 0.2521\tRec: 0.2491\tE: 0.0065\tR: 0.0040\tP: 0.2415\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9062\n",
      "precision: 0.8605\n",
      "recall: 0.8605\n",
      "f1_score: 0.8605\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78caa85d722442a6b9c68559dcfd2001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a4092f6a784aa6a67c6893c5bb99d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac71c0d98913458ba534621c448a5d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5c6dfd68024b81890c606aaac70fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351df900438049029d9dc81997237891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0396ca5447d449c38ea39f3c0caaa322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c3025c97ee4e8caedf652fde75c0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6298217c2209439195d93d9547f2215c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879898a33dc54e8487e12bb3b76a2985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0840ec99de43e3b5c6b92d2f15113e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40/50][0/20][Time 25.42]\n",
      "Unified LR across all optimizers: 0.00019275692773582703\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.0339\tGen: 0.2395\tRec: 0.2370\tE: 0.0054\tR: 0.0036\tP: 0.2190\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8945\n",
      "precision: 0.8526\n",
      "recall: 0.8617\n",
      "f1_score: 0.8571\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79eb8443d0c44b4282ad4da5005a2ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4af62972f048e38b2d03dca423548a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402f4c6462b340269fadfb89875e072a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b10c4b4f644cd7a44580020e64ac53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ab7d3b70a841cdb209fb1601d336d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26065af4c52d4e2ea97bf489818817f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0bdb8208d69446f89f7eab1b5944830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a9cbd9083d4604ae21be594e593f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e46ecb09ae4b1abd357acd0d728aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generation loss should close to 0.0\n",
    "load_model = False\n",
    "if load_model:\n",
    "    trainer_hub.load_trainer()\n",
    "else:\n",
    "    trainer_hub.train(trainset, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manipulated Data Shape: torch.Size([2989, 13])\n",
      "Reversed Labels Shape: torch.Size([2989, 1])\n",
      "Original Labels Shape: torch.Size([2989, 1])\n"
     ]
    }
   ],
   "source": [
    "# DataLoader for processing training data in larger batches without shuffling\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset, batch_size=256, shuffle=False, drop_last=False)\n",
    "\n",
    "# CCNet setup for generating synthetic manipulated data\n",
    "ccnet = trainer_hub.ccnet\n",
    "manipulated_data = None\n",
    "random_labels = None\n",
    "original_labels = None\n",
    "probability_positive = 0.6222 # 62.2% of the data is positive\n",
    "\n",
    "# Generate synthetic data to enhance the diversity of the training dataset\n",
    "for data, labels in test_loader:\n",
    "    data = data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Create random labels with 62.2% positive (1) and 37.8% negative (0) labels\n",
    "    random_label = (torch.rand(labels.size(0)).to(device) < probability_positive).float().unsqueeze(-1)\n",
    "\n",
    "    # Use CCNet to explain the original data and generate synthetic counterparts\n",
    "    explanations = ccnet.explain(data)\n",
    "    synthetic_data = ccnet.produce(random_label, explanations)\n",
    "    \n",
    "    # Detach synthetic data and labels from GPU for further processing\n",
    "    synthetic_data = synthetic_data.detach().cpu()\n",
    "    random_label = random_label.detach().cpu()\n",
    "    labels = labels.detach().cpu()\n",
    "\n",
    "    # Accumulate the generated data for analysis and training\n",
    "    if manipulated_data is None:\n",
    "        manipulated_data = synthetic_data\n",
    "        random_labels = random_label\n",
    "        original_labels = labels\n",
    "    else:\n",
    "        manipulated_data = torch.cat([manipulated_data, synthetic_data], dim=0)\n",
    "        random_labels = torch.cat([random_labels, random_label], dim=0)\n",
    "        original_labels = torch.cat([original_labels, labels], dim=0)\n",
    "\n",
    "# Output the shapes of the datasets for verification\n",
    "print(f\"Manipulated Data Shape: {manipulated_data.shape}\")\n",
    "print(f\"Reversed Labels Shape: {random_labels.shape}\")\n",
    "print(f\"Original Labels Shape: {original_labels.shape}\")\n",
    "\n",
    "# Create datasets for both synthetic and original label scenarios\n",
    "random_label_testset = LabeledDataset(manipulated_data.numpy(), random_labels.numpy())\n",
    "original_label_testset = LabeledDataset(manipulated_data.numpy(), original_labels.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Supervised Models\n",
    "\n",
    "This section outlines the process of training supervised learning models using both original and synthetic datasets. The `train_supervised_model` function is designed to iterate through the dataset, perform forward passes, compute loss, and update model weights using backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised_model(model, dataset, num_epoch=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    # Initialize the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # Ensure reproducibility by resetting the random seed\n",
    "    # Create DataLoader for batch processing\n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    # Training loop\n",
    "    for epoch in range(num_epoch):  # Train for 2 epochs as an example\n",
    "        for i, (data, label) in enumerate(trainloader):\n",
    "            data = data.to(device).clone().detach()\n",
    "            label = label.to(device).float()\n",
    "            # Perform forward pass\n",
    "            output = model(data)\n",
    "            # Compute loss\n",
    "            loss = torch.nn.functional.binary_cross_entropy(output, label)\n",
    "            # Backward pass to compute gradients\n",
    "            loss.backward()\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers=4, hidden_size=256):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Create a list to hold all layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(torch.nn.Linear(input_size, hidden_size))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(torch.nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(torch.nn.Linear(hidden_size, output_size))\n",
    "        \n",
    "        # Register all layers\n",
    "        self.layers = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "# Initialize and train a model on the recreated dataset\n",
    "decision_making_model = MLP(input_size=n_features, output_size=n_classes).to(device)\n",
    "train_supervised_model(decision_making_model, trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the supervised learning model tested on the original data:  0.9380873410724158\n",
      "F1 score of the supervised learning model tested on the manipulated data with random label:  0.9989229940764675\n",
      "F1 score of the supervised learning model tested on the manipulated data with original label:  0.6257104194857915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_f1_score(model, input_testset, device, batch_size=64):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    # DataLoader for testing\n",
    "    test_loader = torch.utils.data.DataLoader(input_testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # No gradient computation needed during inference\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(data)\n",
    "            # Process output for binary classification\n",
    "            predicted = (output.squeeze() > 0.5).long()\n",
    "            y_true.extend(label.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Compute and return the F1 score\n",
    "    score = f1_score(y_true, y_pred, average='binary')\n",
    "    return score\n",
    "\n",
    "# Calculate F1 scores for both models\n",
    "fraud_detection_f1_score = get_f1_score(decision_making_model, testset, device)\n",
    "random_testset_f1_score = get_f1_score(decision_making_model, random_label_testset, device)\n",
    "manipulated_testset_f1_score = get_f1_score(decision_making_model, original_label_testset, device)\n",
    "\n",
    "# Output the results\n",
    "print(\"F1 score of the supervised learning model tested on the original data: \", fraud_detection_f1_score)\n",
    "print(\"F1 score of the supervised learning model tested on the manipulated data with random label: \", random_testset_f1_score)\n",
    "print(\"F1 score of the supervised learning model tested on the manipulated data with original label: \", manipulated_testset_f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
