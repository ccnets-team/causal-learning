{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author:\n",
    "        \n",
    "        PARK, JunHo, junho@ccnets.org\n",
    "        \n",
    "    COPYRIGHT (c) 2024. CCNets. All Rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path_append = \"../../\"\n",
    "sys.path.append(path_append)  # Go up one directory from where you are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/competitions/santander-customer-transaction-prediction\n",
    "import pandas as pd \n",
    "\n",
    "file_name = 'Santander Customer Transaction Prediction Dataset'\n",
    "train_dataroot = path_append + f\"../data/{file_name}/train.csv\"\n",
    "test_dataroot = path_append + f\"../data/{file_name}/test.csv\"\n",
    "df_train = pd.read_csv(train_dataroot)\n",
    "df_test = pd.read_csv(test_dataroot)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tools.preprocessing.data_frame import auto_preprocess_dataframe\n",
    "\n",
    "len_train = len(df_train)\n",
    "# df = pd.concat([df_train, df_test], axis=0)\n",
    "df_train, description = auto_preprocess_dataframe(df_train, target_columns=['target'], drop_columns= ['ID_code'])\n",
    "# split the df back into training and test df\n",
    "# df_train = df[:len_train].reset_index(drop=True)\n",
    "# df_test = df[len_train:].reset_index(drop=True)\n",
    "\n",
    "# Calculate the number of features and classes\n",
    "num_features = description['num_features']\n",
    "num_classes = description['num_classes']\n",
    "\n",
    "print(num_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining the labeled and unlabeled dataset classes\n",
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            self.x = torch.tensor(x, dtype=torch.float32)\n",
    "        else:\n",
    "            self.x = x.clone().detach()\n",
    "\n",
    "        if not isinstance(y, torch.Tensor):\n",
    "            self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        else:\n",
    "            self.y = y.clone().detach()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        vals = self.x[index]\n",
    "        label = self.y[index]\n",
    "        return vals, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Splitting for Training and Testing\n",
    "\n",
    "The original dataset is split into training and testing parts to evaluate the model's performance accurately. This step is crucial for validating the effectiveness of the training on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_eval = train_test_split(df_train, test_size=0.2, random_state=42)\n",
    "X_train, y_train = df_train.iloc[:, :-1], df_train.iloc[:, -1:]\n",
    "X_eval, y_eval = df_eval.iloc[:, :-1], df_eval.iloc[:, -1:]\n",
    "\n",
    "# Labeled datasets for supervised learning tasks\n",
    "trainset = LabeledDataset(X_train.values, y_train.values)  # Corrected to include training data\n",
    "evalset = LabeledDataset(X_eval.values, y_eval.values)     # Test set with proper labels\n",
    "\n",
    "# Printing the shapes of the datasets for verification\n",
    "print(f\"Labeled Trainset Shape: {len(trainset)}, {trainset.x.shape[1]}\")\n",
    "print(f\"Labeled Testset Shape: {len(evalset)}, {evalset.x.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Setup and Model Configuration\n",
    "\n",
    "This section initializes the environment by setting a fixed random seed to ensure reproducibility of results. It imports necessary configurations and initializes model parameters with specific configurations. The model specified here is set to have no core model but uses a 'tabnet' encoder model for data processing, which is particularly tailored for structured or tabular data like credit card transactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility of experiments\n",
    "from nn.utils.init import set_random_seed\n",
    "set_random_seed(0)\n",
    "\n",
    "# Importing configuration setups for ML parameters and data\n",
    "from tools.setting.ml_params import MLParameters\n",
    "from tools.setting.data_config import DataConfig\n",
    "from trainer_hub import TrainerHub\n",
    "\n",
    "# Configuration for the data handling, defining dataset specifics and the task type\n",
    "data_config = DataConfig(dataset_name=file_name, task_type='binary_classification', obs_shape=[num_features], label_size=num_classes)\n",
    "\n",
    "# Initializing ML parameters without a core model and setting the encoder model to 'tabnet' with specific configurations\n",
    "ml_params = MLParameters(ccnet_network='tabnet')\n",
    "\n",
    "# Setting training parameters and device configuration\n",
    "ml_params.model.ccnet_config.num_layers = 4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Create a TrainerHub instance to manage training and data processing\n",
    "causal_trainer_from_dataset = TrainerHub(ml_params, data_config, device, use_print=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_trainer_from_dataset.train(trainset, evalset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_trainer_from_dataset.test(evalset)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from tools.report import get_test_results\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configure the model to use 500 trees, each with a maximum depth of 3\n",
    "xgb_model = XGBClassifier(n_estimators=500, max_depth=3)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "eval_predictions = xgb_model.predict(X_eval)\n",
    "metrics = get_test_results(eval_predictions, y_eval.values, task_type = 'binary_classification', num_classes=2)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgb_predictions = xgb_model.predict(X_train)\n",
    "training_data_with_xgb_predictions = LabeledDataset(torch.tensor(X_train.values).detach(), torch.tensor(train_xgb_predictions).unsqueeze(-1).detach())  # Corrected to include training data\n",
    "\n",
    "print(\"LabeledDataset X: \", training_data_with_xgb_predictions.x.shape)\n",
    "print(\"LabeledDataset Y: \", training_data_with_xgb_predictions.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TrainerHub instance to manage training and data processing\n",
    "causal_trainer_from_xgboost = TrainerHub(ml_params, data_config, device, use_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_trainer_from_xgboost.train(training_data_with_xgb_predictions, evalset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_trainer_from_xgboost.test(evalset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
