{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author:\n",
    "        \n",
    "        PARK, JunHo, junho@ccnets.org\n",
    "\n",
    "        \n",
    "        KIM, JeongYoong, jeongyoong@ccnets.org\n",
    "        \n",
    "    COPYRIGHT (c) 2024. CCNets. All Rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection: Proving Causal Generation in Causal Cooperative Network (CCNet)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial delves into the application of the Cooperative Network (CCNet), enhanced with the CounterGenerate method, to address challenges associated with imbalanced datasets in the domain of credit card fraud detection. We aim to leverage CCNet not just as a data augmentation tool but as a causal model capable of generating realistic training data distributions through counterfactual scenarios. By proving that \\(Y\\) (outcomes) and \\(E\\) (explanations) are necessary and sufficient causes within our model, we intend to demonstrate CCNet's ability to perform causal generation, thereby enhancing the diversity, realism, and balance of the training data. This, in turn, is expected to improve the robustness and accuracy of predictive models designed to identify fraudulent transactions.\n",
    "\n",
    "## Tutorial Goals\n",
    "\n",
    "The objectives of this tutorial are to guide you through the innovative use of causal data generation techniques and validate the effectiveness of these methods in a practical setting:\n",
    "\n",
    "### Counterfactual Scenarios\n",
    "- **Manipulate \\(Y\\), Maintain \\(E\\)**: Delve into the dynamics of manipulating the outcome variable \\(Y\\) while keeping the explanation variable \\(E\\) constant to create counterfactually augmented datasets. These datasets will represent balanced and redistributed conditions, allowing us to test the hypothesis that they can accurately mirror real-world data distributions. This approach underscores the causal influence of \\(Y\\) in the data generation process and tests the robustness of \\(E\\) as a stable explanatory factor.\n",
    "\n",
    "### Classifier Training and Causal Model Validation\n",
    "- **Classifier Training**: Engage in the training of classifiers using datasets that have been created under specific causal assumptions to compare their performance metrics:\n",
    "  - **First Classifier**: Trained on the **original dataset**, serving as the baseline for performance comparison.\n",
    "  - **Second Classifier**: Trained on the **redistributed fraud outcome dataset**, designed to maintain the same frequency of fraud as the original dataset. This approach aims to test the model's effectiveness under different distribution conditions while expecting to achieve similar prediction accuracy to the first classifier.\n",
    "  - **Third Classifier**: Trained on a **balanced fraud outcome dataset** where the fraud rate is intentionally reduced to half of its original rate, representing a more severe case of imbalance. This setup challenges the classifier to maintain high detection capabilities under significantly skewed conditions.\n",
    "- **Performance Metrics**: Utilize key metrics such as the F1 score to evaluate and substantiate the causal generation effectiveness of CCNet. \n",
    "- **Counter Generation's Role in Imbalance Mitigation**: Explore how CCNet's counter generation capabilities can be leveraged to synthetically enhance the dataset. By generating realistic, counterfactual instances of both fraudulent and non-fraudulent transactions, CCNet aims to provide a richer, more balanced dataset that helps models learn more effective patterns for fraud detection without the typical bias introduced by skewed data distributions.\n",
    "\n",
    "### Testing and Empirical Validation\n",
    "- **Model Evaluation**: Conduct rigorous evaluations of models across all datasets using a consistent test set to ensure fair comparisons.\n",
    "- **Causal Effectiveness Analysis**: Analyze whether the counterfactually generated data provides a statistically significant improvement in model performance, thereby supporting the hypothesis of effective causal generation.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By the end of this tutorial, participants will not only comprehend the theoretical framework behind using CCNet for causal data generation but will also witness firsthand the practical benefits of this approach in enhancing model performance for fraud detection tasks. This exploration is intended to solidify the understanding of CCNet as a powerful tool for generating realistic and balanced datasets through principled causal mechanisms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path_append = \"../../\"\n",
    "sys.path.append(path_append)  # Go up one directory from where you are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://fraud-detection-handbook.github.io/fraud-detection-handbook/Chapter_7_DeepLearning/FeedForwardNeuralNetworks.html\n",
    "import pandas as pd \n",
    "\n",
    "file_name = 'credit_card_fraud_detection'\n",
    "dataroot = path_append + f\"../data/{file_name}/creditcard.csv\"\n",
    "df = pd.read_csv(dataroot)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No Frauds', round(df['Class'].value_counts()[0] / len(df) *100,2), '%of the dataset')\n",
    "print('Frauds', round(df['Class'].value_counts()[1] / len(df) *100,2), '%of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tools.preprocessing.data_frame import auto_preprocess_dataframe\n",
    "\n",
    "# convert column name time to v0\n",
    "df.rename(columns={'Time': 'V0'}, inplace=True)\n",
    "df, description = auto_preprocess_dataframe(df, target_columns=['Class'])\n",
    "\n",
    "# Calculate the number of features and classes\n",
    "num_features = description['num_features']\n",
    "num_classes = description['num_classes']\n",
    "\n",
    "print(num_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining the labeled and unlabeled dataset classes\n",
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(self.x[index], torch.Tensor):\n",
    "            vals = self.x[index]\n",
    "        else:\n",
    "            vals = torch.tensor(self.x[index], dtype=torch.float32)\n",
    "        if isinstance(self.y[index], torch.Tensor):\n",
    "            label = self.y[index]\n",
    "        else:\n",
    "            label = torch.tensor(self.y[index], dtype=torch.float32)\n",
    "        return vals, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Splitting for Training and Testing\n",
    "\n",
    "The original dataset is split into training and testing parts to evaluate the model's performance accurately. This step is crucial for validating the effectiveness of the training on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset into training and test sets for model evaluation\n",
    "df_train, df_test = train_test_split(df, test_size=0.5, shuffle=False)\n",
    "\n",
    "fraud_counts = df_train['Class'].value_counts(normalize=True)[1]\n",
    "non_fraud_counts = df_train['Class'].value_counts(normalize=True)[0]\n",
    "train_fraud_rate = fraud_counts/(fraud_counts+non_fraud_counts)\n",
    "print(\"fraud_rate: \", train_fraud_rate)\n",
    "\n",
    "X_train, y_train = df_train.iloc[:, :-1].values, df_train.iloc[:, -1:].values\n",
    "X_test, y_test = df_test.iloc[:, :-1].values, df_test.iloc[:, -1:].values\n",
    "\n",
    "# Labeled datasets for supervised learning tasks\n",
    "trainset = LabeledDataset(X_train, y_train)  # Corrected to include training data\n",
    "testset = LabeledDataset(X_test, y_test)     # Test set with proper labels\n",
    "\n",
    "# Printing the shapes of the datasets for verification\n",
    "print(f\"Labeled Trainset Shape: {len(trainset)}, {trainset.x.shape[1]}\")\n",
    "print(f\"Labeled Testset Shape: {len(testset)}, {testset.x.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Setup and Model Configuration\n",
    "\n",
    "This section initializes the environment by setting a fixed random seed to ensure reproducibility of results. It imports necessary configurations and initializes model parameters with specific configurations. The model specified here is set to have no core model but uses a 'tabnet' encoder model for data processing, which is particularly tailored for structured or tabular data like credit card transactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility of experiments\n",
    "from nn.utils.init_layer import set_random_seed\n",
    "set_random_seed(0)\n",
    "\n",
    "# Importing configuration setups for ML parameters and data\n",
    "import torch\n",
    "from tools.setting.ml_params import MLParameters\n",
    "from tools.setting.data_config import DataConfig\n",
    "from trainer_hub import TrainerHub\n",
    "\n",
    "# Configuration for the data handling, defining dataset specifics and the task type\n",
    "data_config = DataConfig(dataset_name='credit_card_fraud_detection', task_type='binary_classification', obs_shape=[num_features], label_size=num_classes, explain_size=num_features-num_classes)\n",
    "\n",
    "# Initializing ML parameters without a core model and setting the encoder model to 'tabnet' with specific configurations\n",
    "ml_params = MLParameters(model_name='tabnet', encoder_network='none')\n",
    "\n",
    "# Setting training parameters and device configuration\n",
    "ml_params.training.num_epoch = 10\n",
    "ml_params.model.num_layers = 4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Create a TrainerHub instance to manage training and data processing\n",
    "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=False, use_wandb=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_hub.train(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading and Counter Generation\n",
    "\n",
    "This section deals with loading the unlabelled dataset, processing it through the trained model to create synthetic data. This data augmentation step is crucial for models that benefit from larger datasets, such as in fraud detection scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "len_trainset = len(trainset)\n",
    "causal_model = trainer_hub.ccnet\n",
    "# Initialize the recreated dataset container\n",
    "\n",
    "# Assuming X_train and y_train are your features and binary labels respectively\n",
    "train_X = torch.tensor(X_train, dtype=torch.float32)\n",
    "train_y = torch.tensor(y_train, dtype=torch.float32)  # Ensure labels are in long dtype for one_hot\n",
    "\n",
    "# Convert binary labels to one-hot encoding\n",
    "# PyTorch's one_hot requires the number of classes, which is 2 for binary labels\n",
    "train_rand_y = torch.rand_like(train_y)  \n",
    "balanced_train_y = torch.where(torch.rand_like(train_rand_y) > 0.5, torch.ones_like(train_y), torch.zeros_like(train_y)).float()\n",
    "redistributed_train_y = torch.where(torch.rand_like(train_rand_y) < train_fraud_rate, torch.ones_like(train_y), torch.zeros_like(train_y)).float()\n",
    "\n",
    "# Now use the one-hot encoded labels for generating data\n",
    "balanced_data = causal_model.causal_generate(train_X.to(device), balanced_train_y.to(device))\n",
    "redistributed_data = causal_model.causal_generate(train_X.to(device), redistributed_train_y.to(device))\n",
    "\n",
    "# Assuming recreated_dataset is a PyTorch tensor already available in your context\n",
    "balanced_trainset = LabeledDataset(balanced_data.clone().detach().cpu(), balanced_train_y)\n",
    "redistributed_trainset = LabeledDataset(redistributed_data.clone().detach().cpu(), redistributed_train_y)\n",
    "\n",
    "print(f\"Original Trainset Shape: {len(trainset)}, {trainset.x.shape[1]}\")\n",
    "print(f\"Balanced Trainset Shape: {len(balanced_trainset)}, {balanced_trainset.x.shape[1]}\")\n",
    "print(f\"Redistributed Trainset Shape: {len(redistributed_trainset)}, {redistributed_trainset.x.shape[1]}\")\n",
    "\n",
    "print(f\"Original Trainset Fraud Rate: {trainset.y.sum().item()/len(trainset)}\")\n",
    "print(f\"Balanced Trainset Fraud Rate: {balanced_trainset.y.sum().item()/len(balanced_trainset)}\")\n",
    "print(f\"Redistributed Trainset Fraud Rate: {redistributed_trainset.y.sum().item()/len(redistributed_trainset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation for Model Training\n",
    "\n",
    "After synthetic data generation, this section separates the data and labels for training purposes, preparing them for use in machine learning models to ensure proper supervision and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn.tabnet import EncoderTabNet as TabNet\n",
    "from tools.setting.ml_params import ModelParameters, CooperativeNetworkConfig\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers=3, hidden_size=256):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        model_config = ModelParameters('tabnet')\n",
    "        model_config.num_layers = num_layers\n",
    "        model_config.d_model = hidden_size\n",
    "        network_config = CooperativeNetworkConfig(model_config, \"Classifier\", input_size, output_size, 'sigmoid')\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Create a list to hold all layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(torch.nn.Linear(input_size, hidden_size))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        ## Add TabNet layers\n",
    "        layers.append(TabNet(network_config))\n",
    "\n",
    "        # Register all layers\n",
    "        self.layers = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Supervised Models\n",
    "\n",
    "This section outlines the process of training supervised learning models using both original and synthetic datasets. The `train_supervised_model` function is designed to iterate through the dataset, perform forward passes, compute loss, and update model weights using backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, dataset, num_epoch=4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    # Initialize the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # Ensure reproducibility by resetting the random seed\n",
    "    # Create DataLoader for batch processing\n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    # Training loop\n",
    "    len_trainset = len(dataset)\n",
    "    for epoch in range(num_epoch):  # Train for 2 epochs as an example\n",
    "        sum_loss = 0\n",
    "        for i, (data, label) in enumerate(trainloader):\n",
    "            data = data.to(device).clone().detach()\n",
    "            label = label.to(device).float()\n",
    "            # Perform forward pass\n",
    "            output = model(data)\n",
    "            # Compute loss\n",
    "            loss = torch.nn.functional.binary_cross_entropy(output, label)\n",
    "            # Backward pass to compute gradients\n",
    "            loss.backward()\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            sum_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}, Loss: {sum_loss/len_trainset}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training Using Recreated and Original Datasets\n",
    "\n",
    "Models are trained using both datasets generated through the Data Augmentation process and the original dataset. This comparison helps to determine the effectiveness of the synthetic data in improving model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train a model on the recreated dataset\n",
    "model_trained_on_balanced = Classifier(input_size=num_features, output_size=num_classes).to(device)\n",
    "train_classifier(model_trained_on_balanced, balanced_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train a model on the recreated dataset\n",
    "model_trained_on_redistributed = Classifier(input_size=num_features, output_size=num_classes).to(device)\n",
    "train_classifier(model_trained_on_redistributed, redistributed_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize and train a model on the original dataset\n",
    "model_trained_on_original = Classifier(input_size=num_features, output_size=num_classes).to(device)\n",
    "train_classifier(model_trained_on_original, trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Model Performance\n",
    "\n",
    "After training, the models are evaluated using the F1 score, a harmonic mean of precision and recall, which is particularly useful in the context of imbalanced datasets like fraud detection. This step is critical for assessing the quality of the models trained on different types of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_f1_score(model, testset, batch_size=256):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    # DataLoader for testing\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # No gradient computation needed during inference\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(data)\n",
    "            # Process output for binary classification\n",
    "            predicted = (output.squeeze() > 0.5).long()\n",
    "            y_true.extend(label.squeeze().long().cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Compute and return the F1 score\n",
    "    score = f1_score(y_true, y_pred, average='binary')\n",
    "    return score\n",
    "\n",
    "# Calculate F1 scores for both models\n",
    "f1_score_original = get_f1_score(model_trained_on_original, testset)\n",
    "f1_score_balanced = get_f1_score(model_trained_on_balanced, testset)\n",
    "f1_score_redistributed = get_f1_score(model_trained_on_redistributed, testset)\n",
    "\n",
    "# Output the results\n",
    "print(\"F1 score of the classifier trained on the original data: \", f1_score_original)\n",
    "print(\"F1 score of the classifier trained on the balanced data: \", f1_score_balanced)\n",
    "print(\"F1 score of the classifier trained on the redistributed data: \", f1_score_redistributed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
