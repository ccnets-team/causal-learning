{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author:\n",
    "        \n",
    "        PARK, JunHo, junho@ccnets.org\n",
    "\n",
    "    COPYRIGHT (c) 2024. CCNets. All Rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheat Your Decision-Making Model: Manipulating Credit Card Data with Causal Inference\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this tutorial, we delve into the intricate world of causal inference to manipulate decision-making models in the domain of credit card fraud detection. Utilizing a Cooperative Network (CCNet), we explore how subtle manipulations in non-fraudulent data can expose and test the vulnerabilities of machine learning models. This demonstration not only highlights the robustness and weaknesses of current fraud detection systems but also illustrates the ethical considerations and potential risks inherent in data manipulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Goals\n",
    "\n",
    "This tutorial provides a guide on using causal inference to manipulate data and evaluate its impact on machine learning models. We will cover:\n",
    "\n",
    "1. **Training a Binary Classifier**: Train a model to distinguish between fraudulent and non-fraudulent transactions using real credit card data.\n",
    "2. **Generating Manipulated Data with CCNet**: Use CCNet's Explainer, Reasoner, and Producer networks to create data that mimics non-fraudulent transactions but is designed to deceive the classifier by switching decision label factors.\n",
    "3. **Evaluating Classifier Performance**: Test the classifier on both original and manipulated datasets to assess robustness. The classifier should perform well on both the original test set and the CCNet-recreated test set, indicating that CCNet has learned the data distribution that the classifier mapped for decision making.\n",
    "4. **Comparative Dataset Analysis**:\n",
    "   - **Testset A**: Real Test Data - Authentic credit card transactions used as a baseline to evaluate the classifier's performance.\n",
    "   - **Testset B**: CCNet-generated data with random labels but maintaining the same frequency distribution as the original. The classifier's performance should be similar to Testset A, showing that CCNet understands the classifier's decision-making process.\n",
    "   - **Testset C**: CCNet-generated data with original labels. With Testset B, it is proved that CCNet creates the genuine data distribution for any case of decision label. However, as most information of the data is in the explanation vector, it is easy to manipulate by changing the decision factor of the fraud data. In short, it is easy to manipulate the decision-making process by causal inference.\n",
    "5. **Ethical Considerations**: Discuss the ethical implications of data manipulation in machine learning, emphasizing the need for robustness in model training and the risks of misuse in applications like fraud detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path_append = \"../\"\n",
    "sys.path.append(path_append)  # Go up one directory from where you are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CCNets-team\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\CCNets-team\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://fraud-detection-handbook.github.io/fraud-detection-handbook/Chapter_7_DeepLearning/FeedForwardNeuralNetworks.html\n",
    "import pandas as pd \n",
    "\n",
    "dataroot = path_append + \"../data/credit_card_fraud_detection/creditcard.csv\"\n",
    "df = pd.read_csv(dataroot)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.83 %of the dataset\n",
      "Frauds 0.17 %of the dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('No Frauds', round(df['Class'].value_counts()[0] / len(df) *100,2), '%of the dataset')\n",
    "print('Frauds', round(df['Class'].value_counts()[1] / len(df) *100,2), '%of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from torch.utils.data import Dataset\n",
    "# Initialize scalers\n",
    "sc = RobustScaler()\n",
    "\n",
    "# Scale all columns except the last one (which is the class column)\n",
    "df.iloc[:, :-1] = sc.fit_transform(df.iloc[:, :-1])\n",
    "\n",
    "# Calculate the number of features and classes\n",
    "n_features = len(df.iloc[:, :-1].columns)\n",
    "n_classes = len(df.iloc[:, -1:].columns)\n",
    "\n",
    "print(n_features, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.994983</td>\n",
       "      <td>-0.616237</td>\n",
       "      <td>-0.098602</td>\n",
       "      <td>1.228905</td>\n",
       "      <td>0.878152</td>\n",
       "      <td>-0.217859</td>\n",
       "      <td>0.631245</td>\n",
       "      <td>0.177406</td>\n",
       "      <td>0.142432</td>\n",
       "      <td>0.334787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026866</td>\n",
       "      <td>0.253109</td>\n",
       "      <td>-0.320791</td>\n",
       "      <td>0.032681</td>\n",
       "      <td>0.167619</td>\n",
       "      <td>-0.241182</td>\n",
       "      <td>0.816731</td>\n",
       "      <td>-0.246091</td>\n",
       "      <td>1.783274</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.994983</td>\n",
       "      <td>0.524929</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>-0.006970</td>\n",
       "      <td>0.293974</td>\n",
       "      <td>0.087726</td>\n",
       "      <td>0.164395</td>\n",
       "      <td>-0.105740</td>\n",
       "      <td>0.117064</td>\n",
       "      <td>-0.164482</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.473332</td>\n",
       "      <td>-0.602719</td>\n",
       "      <td>0.363442</td>\n",
       "      <td>-0.479557</td>\n",
       "      <td>0.225462</td>\n",
       "      <td>0.313475</td>\n",
       "      <td>-0.063781</td>\n",
       "      <td>0.026519</td>\n",
       "      <td>-0.269825</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.994972</td>\n",
       "      <td>-0.615587</td>\n",
       "      <td>-1.002407</td>\n",
       "      <td>0.830932</td>\n",
       "      <td>0.251024</td>\n",
       "      <td>-0.344345</td>\n",
       "      <td>1.778007</td>\n",
       "      <td>0.668164</td>\n",
       "      <td>0.420388</td>\n",
       "      <td>-1.179796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.668917</td>\n",
       "      <td>0.714254</td>\n",
       "      <td>2.974603</td>\n",
       "      <td>-0.919589</td>\n",
       "      <td>-0.515430</td>\n",
       "      <td>-0.153111</td>\n",
       "      <td>-0.350218</td>\n",
       "      <td>-0.540962</td>\n",
       "      <td>4.983721</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.994972</td>\n",
       "      <td>-0.440239</td>\n",
       "      <td>-0.178789</td>\n",
       "      <td>0.841250</td>\n",
       "      <td>-0.529808</td>\n",
       "      <td>0.033775</td>\n",
       "      <td>1.303832</td>\n",
       "      <td>0.175637</td>\n",
       "      <td>0.662489</td>\n",
       "      <td>-1.076888</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.190105</td>\n",
       "      <td>-0.001408</td>\n",
       "      <td>-0.578786</td>\n",
       "      <td>-1.531963</td>\n",
       "      <td>0.944482</td>\n",
       "      <td>-0.298959</td>\n",
       "      <td>0.379163</td>\n",
       "      <td>0.382611</td>\n",
       "      <td>1.418291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.994960</td>\n",
       "      <td>-0.526089</td>\n",
       "      <td>0.579239</td>\n",
       "      <td>0.713861</td>\n",
       "      <td>0.265632</td>\n",
       "      <td>-0.270695</td>\n",
       "      <td>0.317183</td>\n",
       "      <td>0.491625</td>\n",
       "      <td>-0.546463</td>\n",
       "      <td>0.700808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048266</td>\n",
       "      <td>0.739092</td>\n",
       "      <td>-0.407980</td>\n",
       "      <td>0.126293</td>\n",
       "      <td>-0.333308</td>\n",
       "      <td>0.976221</td>\n",
       "      <td>1.347133</td>\n",
       "      <td>1.553716</td>\n",
       "      <td>0.670579</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0 -0.994983 -0.616237 -0.098602  1.228905  0.878152 -0.217859  0.631245   \n",
       "1 -0.994983  0.524929  0.143100 -0.006970  0.293974  0.087726  0.164395   \n",
       "2 -0.994972 -0.615587 -1.002407  0.830932  0.251024 -0.344345  1.778007   \n",
       "3 -0.994972 -0.440239 -0.178789  0.841250 -0.529808  0.033775  1.303832   \n",
       "4 -0.994960 -0.526089  0.579239  0.713861  0.265632 -0.270695  0.317183   \n",
       "\n",
       "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0  0.177406  0.142432  0.334787  ...  0.026866  0.253109 -0.320791  0.032681   \n",
       "1 -0.105740  0.117064 -0.164482  ... -0.473332 -0.602719  0.363442 -0.479557   \n",
       "2  0.668164  0.420388 -1.179796  ...  0.668917  0.714254  2.974603 -0.919589   \n",
       "3  0.175637  0.662489 -1.076888  ... -0.190105 -0.001408 -0.578786 -1.531963   \n",
       "4  0.491625 -0.546463  0.700808  ...  0.048266  0.739092 -0.407980  0.126293   \n",
       "\n",
       "        V25       V26       V27       V28    Amount  Class  \n",
       "0  0.167619 -0.241182  0.816731 -0.246091  1.783274      0  \n",
       "1  0.225462  0.313475 -0.063781  0.026519 -0.269825      0  \n",
       "2 -0.515430 -0.153111 -0.350218 -0.540962  4.983721      0  \n",
       "3  0.944482 -0.298959  0.379163  0.382611  1.418291      0  \n",
       "4 -0.333308  0.976221  1.347133  1.553716  0.670579      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the labeled and unlabeled dataset classes\n",
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        vals = torch.tensor(self.x[index], dtype=torch.float32)\n",
    "        label = torch.tensor(self.y[index], dtype=torch.float32)\n",
    "        return vals, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Splitting for Training and Testing\n",
    "\n",
    "The original dataset is split into training and testing parts to evaluate the model's performance accurately. This step is crucial for validating the effectiveness of the training on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled Trainset Shape: 85442, 30\n",
      "Labeled Testset Shape: 199365, 30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset into training and test sets for model evaluation\n",
    "df_train, df_test = train_test_split(df, test_size=0.7, shuffle=True, random_state=42)\n",
    "X_train, y_train = df_train.iloc[:, :-1].values, df_train.iloc[:, -1:].values\n",
    "X_test, y_test = df_test.iloc[:, :-1].values, df_test.iloc[:, -1:].values\n",
    "\n",
    "# Labeled datasets for supervised learning tasks\n",
    "trainset = LabeledDataset(X_train, y_train)  # Corrected to include training data\n",
    "testset = LabeledDataset(X_test, y_test)     # Test set with proper labels\n",
    "\n",
    "# Printing the shapes of the datasets for verification\n",
    "print(f\"Labeled Trainset Shape: {len(trainset)}, {trainset.x.shape[1]}\")\n",
    "print(f\"Labeled Testset Shape: {len(testset)}, {testset.x.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Setup and Model Configuration\n",
    "\n",
    "This section initializes the environment by setting a fixed random seed to ensure reproducibility of results. It imports necessary configurations and initializes model parameters with specific configurations. The model specified here is set to have no core model but uses a 'tabnet' encoder model for data processing, which is particularly tailored for structured or tabular data like credit card transactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CCNets-team\\anaconda3\\lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility of experiments\n",
    "from nn.utils.init import set_random_seed\n",
    "set_random_seed(0)\n",
    "\n",
    "# Importing configuration setups for ML parameters and data\n",
    "import torch\n",
    "from tools.setting.ml_params import MLParameters\n",
    "from tools.setting.data_config import DataConfig\n",
    "from trainer_hub import TrainerHub\n",
    "\n",
    "# Configuration for the data handling, defining dataset specifics and the task type\n",
    "data_config = DataConfig(dataset_name='CreditCardFraudDetection', task_type='binary_classification', obs_shape=[n_features], label_size=n_classes)\n",
    "\n",
    "# Initializing ML parameters without a core model and setting the encoder model to 'tabnet' with specific configurations\n",
    "ml_params = MLParameters(core_model='tabnet', encoder_model='none')\n",
    "\n",
    "# Setting training parameters and device configuration\n",
    "ml_params.training.num_epoch = 10\n",
    "ml_params.model.core_config.num_layers = 5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Create a TrainerHub instance to manage training and data processing\n",
    "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d9b31465094ebd8ff9293c2f01bc5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc7ecedbd85439c8b4cff236331c882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/1335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][50/1335][Time 8.27]\n",
      "Unified LR across all optimizers: 0.0001995308238189185\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 1.3385\tGen: 0.7746\tRec: 1.5001\tE: 0.6130\tR: 2.0640\tP: 0.9362\n",
      "[0/10][100/1335][Time 8.14]\n",
      "Unified LR across all optimizers: 0.00019907191565870155\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 1.2660\tGen: 0.7274\tRec: 1.4309\tE: 0.5624\tR: 1.9695\tP: 0.8923\n",
      "[0/10][150/1335][Time 7.96]\n",
      "Unified LR across all optimizers: 0.00019861406295796434\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 1.1247\tGen: 0.7519\tRec: 1.3875\tE: 0.4891\tR: 1.7603\tP: 1.0146\n",
      "[0/10][200/1335][Time 7.74]\n",
      "Unified LR across all optimizers: 0.00019815726328921765\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.7815\tGen: 0.7270\tRec: 1.1781\tE: 0.3304\tR: 1.2326\tP: 1.1236\n",
      "[0/10][250/1335][Time 7.89]\n",
      "Unified LR across all optimizers: 0.00019770151423055492\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.5684\tGen: 0.7196\tRec: 1.0609\tE: 0.2271\tR: 0.9098\tP: 1.2120\n",
      "[0/10][300/1335][Time 7.72]\n",
      "Unified LR across all optimizers: 0.00019724681336564005\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.4974\tGen: 0.7288\tRec: 1.0062\tE: 0.2200\tR: 0.7747\tP: 1.2377\n",
      "[0/10][350/1335][Time 7.95]\n",
      "Unified LR across all optimizers: 0.00019679315828369438\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.4605\tGen: 0.7342\tRec: 0.9785\tE: 0.2162\tR: 0.7049\tP: 1.2522\n",
      "[0/10][400/1335][Time 7.68]\n",
      "Unified LR across all optimizers: 0.00019634054657948372\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.4728\tGen: 0.7363\tRec: 0.9635\tE: 0.2456\tR: 0.7000\tP: 1.2269\n",
      "[0/10][450/1335][Time 7.75]\n",
      "Unified LR across all optimizers: 0.00019588897585330582\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.4406\tGen: 0.7702\tRec: 0.9646\tE: 0.2461\tR: 0.6350\tP: 1.2942\n",
      "[0/10][500/1335][Time 7.62]\n",
      "Unified LR across all optimizers: 0.00019543844371097777\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.4316\tGen: 0.7255\tRec: 0.9281\tE: 0.2290\tR: 0.6343\tP: 1.2220\n",
      "[0/10][550/1335][Time 7.99]\n",
      "Unified LR across all optimizers: 0.00019498894776382288\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.4786\tGen: 0.7413\tRec: 0.9898\tE: 0.2301\tR: 0.7271\tP: 1.2526\n",
      "[0/10][600/1335][Time 7.59]\n",
      "Unified LR across all optimizers: 0.00019454048562865856\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.4576\tGen: 0.7448\tRec: 1.0010\tE: 0.2013\tR: 0.7139\tP: 1.2882\n",
      "[0/10][650/1335][Time 7.60]\n",
      "Unified LR across all optimizers: 0.00019409305492778308\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.4268\tGen: 0.7178\tRec: 0.9475\tE: 0.1972\tR: 0.6565\tP: 1.2385\n",
      "[0/10][700/1335][Time 7.60]\n",
      "Unified LR across all optimizers: 0.00019364665328896346\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.3914\tGen: 0.7269\tRec: 0.9086\tE: 0.2097\tR: 0.5731\tP: 1.2441\n",
      "[0/10][750/1335][Time 7.90]\n",
      "Unified LR across all optimizers: 0.00019320127834542263\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.3912\tGen: 0.7354\tRec: 0.9170\tE: 0.2096\tR: 0.5728\tP: 1.2613\n",
      "[0/10][800/1335][Time 7.70]\n",
      "Unified LR across all optimizers: 0.00019275692773582703\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.3755\tGen: 0.7300\tRec: 0.9046\tE: 0.2009\tR: 0.5501\tP: 1.2591\n",
      "[0/10][850/1335][Time 7.82]\n",
      "Unified LR across all optimizers: 0.0001923135991042739\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.3862\tGen: 0.7609\tRec: 0.9207\tE: 0.2264\tR: 0.5460\tP: 1.2954\n",
      "[0/10][900/1335][Time 7.63]\n",
      "Unified LR across all optimizers: 0.0001918712901002789\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.3920\tGen: 0.7456\tRec: 0.9205\tE: 0.2171\tR: 0.5669\tP: 1.2741\n",
      "[0/10][950/1335][Time 7.99]\n",
      "Unified LR across all optimizers: 0.00019142999837876384\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.3842\tGen: 0.7417\tRec: 0.9266\tE: 0.1993\tR: 0.5691\tP: 1.2841\n",
      "[0/10][1000/1335][Time 7.94]\n",
      "Unified LR across all optimizers: 0.00019098972160004388\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.3750\tGen: 0.7187\tRec: 0.9110\tE: 0.1827\tR: 0.5673\tP: 1.2546\n",
      "[0/10][1050/1335][Time 7.81]\n",
      "Unified LR across all optimizers: 0.00019055045742981543\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.4089\tGen: 0.7396\tRec: 0.9417\tE: 0.2068\tR: 0.6110\tP: 1.2725\n",
      "[0/10][1100/1335][Time 7.54]\n",
      "Unified LR across all optimizers: 0.00019011220353914353\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.4159\tGen: 0.7196\tRec: 0.9250\tE: 0.2105\tR: 0.6213\tP: 1.2287\n",
      "[0/10][1150/1335][Time 7.99]\n",
      "Unified LR across all optimizers: 0.00018967495760444968\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.3732\tGen: 0.7486\tRec: 0.9048\tE: 0.2170\tR: 0.5294\tP: 1.2802\n",
      "[0/10][1200/1335][Time 7.88]\n",
      "Unified LR across all optimizers: 0.00018923871730749947\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.3369\tGen: 0.7194\tRec: 0.8721\tE: 0.1843\tR: 0.4896\tP: 1.2546\n",
      "[0/10][1250/1335][Time 7.79]\n",
      "Unified LR across all optimizers: 0.00018880348033539028\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.3397\tGen: 0.7587\tRec: 0.8924\tE: 0.2061\tR: 0.4733\tP: 1.3114\n",
      "[0/10][1300/1335][Time 7.78]\n",
      "Unified LR across all optimizers: 0.00018836924438053897\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.3295\tGen: 0.7330\tRec: 0.8597\tE: 0.2029\tR: 0.4562\tP: 1.2632\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4093a45188354c94b0656e9ff4310478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/1335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10][15/1335][Time 7.97]\n",
      "Unified LR across all optimizers: 0.0001879360071406698\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.3224\tGen: 0.7359\tRec: 0.8635\tE: 0.1949\tR: 0.4500\tP: 1.2770\n",
      "[1/10][65/1335][Time 7.47]\n",
      "Unified LR across all optimizers: 0.000187503766318802\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.3380\tGen: 0.7443\tRec: 0.8785\tE: 0.2037\tR: 0.4722\tP: 1.2849\n",
      "[1/10][115/1335][Time 7.81]\n",
      "Unified LR across all optimizers: 0.00018707251962323787\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.3412\tGen: 0.7518\tRec: 0.8917\tE: 0.2014\tR: 0.4811\tP: 1.3023\n",
      "[1/10][165/1335][Time 7.57]\n",
      "Unified LR across all optimizers: 0.0001866422647675502\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.3231\tGen: 0.7446\tRec: 0.8856\tE: 0.1821\tR: 0.4641\tP: 1.3072\n",
      "[1/10][215/1335][Time 7.70]\n",
      "Unified LR across all optimizers: 0.00018621299947057073\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.3220\tGen: 0.7406\tRec: 0.8786\tE: 0.1840\tR: 0.4600\tP: 1.2972\n",
      "[1/10][265/1335][Time 7.89]\n",
      "Unified LR across all optimizers: 0.00018578472145637737\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.2811\tGen: 0.7236\tRec: 0.8499\tE: 0.1549\tR: 0.4074\tP: 1.2923\n",
      "[1/10][315/1335][Time 7.87]\n",
      "Unified LR across all optimizers: 0.00018535742845428288\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.2657\tGen: 0.7398\tRec: 0.8527\tE: 0.1528\tR: 0.3787\tP: 1.3267\n",
      "[1/10][365/1335][Time 7.53]\n",
      "Unified LR across all optimizers: 0.00018493111819882223\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.2661\tGen: 0.7243\tRec: 0.8387\tE: 0.1517\tR: 0.3804\tP: 1.2969\n",
      "[1/10][415/1335][Time 7.50]\n",
      "Unified LR across all optimizers: 0.00018450578842974107\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.2745\tGen: 0.7262\tRec: 0.8399\tE: 0.1608\tR: 0.3882\tP: 1.2916\n",
      "[1/10][465/1335][Time 7.45]\n",
      "Unified LR across all optimizers: 0.00018408143689198318\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.2457\tGen: 0.7233\tRec: 0.8328\tE: 0.1361\tR: 0.3552\tP: 1.3105\n",
      "[1/10][515/1335][Time 7.91]\n",
      "Unified LR across all optimizers: 0.0001836580613356789\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.2183\tGen: 0.7259\tRec: 0.8105\tE: 0.1338\tR: 0.3029\tP: 1.3181\n",
      "[1/10][565/1335][Time 7.78]\n",
      "Unified LR across all optimizers: 0.0001832356595161332\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.2893\tGen: 0.7291\tRec: 0.8523\tE: 0.1661\tR: 0.4124\tP: 1.2922\n",
      "[1/10][615/1335][Time 7.64]\n",
      "Unified LR across all optimizers: 0.00018281422919381367\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.2606\tGen: 0.7172\tRec: 0.8363\tE: 0.1415\tR: 0.3797\tP: 1.2930\n",
      "[1/10][665/1335][Time 7.68]\n",
      "Unified LR across all optimizers: 0.00018239376813433867\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.2206\tGen: 0.7251\tRec: 0.8137\tE: 0.1320\tR: 0.3093\tP: 1.3182\n",
      "[1/10][715/1335][Time 7.79]\n",
      "Unified LR across all optimizers: 0.00018197427410846564\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.2521\tGen: 0.7323\tRec: 0.8494\tE: 0.1350\tR: 0.3692\tP: 1.3296\n",
      "[1/10][765/1335][Time 7.48]\n",
      "Unified LR across all optimizers: 0.00018155574489207887\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.2129\tGen: 0.7331\tRec: 0.8238\tE: 0.1222\tR: 0.3036\tP: 1.3439\n",
      "[1/10][815/1335][Time 7.38]\n",
      "Unified LR across all optimizers: 0.00018113817826617823\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.1961\tGen: 0.7206\tRec: 0.8088\tE: 0.1079\tR: 0.2843\tP: 1.3334\n",
      "[1/10][865/1335][Time 7.49]\n",
      "Unified LR across all optimizers: 0.00018072157201686696\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.1871\tGen: 0.7482\tRec: 0.8361\tE: 0.0993\tR: 0.2750\tP: 1.3972\n",
      "[1/10][915/1335][Time 7.38]\n",
      "Unified LR across all optimizers: 0.00018030592393534033\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.2077\tGen: 0.7078\tRec: 0.8069\tE: 0.1085\tR: 0.3068\tP: 1.3070\n",
      "[1/10][965/1335][Time 7.58]\n",
      "Unified LR across all optimizers: 0.0001798912318178735\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.1435\tGen: 0.7169\tRec: 0.7640\tE: 0.0964\tR: 0.1906\tP: 1.3373\n",
      "[1/10][1015/1335][Time 7.66]\n",
      "Unified LR across all optimizers: 0.00017947749346581006\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.0600\tGen: 0.7224\tRec: 0.7380\tE: 0.0444\tR: 0.0757\tP: 1.4004\n",
      "[1/10][1065/1335][Time 8.03]\n",
      "Unified LR across all optimizers: 0.0001790647066855505\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.1278\tGen: 0.7174\tRec: 0.7607\tE: 0.0846\tR: 0.1710\tP: 1.3503\n",
      "[1/10][1115/1335][Time 7.56]\n",
      "Unified LR across all optimizers: 0.00017865286928854052\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.1503\tGen: 0.7147\tRec: 0.7689\tE: 0.0961\tR: 0.2045\tP: 1.3333\n",
      "[1/10][1165/1335][Time 7.76]\n",
      "Unified LR across all optimizers: 0.00017824197909125899\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.1848\tGen: 0.7236\tRec: 0.7983\tE: 0.1101\tR: 0.2595\tP: 1.3371\n",
      "[1/10][1215/1335][Time 7.63]\n",
      "Unified LR across all optimizers: 0.00017783203391520723\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Tabnet\n",
      "Inf: 0.2140\tGen: 0.7142\tRec: 0.7992\tE: 0.1290\tR: 0.2989\tP: 1.2994\n"
     ]
    }
   ],
   "source": [
    "load_model = False\n",
    "if load_model:\n",
    "    trainer_hub.load_trainer(core_model = True)\n",
    "else:\n",
    "    trainer_hub.train(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader for processing training data in larger batches without shuffling\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset, batch_size=256, shuffle=False, drop_last=False)\n",
    "\n",
    "# CCNet setup for generating synthetic manipulated data\n",
    "ccnet = trainer_hub.core_ccnet\n",
    "manipulated_data = None\n",
    "random_labels = None\n",
    "original_labels = None\n",
    "probability_positive = 0.0017 # 0.17% of the data is positive\n",
    "\n",
    "# Generate synthetic data to enhance the diversity of the training dataset\n",
    "for data, labels in test_loader:\n",
    "    data = data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Create random labels with 0.17% positive (1) and 99.83% negative (0) labels\n",
    "    random_label = (torch.rand_like(labels) < probability_positive).float()\n",
    "\n",
    "    # Use CCNet to explain the original data and generate synthetic counterparts\n",
    "    explanations = ccnet.explain(data)\n",
    "    synthetic_data = ccnet.produce(random_label, explanations)\n",
    "    \n",
    "    # Detach synthetic data and labels from GPU for further processing\n",
    "    synthetic_data = synthetic_data.detach().cpu()\n",
    "    random_label = random_label.detach().cpu()\n",
    "    labels = labels.detach().cpu()\n",
    "\n",
    "    # Accumulate the generated data for analysis and training\n",
    "    if manipulated_data is None:\n",
    "        manipulated_data = synthetic_data\n",
    "        random_labels = random_label\n",
    "        original_labels = labels\n",
    "    else:\n",
    "        manipulated_data = torch.cat([manipulated_data, synthetic_data], dim=0)\n",
    "        random_labels = torch.cat([random_labels, random_label], dim=0)\n",
    "        original_labels = torch.cat([original_labels, labels], dim=0)\n",
    "\n",
    "# Output the shapes of the datasets for verification\n",
    "print(f\"Manipulated Data Shape: {manipulated_data.shape}\")\n",
    "print(f\"Reversed Labels Shape: {random_labels.shape}\")\n",
    "print(f\"Original Labels Shape: {original_labels.shape}\")\n",
    "\n",
    "# Create datasets for both synthetic and original label scenarios\n",
    "random_label_testset = LabeledDataset(manipulated_data.numpy(), random_labels.numpy())\n",
    "original_label_testset = LabeledDataset(manipulated_data.numpy(), original_labels.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Supervised Models\n",
    "\n",
    "This section outlines the process of training supervised learning models using both original and synthetic datasets. The `train_supervised_model` function is designed to iterate through the dataset, perform forward passes, compute loss, and update model weights using backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised_model(model, dataset, num_epoch=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    # Initialize the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # Ensure reproducibility by resetting the random seed\n",
    "    # Create DataLoader for batch processing\n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    # Training loop\n",
    "    for epoch in range(num_epoch):  # Train for 2 epochs as an example\n",
    "        for i, (data, label) in enumerate(trainloader):\n",
    "            data = data.to(device).clone().detach()\n",
    "            label = label.to(device)\n",
    "            # Perform forward pass\n",
    "            output = model(data)\n",
    "            # Compute loss\n",
    "            loss = torch.nn.functional.binary_cross_entropy(output, label)\n",
    "            # Backward pass to compute gradients\n",
    "            loss.backward()\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers=4, hidden_size=256):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Create a list to hold all layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(torch.nn.Linear(input_size, hidden_size))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(torch.nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(torch.nn.Linear(hidden_size, output_size))\n",
    "        \n",
    "        # Register all layers\n",
    "        self.layers = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "# Initialize and train a model on the recreated dataset\n",
    "decision_making_model = MLP(input_size=n_features, output_size=n_classes).to(device)\n",
    "train_supervised_model(decision_making_model, trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_f1_score(model, input_testset, device, batch_size=64):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    # DataLoader for testing\n",
    "    test_loader = torch.utils.data.DataLoader(input_testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # No gradient computation needed during inference\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(data)\n",
    "            # Process output for binary classification\n",
    "            predicted = (output.squeeze() > 0.5).long()\n",
    "            y_true.extend(label.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Compute and return the F1 score\n",
    "    score = f1_score(y_true, y_pred, average='binary')\n",
    "    return score\n",
    "\n",
    "# Calculate F1 scores for both models\n",
    "fraud_detection_f1_score = get_f1_score(decision_making_model, testset, device)\n",
    "random_testset_f1_score = get_f1_score(decision_making_model, random_label_testset, device)\n",
    "manipulated_testset_f1_score = get_f1_score(decision_making_model, original_label_testset, device)\n",
    "\n",
    "# Output the results\n",
    "print(\"F1 score of the supervised learning model tested on the original data: \", fraud_detection_f1_score)\n",
    "print(\"F1 score of the supervised learning model tested on the manipulated data with random label: \", random_testset_f1_score)\n",
    "print(\"F1 score of the supervised learning model tested on the manipulated data with original label: \", manipulated_testset_f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
