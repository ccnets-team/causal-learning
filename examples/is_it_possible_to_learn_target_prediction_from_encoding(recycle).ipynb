{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Author:\n",
    "        \n",
    "        PARK, JunHo, junho@ccnets.org\n",
    "\n",
    "        \n",
    "        KIM, JoengYoong, jeongyoong@ccnets.org\n",
    "        \n",
    "    COPYRIGHT (c) 2024. CCNets. All Rights reserved.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is it Possible to Learn Target Prediction from Encoding? \n",
    "## Recyclable and Household Waste Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates the use of a cooperative encoding\n",
    "network for causal encoding. The pipeline involves an encoding\n",
    "model (cooperative encoding network with three ResNets) and\n",
    "a core model (three GPTs). It showcases how the encoding\n",
    "process and prediction learning happen internally in the API,\n",
    "as illustrated in the example file.\n",
    "\n",
    "## Problem Definition\n",
    "\n",
    "In many machine learning tasks, effectively encoding data so that\n",
    "the predictive model can learn the underlying patterns and\n",
    "relationships is challenging. Traditional encoding methods, such\n",
    "as autoencoders, often fail to capture the causal relationships\n",
    "and inherent attributes of the data effectively. These methods\n",
    "typically compress the data stochastically, requiring a decoding\n",
    "step with the paired decoder used in training to retrieve the\n",
    "original data before further prediction learning can occur.\n",
    "\n",
    "### Key Issues with Traditional Encoding:\n",
    "1. **Need for Decoding:** Further prediction learning from\n",
    "   compressed data (e.g., large language model learning)\n",
    "   necessitates a decoder.\n",
    "2. **Paired Decoding:** The decoder must be trained together\n",
    "   with the encoder to effectively reconstruct the original data.\n",
    "\n",
    "In contrast, the causal encoding framework directly addresses these\n",
    "issues by uncovering and manipulating independent causal factors\n",
    "and common attributes in dataset observations. This allows for\n",
    "a more structured and meaningful representation of the data,\n",
    "facilitating accurate prediction learning without the need\n",
    "for intermediate decoding steps.\n",
    "\n",
    "### Advantages of Causal Encoding Framework:\n",
    "- **Independent of Prediction Model:** The cooperative encoding\n",
    "  network does not require training or interaction with the\n",
    "  prediction model.\n",
    "- **Structured Representation:** Utilizes both stochastic and\n",
    "  deterministic variables to capture all causal factors in the\n",
    "  latent representation, which is a concatenation of the Explainer\n",
    "  and Reasoner outputs in the encoding network.\n",
    "\n",
    "This structured approach allows the model to learn from the encoded\n",
    "data and make accurate predictions more efficiently.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "[Recyclable and Household Waste Classification](https://www.kaggle.com/datasets/alistairking/recyclable-and-household-waste-classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "path_append = \"../\" # Go up one directory from where you are.\n",
    "sys.path.append(path_append) \n",
    "\n",
    "from tools.setting.ml_params import MLParameters\n",
    "from tools.setting.data_config import DataConfig\n",
    "from nn.utils.init import set_random_seed\n",
    "set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../data/Recyclable and Household Waste Classification/images/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def split_images(root_dir, split_ratio=(0.6, 0.2, 0.2)):\n",
    "    classes = sorted(os.listdir(root_dir))\n",
    "    train_paths, val_paths, test_paths = [], [], []\n",
    "\n",
    "    for class_name in classes:\n",
    "        class_dir = os.path.join(root_dir, class_name)\n",
    "        for subfolder in ['default', 'real_world']:\n",
    "            subfolder_dir = os.path.join(class_dir, subfolder)\n",
    "            if os.path.exists(subfolder_dir):\n",
    "                image_names = os.listdir(subfolder_dir)\n",
    "                random.shuffle(image_names)\n",
    "                num_images = len(image_names)\n",
    "                train_split = int(split_ratio[0] * num_images)\n",
    "                val_split = int(split_ratio[1] * num_images) + train_split\n",
    "\n",
    "                train_paths += [(os.path.join(subfolder_dir, img), class_name) for img in image_names[:train_split]]\n",
    "                val_paths += [(os.path.join(subfolder_dir, img), class_name) for img in image_names[train_split:val_split]]\n",
    "                test_paths += [(os.path.join(subfolder_dir, img), class_name) for img in image_names[val_split:]]\n",
    "    \n",
    "    return train_paths, val_paths, test_paths\n",
    "\n",
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def set_seed(self, seed):\n",
    "        self.seed = seed\n",
    "        set_random_seed(self.seed)\n",
    "        random.shuffle(self.image_paths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def _load_image(self, image_path):\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "class EncoderDataset(BaseDataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        super().__init__(image_paths, transform)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path, _ = self.image_paths[index]\n",
    "        image = self._load_image(image_path)\n",
    "        return image, None\n",
    "\n",
    "class CoreDataset(BaseDataset):\n",
    "    def __init__(self, encoder, image_paths, classes, transform=None, device='cuda', precompute_batches=64):\n",
    "\n",
    "        self.labels = [classes.index(cls) for _, cls in image_paths]\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.device = device\n",
    "        self.precompute_batches = precompute_batches\n",
    "        self.X_cache = None\n",
    "        self.y_cache = None\n",
    "        self.total_iters = 0\n",
    "        self.batch_indices = []\n",
    "        self.seed = 0\n",
    "        super().__init__(image_paths, transform)\n",
    "        self.dataset_length = len(self.image_paths)\n",
    "        self._shuffle_indices()\n",
    "        self._precompute_batches(0)\n",
    "\n",
    "    def _shuffle_indices(self):\n",
    "        set_random_seed(self.seed + self.total_iters)\n",
    "        self.batch_indices = torch.randperm(self.dataset_length)\n",
    "\n",
    "    def _precompute_batches(self, start_idx):\n",
    "        end_idx = min(start_idx + self.precompute_batches, self.dataset_length)\n",
    "        batch_indices = self.batch_indices[start_idx:end_idx].tolist()\n",
    "        \n",
    "        images = [self._load_image(self.image_paths[i][0]) for i in batch_indices]\n",
    "        images = torch.stack(images).to(self.device)\n",
    "        \n",
    "        labels = torch.tensor([self.labels[i] for i in batch_indices], dtype=torch.long).unsqueeze(-1)\n",
    "        \n",
    "        codes = self.encoder.encode(images)\n",
    "        self.X_cache = codes\n",
    "        self.y_cache = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_idx = idx // self.precompute_batches\n",
    "        batch_start_idx = batch_idx * self.precompute_batches\n",
    "        cur_idx = idx % self.precompute_batches\n",
    "        \n",
    "        if self.total_iters % self.dataset_length == 0:\n",
    "            self._shuffle_indices()\n",
    "        if self.total_iters % self.precompute_batches == 0:\n",
    "            self._precompute_batches(batch_start_idx)\n",
    "        \n",
    "        self.total_iters += 1\n",
    "        \n",
    "        if cur_idx >= len(self.X_cache):\n",
    "            cur_idx = idx % len(self.X_cache)\n",
    "        return self.X_cache[cur_idx], self.y_cache[cur_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Create the datasets and data loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "num_classes = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_config = DataConfig(dataset_name = 'recycle_image', task_type='multi_class_classification', obs_shape=[3, 128, 128], label_size=num_classes)\n",
    "\n",
    "# Set the training parameters for the `encoder model`\n",
    "encoder_params = MLParameters(core_model = 'none', encoder_model = 'resnet')\n",
    "\n",
    "# Set the training parameters for the `core model`\n",
    "core_params = MLParameters(core_model = 'gpt', encoder_model = 'none')\n",
    "\n",
    "encoder_params.training.num_epoch = 1\n",
    "core_params.training.num_epoch = 1\n",
    "encoder_params.encoder_config.num_layers = 5\n",
    "encoder_params.encoder_config.d_model = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ccn-team\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ccn-team\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from trainer_hub import TrainerHub\n",
    "\n",
    "# Set the device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# encoder_hub is for the encoder model\n",
    "encoder_hub = TrainerHub(encoder_params, data_config, device, use_print=True, use_wandb=False)\n",
    "\n",
    "# core_hub is for the core model\n",
    "core_hub = TrainerHub(core_params, data_config, device, use_print=True, use_wandb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use function `encode_inputs` in `trainer_hub1.helper` \n",
    "encoder = encoder_hub.encoder_ccnet\n",
    "\n",
    "root_dir = path_append + dataset_path\n",
    "classes = sorted(os.listdir(root_dir))\n",
    "train_paths, val_paths, test_paths = split_images(root_dir)\n",
    "\n",
    "# Create datasets\n",
    "encoder_train_dataset = EncoderDataset(train_paths, transform=transform)\n",
    "\n",
    "core_train_dataset = CoreDataset(encoder, train_paths, classes, transform=transform)\n",
    "core_val_dataset = CoreDataset(encoder, val_paths, classes, transform=transform)\n",
    "test_val_dataset = CoreDataset(encoder, test_paths, classes, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Encoder Epoch 0 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735a2c14bd8845e7aafd696662a7b75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980fc89b3b30404d97b087cd9f8383a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][50/140][Time 16.94]\n",
      "Unified LR across all optimizers: 0.0001995308238189185\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(encoder):  Three Resnet\n",
      "Inf: 1.2369\tGen: 1.1362\tRec: 1.3511\tE: 1.0219\tR: 1.4519\tP: 1.2504\n",
      "[0/1][100/140][Time 16.42]\n",
      "Unified LR across all optimizers: 0.00019907191565870155\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(encoder):  Three Resnet\n",
      "Inf: 1.1123\tGen: 1.0689\tRec: 1.2009\tE: 0.9803\tR: 1.2443\tP: 1.1574\n",
      "========== Core Epoch 0 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d473720006f490b8fc2d489a99c3c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121dd86170364c9f9efb47b8128250be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][50/140][Time 11.49]\n",
      "Unified LR across all optimizers: 0.0001995308238189185\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Gpt\n",
      "Inf: 0.2921\tGen: 0.5550\tRec: 0.4717\tE: 0.3754\tR: 0.2088\tP: 0.7345\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.0312\n",
      "precision: 0.0027\n",
      "recall: 0.0222\n",
      "f1_score: 0.0048\n",
      "\n",
      "[0/1][100/140][Time 11.44]\n",
      "Unified LR across all optimizers: 0.00019907191565870155\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Gpt\n",
      "Inf: 0.0816\tGen: 0.2577\tRec: 0.2407\tE: 0.0986\tR: 0.0645\tP: 0.4168\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.0664\n",
      "precision: 0.0128\n",
      "recall: 0.0492\n",
      "f1_score: 0.0160\n",
      "\n",
      "========== Encoder Epoch 1 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b06864ee1b4518a2d0210d0e4d71db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7ada755b0d487c86c33fe5a8cd9c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][50/140][Time 63.03]\n",
      "Unified LR across all optimizers: 0.00019824853909673957\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(encoder):  Three Resnet\n",
      "Inf: 1.9668\tGen: 1.8835\tRec: 2.0800\tE: 1.7703\tR: 2.1634\tP: 1.9966\n",
      "[0/1][100/140][Time 16.50]\n",
      "Unified LR across all optimizers: 0.000197792580109545\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(encoder):  Three Resnet\n",
      "Inf: 1.0759\tGen: 1.0243\tRec: 1.1316\tE: 0.9686\tR: 1.1832\tP: 1.0800\n",
      "========== Core Epoch 1 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28692393314541a6a96ca0fb3eb16688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299d1e24b3d6443db54e71498fedb684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][50/140][Time 68.99]\n",
      "Unified LR across all optimizers: 0.00019824853909673957\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Gpt\n",
      "Inf: 0.1141\tGen: 0.6790\tRec: 0.6658\tE: 0.1274\tR: 0.1008\tP: 1.2307\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.0195\n",
      "precision: 0.0007\n",
      "recall: 0.0333\n",
      "f1_score: 0.0013\n",
      "\n",
      "[0/1][100/140][Time 11.36]\n",
      "Unified LR across all optimizers: 0.000197792580109545\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(core):  Three Gpt\n",
      "Inf: 0.0551\tGen: 0.3333\tRec: 0.3281\tE: 0.0603\tR: 0.0499\tP: 0.6063\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.0195\n",
      "precision: 0.0007\n",
      "recall: 0.0333\n",
      "f1_score: 0.0013\n",
      "\n",
      "========== Encoder Epoch 2 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a84b09b40940158df6e6eb61f74b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5897b8a31cec4660958ee70202b2358e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][50/140][Time 63.95]\n",
      "Unified LR across all optimizers: 0.00019697449497657537\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(encoder):  Three Resnet\n",
      "Inf: 1.9213\tGen: 1.8351\tRec: 2.0073\tE: 1.7491\tR: 2.0935\tP: 1.9211\n",
      "[0/1][100/140][Time 16.49]\n",
      "Unified LR across all optimizers: 0.00019652146620954448\n",
      "--------------------Training Metrics--------------------\n",
      "Cooperative Network(encoder):  Three Resnet\n",
      "Inf: 1.0847\tGen: 1.0232\tRec: 1.1261\tE: 0.9818\tR: 1.1876\tP: 1.0646\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"=\"*10,\"Encoder Epoch\", i,\"=\"*10)\n",
    "    # Train the encoder models with the encoder dataset\n",
    "    encoder_train_dataset.set_seed(i)\n",
    "    encoder_hub.train(encoder_train_dataset)\n",
    "    \n",
    "    print(\"=\"*10,\"Core Epoch\", i,\"=\"*10)\n",
    "    # Train the core models with encoded inputs\n",
    "    core_train_dataset.set_seed(i)\n",
    "    core_hub.train(core_train_dataset, core_val_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
