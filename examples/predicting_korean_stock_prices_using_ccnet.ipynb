{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "path_append = \"../\"\n",
        "sys.path.append(path_append)  # Go up one directory from where you are.\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import glob\n",
        "import tqdm\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "    TRD_DD : Date\n",
        "    ISU_CD : Stock Code\n",
        "    ISU_NM : Stock Name\n",
        "    TDD_CLSPRC : Closing Price\n",
        "    TDD_OPNPRC : Opening Price\n",
        "    TDD_HGPRC : High Price\n",
        "    TDD_LWPRC : Low Price\n",
        "    MKTCAP : Market Capitalization\n",
        "    ACC_TRDVOL : Trading Volume\n",
        "    EPS : Earnings Per Share\n",
        "    PER : Price-Earnings Ratio\n",
        "    BPS : Book Value Per Share\n",
        "    PBR : Price-Book Ratio\n",
        "    DPS : Dividends Per Share\n",
        "    DVD_YLD : Dividend Yield\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def load_and_merge_csv_files(data_directory, preprocessed_directory, file_limit=None):\n",
        "    data_files = glob.glob(os.path.join(data_directory, \"*.csv\"))\n",
        "    preprocessed_files = glob.glob(os.path.join(preprocessed_directory, \"*.csv\"))\n",
        "    \n",
        "    data_files = data_files[:file_limit]\n",
        "    preprocessed_files = preprocessed_files[:file_limit]\n",
        "    \n",
        "    merged_dfs = []\n",
        "    \n",
        "    for data_file in data_files:\n",
        "        file_name = os.path.basename(data_file)\n",
        "        \n",
        "        preprocessed_file_name = file_name.replace('.csv', '_preprocessed.csv')\n",
        "        preprocessed_file_path = os.path.join(preprocessed_directory, preprocessed_file_name)\n",
        "        \n",
        "        if preprocessed_file_path in preprocessed_files:\n",
        "            df_data = pd.read_csv(data_file)\n",
        "            df_preprocessed = pd.read_csv(preprocessed_file_path)\n",
        "            \n",
        "            merged_df = pd.merge(df_data, df_preprocessed, on='TRD_DD', suffixes=('_data', '_preprocessed'))\n",
        "            merged_dfs.append(merged_df)\n",
        "    \n",
        "    total_df = pd.concat(merged_dfs, ignore_index=True)\n",
        "    \n",
        "    return total_df\n",
        "\n",
        "data_directory = path_append + \"../data/KR_Data/data\"\n",
        "preprocessed_directory = path_append + \"../data/KR_Data/preprocessed\"\n",
        "total_df = load_and_merge_csv_files(data_directory, preprocessed_directory)\n",
        "\n",
        "total_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming total_df is already defined and merged from previous steps\n",
        "\n",
        "# Reverse the DataFrame to sort dates from past to present\n",
        "total_df = total_df[::-1].reset_index(drop=True)\n",
        "\n",
        "# Split the \"TRD_DD\" column into year, month, and day columns\n",
        "total_df[[\"Y\", \"M\", \"D\"]] = total_df[\"TRD_DD\"].str.split(\"/\", expand=True)\n",
        "\n",
        "# Drop the original \"TRD_DD\" column\n",
        "total_df = total_df.drop(\"TRD_DD\", axis=1)\n",
        "\n",
        "# Rearrange columns to have year, month, and day first\n",
        "total_df = total_df[[\"Y\", \"M\", \"D\"] + total_df.columns[:-3].to_list()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new 'Date' column by combining 'Y', 'M', 'D' columns\n",
        "total_df['Date'] = pd.to_datetime(total_df[['Y', 'M', 'D']].rename(columns={'Y': 'year', 'M': 'month', 'D': 'day'}))\n",
        "\n",
        "# Set 'Date' as the index\n",
        "total_df.set_index('Date', inplace=True)\n",
        "\n",
        "# Create a 'count_day' column that represents the number of days from the first date\n",
        "total_df['count_day'] = (total_df.index - total_df.index.min()).days\n",
        "\n",
        "# Drop the 'Y', 'M', 'Day' columns as they're no longer needed\n",
        "total_df.drop(columns=['Y', 'M', 'D'], inplace=True)\n",
        "\n",
        "# Reorder the columns to make 'count_day' first\n",
        "cols = ['count_day'] + [col for col in total_df.columns if col != 'count_day']\n",
        "total_df = total_df[cols]\n",
        "\n",
        "total_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_df.drop(['ISU_CD'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display non-NaN values of the columns to be dropped (for verification)\n",
        "print(\"EPS non-NaN values:\\n\", total_df[\"EPS\"].dropna())\n",
        "print(\"PER non-NaN values:\\n\", total_df[\"PER\"].dropna())\n",
        "print(\"BPS non-NaN values:\\n\", total_df[\"BPS\"].dropna())\n",
        "print(\"PBR non-NaN values:\\n\", total_df[\"PBR\"].dropna())\n",
        "print(\"DPS non-NaN values:\\n\", total_df[\"DPS\"].dropna())\n",
        "print(\"DVD_YLD non-NaN values:\\n\", total_df[\"DVD_YLD\"].dropna())\n",
        "\n",
        "# Drop the unusable columns\n",
        "total_df = total_df.drop([\"EPS\", \"PER\", \"BPS\", \"PBR\", \"DPS\", \"DVD_YLD\"], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming total_df is already defined and filled with NaN values replaced by 0\n",
        "# total_df = ...\n",
        "\n",
        "# 1) Set TREND to 0 for any value that is not -1, 0, or 1\n",
        "total_df.loc[~total_df[\"TREND\"].isin([-1, 0, 1]), \"TREND\"] = 0\n",
        "\n",
        "# 2) Set TREND to -1 for negative values and 1 for positive values\n",
        "total_df.loc[total_df[\"TREND\"] < 0, \"TREND\"] = -1\n",
        "total_df.loc[total_df[\"TREND\"] > 0, \"TREND\"] = 1\n",
        "\n",
        "# 3) Adjust TREND values based on the specified conditions\n",
        "total_df.loc[total_df[\"TREND\"] <= -0.5, \"TREND\"] = -1\n",
        "total_df.loc[total_df[\"TREND\"] >= 0.5, \"TREND\"] = 1\n",
        "total_df.loc[(total_df[\"TREND\"] > -0.5) & (total_df[\"TREND\"] < 0.5), \"TREND\"] = 0\n",
        "\n",
        "# Check the unique values in the TREND column and their counts\n",
        "unique_trends = set(total_df[\"TREND\"])\n",
        "trend_counts = total_df[\"TREND\"].value_counts()\n",
        "\n",
        "# Print the unique values and their counts\n",
        "print(\"Unique TREND values:\", unique_trends)\n",
        "print(\"TREND value counts:\\n\", trend_counts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_df[\"TREND\"] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_df[\"TREND\"] = total_df[\"TREND\"].convert_dtypes(int)\n",
        "total_df[\"TREND\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# List of columns to convert from strings to numeric values\n",
        "columns_to_convert = [\"TDD_CLSPRC\", \"TDD_OPNPRC\", \"TDD_HGPRC\", \"TDD_LWPRC\", \"MKTCAP\", \"ACC_TRDVOL\"]\n",
        "\n",
        "# Convert the columns to numeric values\n",
        "for col in columns_to_convert:\n",
        "    total_df[col] = total_df[col].str.replace(pat=r'[^0-9]', repl=r'' ,regex=True).apply(pd.to_numeric)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
        "\n",
        "# Assuming total_df is already defined and filled with NaN values replaced by 0\n",
        "# total_df = ...\n",
        "\n",
        "# Define the scalers\n",
        "mm = MinMaxScaler()\n",
        "sc = RobustScaler()\n",
        "\n",
        "# total_df = total_df[['ISU_NM', 'TDD_CLSPRC', 'TDD_OPNPRC', 'TDD_HGPRC', 'TDD_LWPRC', 'MKTCAP', 'ACC_TRDVOL', 'TREND']]\n",
        "\n",
        "# # Apply MinMax scaling to the specified columns\n",
        "minmax_cols = [\"count_day\", \"TDD_CLSPRC\", \"TDD_OPNPRC\", \"TDD_HGPRC\", \"TDD_LWPRC\"]\n",
        "for col in minmax_cols:\n",
        "    total_df[col] = mm.fit_transform(total_df[col].values.reshape(-1, 1))\n",
        "\n",
        "# Apply Robust scaling to the specified columns\n",
        "robust_cols = [\"MKTCAP\", \"ACC_TRDVOL\"]\n",
        "for col in robust_cols:\n",
        "    total_df[col] = sc.fit_transform(total_df[col].values.reshape(-1, 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure 'ISU_NM' is of string type\n",
        "total_df[\"ISU_NM\"] = total_df[\"ISU_NM\"].astype(str)\n",
        "\n",
        "# Calculate where 'ISU_NM' column changes value\n",
        "isu_nm_changes = total_df['ISU_NM'].shift() != total_df['ISU_NM']\n",
        "change_indices = [0] + isu_nm_changes[isu_nm_changes].index.tolist() + [len(total_df)]\n",
        "\n",
        "# Compute pairs of (start, end) indices\n",
        "segment_pairs = [(change_indices[i], change_indices[i+1]) for i in range(len(change_indices) - 1)]\n",
        "\n",
        "print(\"Pairs of (start, end) indices:\", segment_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop ISU_NM if it exists\n",
        "if \"ISU_NM\" in total_df.columns:\n",
        "    total_df = total_df.drop(\"ISU_NM\", axis=1)\n",
        "else:\n",
        "    print(\"Column 'ISU_NM' not found in DataFrame\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dask.dataframe as dd\n",
        "\n",
        "# Convert to Dask DataFrame\n",
        "ddf = dd.from_pandas(total_df, npartitions=10)\n",
        "\n",
        "# Apply pd.to_numeric in a distributed manner\n",
        "ddf_numeric = ddf.map_partitions(lambda df: df.apply(pd.to_numeric, errors='coerce')).compute()\n",
        "\n",
        "ddf_numeric.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def convert_nullable_int_columns(df):\n",
        "\n",
        "    int_columns = df.select_dtypes(include=['Int64']).columns\n",
        "    for col in int_columns:\n",
        "        df[col] = df[col].astype('int64')\n",
        "    return df\n",
        "\n",
        "def process_dataframe(df, segments):\n",
        "\n",
        "    df_numeric = df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    df_numeric = convert_nullable_int_columns(df_numeric)\n",
        "\n",
        "    df_numeric = df_numeric.dropna()\n",
        "\n",
        "    df_tensor = torch.tensor(df_numeric.values, dtype=torch.float64).cuda()\n",
        "\n",
        "    processed_tensor = torch.cat([df_tensor[start:end] for start, end in segments], dim=0)\n",
        "\n",
        "    processed_df = pd.DataFrame(processed_tensor.cpu().numpy(), columns=df_numeric.columns)\n",
        "\n",
        "    return processed_df, segments\n",
        "\n",
        "total_df, segment_pairs = process_dataframe(total_df, segment_pairs)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Overview and Usage Guide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\CCNets-team\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
            "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
            "c:\\Users\\CCNets-team\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "\n",
        "use_save_file = True\n",
        "if use_save_file:\n",
        "    import sys\n",
        "    path_append = \"../\"\n",
        "    sys.path.append(path_append)  # Go up one directory from where you are.\n",
        "\n",
        "    from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, LabelEncoder\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "    \n",
        "    total_df = pd.read_parquet('processed_total_df.parquet')\n",
        "    # # Load the list from the pickle file\n",
        "    with open('segment_pairs.pkl', 'rb') as f:\n",
        "        segment_pairs = pickle.load(f)\n",
        "else:\n",
        "    # Save the processed DataFrame to a Parquet file\n",
        "    total_df.to_parquet('processed_total_df.parquet')\n",
        "    # save segment_pairs\n",
        "    with open('segment_pairs.pkl', 'wb') as f:\n",
        "        pickle.dump(segment_pairs, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n- Data Overview\\n    Preprocessed Data: total_df\\n    Categorical Columns: Y, M, D, ISU_CD, GDC_sig, RSI_sig, ROC_sig, MAP_sig, STC_sig\\n    Numerical Columns: TDD_CLSPRC, TDD_OPNPRC, TDD_HGPRC, TDD_LWPRC, MKTCAP, ACC_TRDVOL\\n    Label: TREND\\n\\n- Considerations:\\n    1) It is recommended to use embedding techniques for categorical data.\\n    2) Labels:\\n        NaN values have been replaced with 0.\\n\\n        2-1) Label Processing:\\n            How to handle -1, 0, 1 depends on the definition.\\n            ● Classification of -1, 0, 1:\\n                Commonly, the label being discrete is an issue.\\n                (1) Set to -1 for values less than 0, and 1 for values greater than 0.\\n                    # Ratio of -1, 0, 1 = 1397:1440:55\\n                    : This results in very frequent trading.\\n\\n                (2) Use only -1, 0, 1.\\n                    # Ratio of -1, 0, 1 = 76:2740:76\\n                    : This might cause the model to miss buying opportunities when it should, making it difficult for the model to make accurate predictions.\\n\\n                (3) Set to -1 for values less than -0.5, and 1 for values greater than 0.5, otherwise 0.\\n                    # Ratio of -1, 0, 1 = 752:1409:731\\n                    : (Current preprocessing state) This provides a somewhat balanced ratio.\\n\\n            ● Regression:\\n                Keep the label as it is.\\n                (1) The model performs regression and decides whether to buy or sell based on the predicted increase or decrease.\\n\\n    3) The utility of GDC, RSI, ROC, MAP, STC indicators for learning is uncertain.\\n'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Data Overview and Usage Guide\n",
        "\n",
        "\"\"\"\n",
        "- Data Overview\n",
        "    Preprocessed Data: total_df\n",
        "    Categorical Columns: Y, M, D, ISU_CD, GDC_sig, RSI_sig, ROC_sig, MAP_sig, STC_sig\n",
        "    Numerical Columns: TDD_CLSPRC, TDD_OPNPRC, TDD_HGPRC, TDD_LWPRC, MKTCAP, ACC_TRDVOL\n",
        "    Label: TREND\n",
        "\n",
        "- Considerations:\n",
        "    1) It is recommended to use embedding techniques for categorical data.\n",
        "    2) Labels:\n",
        "        NaN values have been replaced with 0.\n",
        "\n",
        "        2-1) Label Processing:\n",
        "            How to handle -1, 0, 1 depends on the definition.\n",
        "            ● Classification of -1, 0, 1:\n",
        "                Commonly, the label being discrete is an issue.\n",
        "                (1) Set to -1 for values less than 0, and 1 for values greater than 0.\n",
        "                    # Ratio of -1, 0, 1 = 1397:1440:55\n",
        "                    : This results in very frequent trading.\n",
        "\n",
        "                (2) Use only -1, 0, 1.\n",
        "                    # Ratio of -1, 0, 1 = 76:2740:76\n",
        "                    : This might cause the model to miss buying opportunities when it should, making it difficult for the model to make accurate predictions.\n",
        "\n",
        "                (3) Set to -1 for values less than -0.5, and 1 for values greater than 0.5, otherwise 0.\n",
        "                    # Ratio of -1, 0, 1 = 752:1409:731\n",
        "                    : (Current preprocessing state) This provides a somewhat balanced ratio.\n",
        "\n",
        "            ● Regression:\n",
        "                Keep the label as it is.\n",
        "                (1) The model performs regression and decides whether to buy or sell based on the predicted increase or decrease.\n",
        "\n",
        "    3) The utility of GDC, RSI, ROC, MAP, STC indicators for learning is uncertain.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SequentialDataset(Dataset):\n",
        "    def __init__(self, df, segment_pairs, max_window_size):\n",
        "        self.df = df\n",
        "        self.segment_pairs = segment_pairs\n",
        "        self.max_window_size = max_window_size\n",
        "        self.min_window_size = max_window_size // 2\n",
        "        \n",
        "        # Compute the total number of possible subsequences\n",
        "        self.subsequence_lengths = [\n",
        "            max(0, end - start - self.min_window_size + 1)\n",
        "            for start, end in self.segment_pairs\n",
        "        ]\n",
        "        self.cumulative_lengths = [sum(self.subsequence_lengths[:i+1]) for i in range(len(self.subsequence_lengths))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.cumulative_lengths[-1]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Find the appropriate segment for the given idx\n",
        "        segment_idx = next(i for i, cum_len in enumerate(self.cumulative_lengths) if cum_len > idx)\n",
        "        \n",
        "        # Adjust idx to be relative to the found segment\n",
        "        if segment_idx > 0:\n",
        "            idx -= self.cumulative_lengths[segment_idx - 1]\n",
        "        \n",
        "        start_idx, end_idx = self.segment_pairs[segment_idx]\n",
        "        actual_start_idx = start_idx + idx\n",
        "        \n",
        "        # Ensure we stay within the bounds of the current segment\n",
        "        window_size = random.randint(self.min_window_size, self.max_window_size)\n",
        "        seq_end_idx = min(actual_start_idx + window_size, end_idx)\n",
        "\n",
        "        seq = self.df.iloc[actual_start_idx:seq_end_idx]\n",
        "\n",
        "        X = seq.drop(['TREND'], axis=1)[1:]\n",
        "        y = seq['TREND'][:-1]\n",
        "        \n",
        "        # y = torch.tensor(y.values, dtype=torch.float32)\n",
        "        X = torch.tensor(X.values, dtype=torch.float32)\n",
        "        y = torch.tensor(y.values, dtype=torch.long)\n",
        "        y = F.one_hot(y, num_classes=3).float()\n",
        "        \n",
        "        data = torch.cat([X, y], dim=1)\n",
        "        return data, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train indices:  3926\n",
            "Test indices:  982\n",
            "torch.Size([71, 15])\n",
            "6984787\n"
          ]
        }
      ],
      "source": [
        "from random import shuffle\n",
        "\n",
        "# # Assuming 'total_df' contains the stock data and 'segment_pairs' is a list of (stock_id, period_indices) pairs\n",
        "max_window_size = 128\n",
        "\n",
        "# Assuming 'df' and 'num_classes' are defined\n",
        "train_indices, test_indices = segment_pairs[:int(0.8 * len(segment_pairs))], segment_pairs[int(0.8 * len(segment_pairs)):]\n",
        "\n",
        "trainset = SequentialDataset(df=total_df, segment_pairs=train_indices, max_window_size=max_window_size)\n",
        "testset = SequentialDataset(df=total_df, segment_pairs=test_indices, max_window_size=max_window_size)\n",
        "\n",
        "print('Train indices: ', len(train_indices))\n",
        "print('Test indices: ', len(test_indices))\n",
        "\n",
        "print(trainset[0][0].shape)\n",
        "print(len(trainset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\CCNets-team\\anaconda3\\lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n"
          ]
        }
      ],
      "source": [
        "from tools.setting.ml_params import MLParameters\n",
        "from tools.setting.data_config import DataConfig\n",
        "from nn.utils.init import set_random_seed\n",
        "set_random_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([112, 15])\n",
            "Label shape: torch.Size([112, 3])\n",
            "Total number of samples in trainset: 6984787\n"
          ]
        }
      ],
      "source": [
        "data_config = DataConfig(dataset_name = 'stock_price', task_type='multi_class_classification', obs_shape=[15], label_size=3)\n",
        "\n",
        "#  Set training configuration from the AlgorithmConfig class, returning them as a Namespace object.\n",
        "ml_params = MLParameters(ccnet_network = 'gpt', encoder_network = 'none')\n",
        "ml_params.training.num_epoch = 100\n",
        "ml_params.training.batch_size = 64\n",
        "\n",
        "first_data = trainset[0]\n",
        "X, y = first_data\n",
        "\n",
        "print(f\"Input shape: {X.shape}\")\n",
        "print(f\"Label shape: {y.shape}\")\n",
        "\n",
        "print(f\"Total number of samples in trainset: {len(trainset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trainer_hub import TrainerHub\n",
        "\n",
        "# Set the device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "\n",
        "# Initialize the TrainerHub class with the training configuration, data configuration, device, and use_print and use_wandb flags\n",
        "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4cf5b382b6664e1b830273acfee22313",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1ab75159b7642a9af0cba16cbcd4c6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Iterations:   0%|          | 0/109137 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0/100][50/109137][Time 19.47]\n",
            "Unified LR across all optimizers: 0.0001995308238189185\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0424\tGen: 0.3491\tRec: 0.3428\tE: 0.0479\tR: 0.0353\tP: 0.6502\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 0.9531\n",
            "precision: 0.3177\n",
            "recall: 0.3333\n",
            "f1_score: 0.3253\n",
            "\n",
            "[0/100][100/109137][Time 18.52]\n",
            "Unified LR across all optimizers: 0.00019907191565870155\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0201\tGen: 0.2491\tRec: 0.2519\tE: 0.0173\tR: 0.0229\tP: 0.4840\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 0.9766\n",
            "precision: 0.6588\n",
            "recall: 0.6667\n",
            "f1_score: 0.6627\n",
            "\n",
            "[0/100][150/109137][Time 18.56]\n",
            "Unified LR across all optimizers: 0.00019861406295796434\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0110\tGen: 0.2305\tRec: 0.2325\tE: 0.0086\tR: 0.0126\tP: 0.4418\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 0.9844\n",
            "precision: 0.6614\n",
            "recall: 0.6667\n",
            "f1_score: 0.6640\n",
            "\n",
            "[0/100][200/109137][Time 18.62]\n",
            "Unified LR across all optimizers: 0.00019815726328921765\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0093\tGen: 0.2013\tRec: 0.2038\tE: 0.0067\tR: 0.0117\tP: 0.3962\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 0.9453\n",
            "precision: 0.4074\n",
            "recall: 0.6667\n",
            "f1_score: 0.4545\n",
            "\n",
            "[0/100][250/109137][Time 18.65]\n",
            "Unified LR across all optimizers: 0.00019770151423055492\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0070\tGen: 0.2171\tRec: 0.2192\tE: 0.0049\tR: 0.0090\tP: 0.4365\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 0.9922\n",
            "precision: 0.9974\n",
            "recall: 0.8333\n",
            "f1_score: 0.8876\n",
            "\n",
            "[0/100][300/109137][Time 18.63]\n",
            "Unified LR across all optimizers: 0.00019724681336564005\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0021\tGen: 0.2266\tRec: 0.2264\tE: 0.0023\tR: 0.0019\tP: 0.4469\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 1.0000\n",
            "precision: 0.6667\n",
            "recall: 0.6667\n",
            "f1_score: 0.6667\n",
            "\n",
            "[0/100][350/109137][Time 18.69]\n",
            "Unified LR across all optimizers: 0.00019679315828369438\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0015\tGen: 0.1963\tRec: 0.1962\tE: 0.0017\tR: 0.0014\tP: 0.3874\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 1.0000\n",
            "precision: 1.0000\n",
            "recall: 1.0000\n",
            "f1_score: 1.0000\n",
            "\n",
            "[0/100][400/109137][Time 18.81]\n",
            "Unified LR across all optimizers: 0.00019634054657948372\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0017\tGen: 0.2045\tRec: 0.2043\tE: 0.0018\tR: 0.0017\tP: 0.3962\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 1.0000\n",
            "precision: 1.0000\n",
            "recall: 1.0000\n",
            "f1_score: 1.0000\n",
            "\n",
            "[0/100][450/109137][Time 18.86]\n",
            "Unified LR across all optimizers: 0.00019588897585330582\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0013\tGen: 0.1620\tRec: 0.1619\tE: 0.0013\tR: 0.0012\tP: 0.3049\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 1.0000\n",
            "precision: 1.0000\n",
            "recall: 1.0000\n",
            "f1_score: 1.0000\n",
            "\n",
            "[0/100][500/109137][Time 18.75]\n",
            "Unified LR across all optimizers: 0.00019543844371097777\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0015\tGen: 0.1689\tRec: 0.1689\tE: 0.0014\tR: 0.0015\tP: 0.3424\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 1.0000\n",
            "precision: 1.0000\n",
            "recall: 1.0000\n",
            "f1_score: 1.0000\n",
            "\n",
            "[0/100][550/109137][Time 18.81]\n",
            "Unified LR across all optimizers: 0.00019498894776382288\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0012\tGen: 0.2255\tRec: 0.2255\tE: 0.0012\tR: 0.0012\tP: 0.4759\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 1.0000\n",
            "precision: 1.0000\n",
            "recall: 1.0000\n",
            "f1_score: 1.0000\n",
            "\n",
            "[0/100][600/109137][Time 18.82]\n",
            "Unified LR across all optimizers: 0.00019454048562865856\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0014\tGen: 0.1798\tRec: 0.1800\tE: 0.0012\tR: 0.0016\tP: 0.3584\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 1.0000\n",
            "precision: 1.0000\n",
            "recall: 1.0000\n",
            "f1_score: 1.0000\n",
            "\n",
            "[0/100][650/109137][Time 18.79]\n",
            "Unified LR across all optimizers: 0.00019409305492778308\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0013\tGen: 0.1970\tRec: 0.1971\tE: 0.0012\tR: 0.0015\tP: 0.3843\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 0.9609\n",
            "precision: 0.3203\n",
            "recall: 0.3333\n",
            "f1_score: 0.3267\n",
            "\n",
            "[0/100][700/109137][Time 18.83]\n",
            "Unified LR across all optimizers: 0.00019364665328896346\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0011\tGen: 0.1486\tRec: 0.1486\tE: 0.0011\tR: 0.0012\tP: 0.2950\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 1.0000\n",
            "precision: 1.0000\n",
            "recall: 1.0000\n",
            "f1_score: 1.0000\n",
            "\n",
            "[0/100][750/109137][Time 18.81]\n",
            "Unified LR across all optimizers: 0.00019320127834542263\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0010\tGen: 0.1884\tRec: 0.1885\tE: 0.0009\tR: 0.0012\tP: 0.3855\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 1.0000\n",
            "precision: 1.0000\n",
            "recall: 1.0000\n",
            "f1_score: 1.0000\n",
            "\n",
            "[0/100][800/109137][Time 18.81]\n",
            "Unified LR across all optimizers: 0.00019275692773582703\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0016\tGen: 0.1844\tRec: 0.1848\tE: 0.0013\tR: 0.0019\tP: 0.3990\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 1.0000\n",
            "precision: 1.0000\n",
            "recall: 1.0000\n",
            "f1_score: 1.0000\n",
            "\n",
            "[0/100][850/109137][Time 18.82]\n",
            "Unified LR across all optimizers: 0.0001923135991042739\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0008\tGen: 0.1321\tRec: 0.1322\tE: 0.0007\tR: 0.0010\tP: 0.2686\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 1.0000\n",
            "precision: 1.0000\n",
            "recall: 1.0000\n",
            "f1_score: 1.0000\n",
            "\n",
            "[0/100][900/109137][Time 18.78]\n",
            "Unified LR across all optimizers: 0.0001918712901002789\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0006\tGen: 0.1525\tRec: 0.1526\tE: 0.0006\tR: 0.0007\tP: 0.2977\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 0.9062\n",
            "precision: 0.3021\n",
            "recall: 0.3333\n",
            "f1_score: 0.3169\n",
            "\n",
            "[0/100][950/109137][Time 18.83]\n",
            "Unified LR across all optimizers: 0.00019142999837876384\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0008\tGen: 0.1516\tRec: 0.1518\tE: 0.0006\tR: 0.0009\tP: 0.3040\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 1.0000\n",
            "precision: 1.0000\n",
            "recall: 1.0000\n",
            "f1_score: 1.0000\n",
            "\n",
            "[0/100][1000/109137][Time 18.80]\n",
            "Unified LR across all optimizers: 0.00019098972160004388\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0010\tGen: 0.1904\tRec: 0.1905\tE: 0.0009\tR: 0.0011\tP: 0.3717\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 1.0000\n",
            "precision: 1.0000\n",
            "recall: 1.0000\n",
            "f1_score: 1.0000\n",
            "\n",
            "[0/100][1050/109137][Time 18.70]\n",
            "Unified LR across all optimizers: 0.00019055045742981543\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0011\tGen: 0.1714\tRec: 0.1715\tE: 0.0010\tR: 0.0013\tP: 0.3503\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 1.0000\n",
            "precision: 1.0000\n",
            "recall: 1.0000\n",
            "f1_score: 1.0000\n",
            "\n",
            "[0/100][1100/109137][Time 18.78]\n",
            "Unified LR across all optimizers: 0.00019011220353914353\n",
            "--------------------Training Metrics--------------------\n",
            "Trainer:  gpt\n",
            "Inf: 0.0013\tGen: 0.1524\tRec: 0.1527\tE: 0.0009\tR: 0.0016\tP: 0.2902\n",
            "--------------------Test Metrics------------------------\n",
            "accuracy: 1.0000\n",
            "precision: 1.0000\n",
            "recall: 1.0000\n",
            "f1_score: 1.0000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainer_hub.train(trainset, testset)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "metadata": {
      "interpreter": {
        "hash": "a7e81af88087f1f4bdc1f0426df14b24fa2673362c5daa7f7f9146748f40b3b1"
      }
    },
    "vscode": {
      "interpreter": {
        "hash": "b287f80b48e4412a59791e63d64f0b079e04f47b5726df5f54fb3b5044d29a99"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
