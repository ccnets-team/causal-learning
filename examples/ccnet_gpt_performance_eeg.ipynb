{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author:\n",
    "        \n",
    "        PARK, JunHo, junho@ccnets.org\n",
    "\n",
    "        Kim, Jinsu \n",
    "\n",
    "        KIM, JeongYoong, jeongyoong@ccnets.org\n",
    "        \n",
    "    COPYRIGHT (c) 2024. CCNets. All Rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path_append = \"../\"\n",
    "sys.path.append(path_append)  # Go up one directory from where you are.\n",
    "\n",
    "from nn.utils.init import set_random_seed\n",
    "set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>...</th>\n",
       "      <th>D24</th>\n",
       "      <th>D25</th>\n",
       "      <th>D26</th>\n",
       "      <th>D27</th>\n",
       "      <th>D28</th>\n",
       "      <th>D29</th>\n",
       "      <th>D30</th>\n",
       "      <th>D31</th>\n",
       "      <th>D32</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3549.790315</td>\n",
       "      <td>4533.538497</td>\n",
       "      <td>3619.665186</td>\n",
       "      <td>3077.291188</td>\n",
       "      <td>-1380.325575</td>\n",
       "      <td>6120.066816</td>\n",
       "      <td>-4072.820600</td>\n",
       "      <td>-2256.511456</td>\n",
       "      <td>1820.012261</td>\n",
       "      <td>-2815.635423</td>\n",
       "      <td>...</td>\n",
       "      <td>-7240.845997</td>\n",
       "      <td>7034.252627</td>\n",
       "      <td>8458.062496</td>\n",
       "      <td>5905.223463</td>\n",
       "      <td>6147.660515</td>\n",
       "      <td>2458.073582</td>\n",
       "      <td>-7465.876831</td>\n",
       "      <td>-3604.133966</td>\n",
       "      <td>-5445.224315</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3551.227812</td>\n",
       "      <td>4534.850995</td>\n",
       "      <td>3622.540181</td>\n",
       "      <td>3077.322438</td>\n",
       "      <td>-1377.575581</td>\n",
       "      <td>6123.066810</td>\n",
       "      <td>-4069.851856</td>\n",
       "      <td>-2252.167714</td>\n",
       "      <td>1825.168502</td>\n",
       "      <td>-2803.072947</td>\n",
       "      <td>...</td>\n",
       "      <td>-7227.283522</td>\n",
       "      <td>7039.627617</td>\n",
       "      <td>8463.874985</td>\n",
       "      <td>5911.598451</td>\n",
       "      <td>6153.504254</td>\n",
       "      <td>2463.354822</td>\n",
       "      <td>-7461.033090</td>\n",
       "      <td>-3594.258985</td>\n",
       "      <td>-5435.693082</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3556.727802</td>\n",
       "      <td>4539.850986</td>\n",
       "      <td>3629.040169</td>\n",
       "      <td>3081.978679</td>\n",
       "      <td>-1370.419344</td>\n",
       "      <td>6130.348047</td>\n",
       "      <td>-4063.508118</td>\n",
       "      <td>-2249.292720</td>\n",
       "      <td>1828.074746</td>\n",
       "      <td>-2804.041695</td>\n",
       "      <td>...</td>\n",
       "      <td>-7227.158522</td>\n",
       "      <td>7048.502600</td>\n",
       "      <td>8473.562467</td>\n",
       "      <td>5921.348433</td>\n",
       "      <td>6163.004236</td>\n",
       "      <td>2469.854810</td>\n",
       "      <td>-7460.470591</td>\n",
       "      <td>-3591.540240</td>\n",
       "      <td>-5433.568086</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3557.915300</td>\n",
       "      <td>4541.225983</td>\n",
       "      <td>3628.540169</td>\n",
       "      <td>3083.197427</td>\n",
       "      <td>-1372.263090</td>\n",
       "      <td>6130.410547</td>\n",
       "      <td>-4062.070620</td>\n",
       "      <td>-2251.667715</td>\n",
       "      <td>1825.856000</td>\n",
       "      <td>-2803.572946</td>\n",
       "      <td>...</td>\n",
       "      <td>-7224.189777</td>\n",
       "      <td>7042.346362</td>\n",
       "      <td>8464.593734</td>\n",
       "      <td>5917.660940</td>\n",
       "      <td>6160.972990</td>\n",
       "      <td>2467.011066</td>\n",
       "      <td>-7458.158095</td>\n",
       "      <td>-3597.008980</td>\n",
       "      <td>-5437.474329</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3553.352808</td>\n",
       "      <td>4535.757243</td>\n",
       "      <td>3622.477681</td>\n",
       "      <td>3079.572434</td>\n",
       "      <td>-1377.763080</td>\n",
       "      <td>6125.598056</td>\n",
       "      <td>-4066.570612</td>\n",
       "      <td>-2255.136459</td>\n",
       "      <td>1821.981008</td>\n",
       "      <td>-2808.041687</td>\n",
       "      <td>...</td>\n",
       "      <td>-7219.971035</td>\n",
       "      <td>7044.658857</td>\n",
       "      <td>8466.843729</td>\n",
       "      <td>5914.848445</td>\n",
       "      <td>6156.785498</td>\n",
       "      <td>2466.948566</td>\n",
       "      <td>-7457.501846</td>\n",
       "      <td>-3585.821500</td>\n",
       "      <td>-5428.630595</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588018</th>\n",
       "      <td>-623.326974</td>\n",
       "      <td>2269.261431</td>\n",
       "      <td>2575.479615</td>\n",
       "      <td>285.733846</td>\n",
       "      <td>907.388947</td>\n",
       "      <td>-491.014719</td>\n",
       "      <td>-2998.447586</td>\n",
       "      <td>1886.043389</td>\n",
       "      <td>1659.637557</td>\n",
       "      <td>416.296105</td>\n",
       "      <td>...</td>\n",
       "      <td>-7176.689865</td>\n",
       "      <td>2116.667963</td>\n",
       "      <td>-901.138961</td>\n",
       "      <td>-227.327706</td>\n",
       "      <td>-657.170662</td>\n",
       "      <td>3025.322534</td>\n",
       "      <td>-12313.149124</td>\n",
       "      <td>-3810.071086</td>\n",
       "      <td>-5620.505241</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588019</th>\n",
       "      <td>-627.420717</td>\n",
       "      <td>2264.448940</td>\n",
       "      <td>2570.323375</td>\n",
       "      <td>281.077605</td>\n",
       "      <td>903.482705</td>\n",
       "      <td>-490.702219</td>\n",
       "      <td>-3001.260080</td>\n",
       "      <td>1884.387142</td>\n",
       "      <td>1657.012562</td>\n",
       "      <td>414.702358</td>\n",
       "      <td>...</td>\n",
       "      <td>-7179.502360</td>\n",
       "      <td>2118.074210</td>\n",
       "      <td>-900.607712</td>\n",
       "      <td>-227.046456</td>\n",
       "      <td>-659.389408</td>\n",
       "      <td>3027.760030</td>\n",
       "      <td>-12307.211635</td>\n",
       "      <td>-3809.946086</td>\n",
       "      <td>-5621.098990</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588020</th>\n",
       "      <td>-631.764459</td>\n",
       "      <td>2260.730197</td>\n",
       "      <td>2566.917131</td>\n",
       "      <td>275.546365</td>\n",
       "      <td>902.045207</td>\n",
       "      <td>-493.545964</td>\n",
       "      <td>-3006.103821</td>\n",
       "      <td>1886.199639</td>\n",
       "      <td>1658.512560</td>\n",
       "      <td>424.202340</td>\n",
       "      <td>...</td>\n",
       "      <td>-7177.439864</td>\n",
       "      <td>2118.199210</td>\n",
       "      <td>-900.920211</td>\n",
       "      <td>-226.140208</td>\n",
       "      <td>-659.764407</td>\n",
       "      <td>3027.103781</td>\n",
       "      <td>-12305.774138</td>\n",
       "      <td>-3805.633594</td>\n",
       "      <td>-5614.880251</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588021</th>\n",
       "      <td>-625.076971</td>\n",
       "      <td>2265.605188</td>\n",
       "      <td>2573.354619</td>\n",
       "      <td>281.702604</td>\n",
       "      <td>904.982702</td>\n",
       "      <td>-490.795969</td>\n",
       "      <td>-3001.416330</td>\n",
       "      <td>1888.387135</td>\n",
       "      <td>1659.418808</td>\n",
       "      <td>420.077348</td>\n",
       "      <td>...</td>\n",
       "      <td>-7172.002374</td>\n",
       "      <td>2119.730457</td>\n",
       "      <td>-898.170216</td>\n",
       "      <td>-224.515211</td>\n",
       "      <td>-656.576913</td>\n",
       "      <td>3032.822520</td>\n",
       "      <td>-12303.742892</td>\n",
       "      <td>-3804.133597</td>\n",
       "      <td>-5614.192752</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588022</th>\n",
       "      <td>-623.639474</td>\n",
       "      <td>2268.636432</td>\n",
       "      <td>2576.229614</td>\n",
       "      <td>286.515095</td>\n",
       "      <td>909.795193</td>\n",
       "      <td>-484.358481</td>\n",
       "      <td>-2996.353839</td>\n",
       "      <td>1895.730871</td>\n",
       "      <td>1668.950040</td>\n",
       "      <td>431.983576</td>\n",
       "      <td>...</td>\n",
       "      <td>-7171.908624</td>\n",
       "      <td>2129.230440</td>\n",
       "      <td>-889.357733</td>\n",
       "      <td>-216.577726</td>\n",
       "      <td>-649.358176</td>\n",
       "      <td>3039.572508</td>\n",
       "      <td>-12297.899153</td>\n",
       "      <td>-3793.383617</td>\n",
       "      <td>-5603.130273</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>588023 rows Ã— 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 A1           A2           A3           A4           A5  \\\n",
       "0       3549.790315  4533.538497  3619.665186  3077.291188 -1380.325575   \n",
       "1       3551.227812  4534.850995  3622.540181  3077.322438 -1377.575581   \n",
       "2       3556.727802  4539.850986  3629.040169  3081.978679 -1370.419344   \n",
       "3       3557.915300  4541.225983  3628.540169  3083.197427 -1372.263090   \n",
       "4       3553.352808  4535.757243  3622.477681  3079.572434 -1377.763080   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "588018  -623.326974  2269.261431  2575.479615   285.733846   907.388947   \n",
       "588019  -627.420717  2264.448940  2570.323375   281.077605   903.482705   \n",
       "588020  -631.764459  2260.730197  2566.917131   275.546365   902.045207   \n",
       "588021  -625.076971  2265.605188  2573.354619   281.702604   904.982702   \n",
       "588022  -623.639474  2268.636432  2576.229614   286.515095   909.795193   \n",
       "\n",
       "                 A6           A7           A8           A9          A10  ...  \\\n",
       "0       6120.066816 -4072.820600 -2256.511456  1820.012261 -2815.635423  ...   \n",
       "1       6123.066810 -4069.851856 -2252.167714  1825.168502 -2803.072947  ...   \n",
       "2       6130.348047 -4063.508118 -2249.292720  1828.074746 -2804.041695  ...   \n",
       "3       6130.410547 -4062.070620 -2251.667715  1825.856000 -2803.572946  ...   \n",
       "4       6125.598056 -4066.570612 -2255.136459  1821.981008 -2808.041687  ...   \n",
       "...             ...          ...          ...          ...          ...  ...   \n",
       "588018  -491.014719 -2998.447586  1886.043389  1659.637557   416.296105  ...   \n",
       "588019  -490.702219 -3001.260080  1884.387142  1657.012562   414.702358  ...   \n",
       "588020  -493.545964 -3006.103821  1886.199639  1658.512560   424.202340  ...   \n",
       "588021  -490.795969 -3001.416330  1888.387135  1659.418808   420.077348  ...   \n",
       "588022  -484.358481 -2996.353839  1895.730871  1668.950040   431.983576  ...   \n",
       "\n",
       "                D24          D25          D26          D27          D28  \\\n",
       "0      -7240.845997  7034.252627  8458.062496  5905.223463  6147.660515   \n",
       "1      -7227.283522  7039.627617  8463.874985  5911.598451  6153.504254   \n",
       "2      -7227.158522  7048.502600  8473.562467  5921.348433  6163.004236   \n",
       "3      -7224.189777  7042.346362  8464.593734  5917.660940  6160.972990   \n",
       "4      -7219.971035  7044.658857  8466.843729  5914.848445  6156.785498   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "588018 -7176.689865  2116.667963  -901.138961  -227.327706  -657.170662   \n",
       "588019 -7179.502360  2118.074210  -900.607712  -227.046456  -659.389408   \n",
       "588020 -7177.439864  2118.199210  -900.920211  -226.140208  -659.764407   \n",
       "588021 -7172.002374  2119.730457  -898.170216  -224.515211  -656.576913   \n",
       "588022 -7171.908624  2129.230440  -889.357733  -216.577726  -649.358176   \n",
       "\n",
       "                D29           D30          D31          D32  event  \n",
       "0       2458.073582  -7465.876831 -3604.133966 -5445.224315      5  \n",
       "1       2463.354822  -7461.033090 -3594.258985 -5435.693082      5  \n",
       "2       2469.854810  -7460.470591 -3591.540240 -5433.568086      5  \n",
       "3       2467.011066  -7458.158095 -3597.008980 -5437.474329      5  \n",
       "4       2466.948566  -7457.501846 -3585.821500 -5428.630595      5  \n",
       "...             ...           ...          ...          ...    ...  \n",
       "588018  3025.322534 -12313.149124 -3810.071086 -5620.505241     10  \n",
       "588019  3027.760030 -12307.211635 -3809.946086 -5621.098990     10  \n",
       "588020  3027.103781 -12305.774138 -3805.633594 -5614.880251     10  \n",
       "588021  3032.822520 -12303.742892 -3804.133597 -5614.192752     10  \n",
       "588022  3039.572508 -12297.899153 -3793.383617 -5603.130273     10  \n",
       "\n",
       "[588023 rows x 129 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# https://github.com/N-Nieto/Inner_Speech_Dataset\n",
    "\n",
    "# Load the Inner Speech Dataset\n",
    "# =============================\n",
    "# This dataset comprises raw EEG data collected from subject 'sub-01' during session 'ses-01'.\n",
    "# Source: https://github.com/N-Nieto/Inner_Speech_Dataset\n",
    "#\n",
    "# Overview:\n",
    "# - The dataset is part of a study on inner speech, capturing brain activity via EEG.\n",
    "# - Each row in the dataset corresponds to a timestamp of EEG readings.\n",
    "# - Columns represent various EEG channels (electrodes placed on the scalp).\n",
    "#\n",
    "# Usage:\n",
    "# - The data is primarily used for cognitive neuroscience research, focusing on the neural correlates of inner speech.\n",
    "# - Users can analyze EEG signals to investigate brain activity patterns associated with the cognitive processes of inner speech.\n",
    "#\n",
    "# File Structure:\n",
    "# - Located at '../data/RAW_EEG/sub-01/sub-01_ses-01.csv' relative to this script.\n",
    "# - It is advisable to preprocess the data (filtering, normalization) before detailed analysis.\n",
    "#\n",
    "# Example:\n",
    "# - To load this data into a DataFrame for analysis and processing, use the following code snippet.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = None\n",
    "for csv in [\"../data/RAW_EEG/sub-01/sub-01_ses-01.csv\", \"../data/RAW_EEG/sub-01/sub-01_ses-02.csv\", \"../data/RAW_EEG/sub-01/sub-01_ses-03.csv\"]:\n",
    "    tmp_df = pd.read_csv(path_append + csv)\n",
    "    if df is None:\n",
    "        df = tmp_df\n",
    "    else:\n",
    "        df = pd.concat([df, tmp_df])\n",
    "df = df.reset_index(drop=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of each class in the 'event' column:\n",
      "1     57650\n",
      "0     57650\n",
      "3     57650\n",
      "2     57650\n",
      "13    57650\n",
      "12    57650\n",
      "11    57650\n",
      "10    57650\n",
      "6     28825\n",
      "9     28825\n",
      "8     28825\n",
      "7     28825\n",
      "5     11523\n",
      "Name: event, dtype: int64\n",
      "\n",
      "Maximum class number:\n",
      "13\n",
      "\n",
      "Expected number of classes (from num_classes variable): 14\n",
      "\n",
      "DataFrame with reset class labels:\n",
      "                 A1           A2           A3           A4           A5  \\\n",
      "0       3549.790315  4533.538497  3619.665186  3077.291188 -1380.325575   \n",
      "1       3551.227812  4534.850995  3622.540181  3077.322438 -1377.575581   \n",
      "2       3556.727802  4539.850986  3629.040169  3081.978679 -1370.419344   \n",
      "3       3557.915300  4541.225983  3628.540169  3083.197427 -1372.263090   \n",
      "4       3553.352808  4535.757243  3622.477681  3079.572434 -1377.763080   \n",
      "...             ...          ...          ...          ...          ...   \n",
      "588018  -623.326974  2269.261431  2575.479615   285.733846   907.388947   \n",
      "588019  -627.420717  2264.448940  2570.323375   281.077605   903.482705   \n",
      "588020  -631.764459  2260.730197  2566.917131   275.546365   902.045207   \n",
      "588021  -625.076971  2265.605188  2573.354619   281.702604   904.982702   \n",
      "588022  -623.639474  2268.636432  2576.229614   286.515095   909.795193   \n",
      "\n",
      "                 A6           A7           A8           A9          A10  ...  \\\n",
      "0       6120.066816 -4072.820600 -2256.511456  1820.012261 -2815.635423  ...   \n",
      "1       6123.066810 -4069.851856 -2252.167714  1825.168502 -2803.072947  ...   \n",
      "2       6130.348047 -4063.508118 -2249.292720  1828.074746 -2804.041695  ...   \n",
      "3       6130.410547 -4062.070620 -2251.667715  1825.856000 -2803.572946  ...   \n",
      "4       6125.598056 -4066.570612 -2255.136459  1821.981008 -2808.041687  ...   \n",
      "...             ...          ...          ...          ...          ...  ...   \n",
      "588018  -491.014719 -2998.447586  1886.043389  1659.637557   416.296105  ...   \n",
      "588019  -490.702219 -3001.260080  1884.387142  1657.012562   414.702358  ...   \n",
      "588020  -493.545964 -3006.103821  1886.199639  1658.512560   424.202340  ...   \n",
      "588021  -490.795969 -3001.416330  1888.387135  1659.418808   420.077348  ...   \n",
      "588022  -484.358481 -2996.353839  1895.730871  1668.950040   431.983576  ...   \n",
      "\n",
      "                D24          D25          D26          D27          D28  \\\n",
      "0      -7240.845997  7034.252627  8458.062496  5905.223463  6147.660515   \n",
      "1      -7227.283522  7039.627617  8463.874985  5911.598451  6153.504254   \n",
      "2      -7227.158522  7048.502600  8473.562467  5921.348433  6163.004236   \n",
      "3      -7224.189777  7042.346362  8464.593734  5917.660940  6160.972990   \n",
      "4      -7219.971035  7044.658857  8466.843729  5914.848445  6156.785498   \n",
      "...             ...          ...          ...          ...          ...   \n",
      "588018 -7176.689865  2116.667963  -901.138961  -227.327706  -657.170662   \n",
      "588019 -7179.502360  2118.074210  -900.607712  -227.046456  -659.389408   \n",
      "588020 -7177.439864  2118.199210  -900.920211  -226.140208  -659.764407   \n",
      "588021 -7172.002374  2119.730457  -898.170216  -224.515211  -656.576913   \n",
      "588022 -7171.908624  2129.230440  -889.357733  -216.577726  -649.358176   \n",
      "\n",
      "                D29           D30          D31          D32  event  \n",
      "0       2458.073582  -7465.876831 -3604.133966 -5445.224315      4  \n",
      "1       2463.354822  -7461.033090 -3594.258985 -5435.693082      4  \n",
      "2       2469.854810  -7460.470591 -3591.540240 -5433.568086      4  \n",
      "3       2467.011066  -7458.158095 -3597.008980 -5437.474329      4  \n",
      "4       2466.948566  -7457.501846 -3585.821500 -5428.630595      4  \n",
      "...             ...           ...          ...          ...    ...  \n",
      "588018  3025.322534 -12313.149124 -3810.071086 -5620.505241      9  \n",
      "588019  3027.760030 -12307.211635 -3809.946086 -5621.098990      9  \n",
      "588020  3027.103781 -12305.774138 -3805.633594 -5614.880251      9  \n",
      "588021  3032.822520 -12303.742892 -3804.133597 -5614.192752      9  \n",
      "588022  3039.572508 -12297.899153 -3793.383617 -5603.130273      9  \n",
      "\n",
      "[588023 rows x 129 columns]\n",
      "\n",
      "New counts of each class in the 'event' column:\n",
      "1     57650\n",
      "0     57650\n",
      "3     57650\n",
      "2     57650\n",
      "12    57650\n",
      "11    57650\n",
      "10    57650\n",
      "9     57650\n",
      "5     28825\n",
      "8     28825\n",
      "7     28825\n",
      "6     28825\n",
      "4     11523\n",
      "Name: event, dtype: int64\n",
      "\n",
      "New maximum class number:\n",
      "12\n",
      "\n",
      "New expected number of classes (from num_classes variable): 13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example setup, assuming df is defined as a DataFrame\n",
    "# Assuming df['event'] contains the class labels\n",
    "\n",
    "# Print counts of each class in the 'event' column\n",
    "event_counts = df['event'].value_counts()\n",
    "print(\"Counts of each class in the 'event' column:\")\n",
    "print(event_counts)\n",
    "\n",
    "# Print the maximum class number\n",
    "max_class_number = df['event'].max()\n",
    "print(\"\\nMaximum class number:\")\n",
    "print(max_class_number)\n",
    "\n",
    "# Determine the number of classes\n",
    "print(\"\\nExpected number of classes (from num_classes variable):\", max_class_number + 1)\n",
    "\n",
    "# Reset the class labels to be in the range [0, num_classes-1]\n",
    "unique_classes = sorted(df['event'].unique())\n",
    "class_mapping = {old_class: new_class for new_class, old_class in enumerate(unique_classes)}\n",
    "\n",
    "df['event'] = df['event'].map(class_mapping)\n",
    "print(\"\\nDataFrame with reset class labels:\")\n",
    "print(df)\n",
    "\n",
    "# Verify the new counts and max class number\n",
    "new_event_counts = df['event'].value_counts()\n",
    "new_max_class_number = df['event'].max()\n",
    "num_classes = new_max_class_number + 1\n",
    "\n",
    "print(\"\\nNew counts of each class in the 'event' column:\")\n",
    "print(new_event_counts)\n",
    "print(\"\\nNew maximum class number:\")\n",
    "print(new_max_class_number)\n",
    "print(\"\\nNew expected number of classes (from num_classes variable):\", new_max_class_number + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices where the 'event' label changes: [0, 3841, 4994, 6147, 8453, 9606, 10759, 11912, 13065, 14218, 15371, 17677, 18830, 19983, 21136, 23442, 24595, 25748, 26901, 28054, 29207, 30360, 31513, 33819, 34972, 36125, 38431, 39584, 40737, 41890, 45349, 46502, 48808, 49961, 52267, 55726, 56879, 58032, 60338, 61491, 62644, 63797, 66103, 67256, 68409, 69562, 70715, 71868, 73021, 74174, 75327, 77633, 78786, 81092, 82245, 83398, 85704, 86857, 88010, 89163, 90316, 91469, 92622, 94928, 96081, 98387, 101846, 102999, 104152, 106458, 107611, 108764, 109917, 112223, 113376, 114529, 115682, 116835, 117988, 119141, 120294, 121447, 123753, 124906, 127212, 128365, 129518, 131824, 132977, 134130, 135283, 136436, 137589, 138742, 141048, 142201, 143354, 144507, 146813, 149119, 150272, 151425, 152578, 153731, 157190, 158343, 160649, 161802, 162955, 164108, 165261, 166414, 168720, 169873, 172179, 173332, 175638, 176791, 179097, 180250, 181403, 182556, 183709, 184862, 186015, 187168, 188321, 190627, 192933, 194086, 195239, 196392, 197545, 198698, 199851, 202157, 203310, 204463, 205616, 207922, 209075, 210228, 211381, 214840, 215993, 217146, 218299, 219452, 220605, 221758, 222911, 224064, 225217, 226370, 228676, 229829, 230982, 232135, 233288, 234441, 238282, 239435, 240588, 241741, 245200, 246353, 248659, 250965, 252118, 253271, 254424, 255577, 256730, 257883, 259036, 260189, 261342, 264801, 265954, 268260, 269413, 270566, 271719, 274025, 275178, 276331, 277484, 278637, 279790, 280943, 282096, 284402, 286708, 289014, 290167, 292473, 293626, 295932, 297085, 298238, 299391, 300544, 301697, 305156, 306309, 309768, 310921, 312074, 313227, 314380, 315533, 316686, 317839, 318992, 320145, 322451, 323604, 325910, 327063, 328216, 330522, 332828, 335134, 336287, 338593, 339746, 342052, 343205, 344358, 345511, 346664, 347817, 351276, 352429, 355888, 357041, 358194, 359347, 360500, 361653, 362806, 363959, 365112, 366265, 368571, 369724, 372030, 373183, 374336, 376642, 377795, 378948, 380101, 381254, 382407, 383560, 384713, 385866, 387019, 388172, 389325, 390478, 391631, 392784, 393937, 395090, 396243, 398549, 399702, 402008, 403161, 404314, 405467, 406620, 407773, 408926, 411232, 412385, 414691, 415844, 416997, 418150, 419303, 420456, 421609, 422762, 423915, 425068, 426221, 427374, 428527, 429680, 430833, 433139, 434292, 435445, 436598, 437751, 438904, 441210, 442363, 443516, 444669, 445822, 446975, 448128, 449281, 450434, 452740, 453893, 455046, 456199, 457352, 458505, 459658, 460811, 461964, 464270, 466576, 467729, 468882, 472723, 473876, 475029, 476182, 479641, 480794, 481947, 484253, 485406, 486559, 487712, 488865, 491171, 492324, 493477, 494630, 495783, 498089, 499242, 500395, 502701, 505007, 506160, 507313, 508466, 509619, 510772, 511925, 516537, 518843, 521149, 522302, 523455, 525761, 528067, 529220, 530373, 531526, 532679, 533832, 534985, 539597, 541903, 543056, 544209, 545362, 546515, 549974, 551127, 552280, 553433, 554586, 555739, 556892, 558045, 559198, 560351, 561504, 562657, 563810, 564963, 568422, 569575, 570728, 571881, 574187, 575340, 576493, 577646, 578799, 579952, 581105, 582258, 583411, 584564, 585717]\n",
      "Lengths between changes: [3841, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 2306, 1153, 1153, 1153, 3459, 1153, 2306, 1153, 2306, 3459, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 3459, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 3459, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 3459, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 3841, 1153, 1153, 1153, 3459, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 3459, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 2306, 2306, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 3459, 1153, 3459, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 2306, 2306, 2306, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 3459, 1153, 3459, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 2306, 1153, 1153, 3841, 1153, 1153, 1153, 3459, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 4612, 2306, 2306, 1153, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 4612, 2306, 1153, 1153, 1153, 1153, 3459, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 3459, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153]\n",
      "Minimum cycle length: 1153\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is defined and already includes an 'event' column\n",
    "# Assuming 'event' column contains class labels\n",
    "event_changes = df['event'].diff().ne(0)\n",
    "change_indices = event_changes[event_changes].index.tolist()\n",
    "\n",
    "# Calculate and print lengths between changes\n",
    "lengths_between_changes = [change_indices[i] - change_indices[i-1] for i in range(1, len(change_indices))]\n",
    "\n",
    "# Find the minimum cycle length where the label changes\n",
    "min_cycle_length = min(lengths_between_changes)\n",
    "\n",
    "print(\"Indices where the 'event' label changes:\", change_indices)\n",
    "print(\"Lengths between changes:\", lengths_between_changes)\n",
    "print(f\"Minimum cycle length: {min_cycle_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_pairs = []\n",
    "for start, end in zip(change_indices[:-1], change_indices[1:]):\n",
    "    segment_length = end - start\n",
    "    if segment_length >= min_cycle_length and segment_length % min_cycle_length == 0:\n",
    "        # Normalize each sub-segment within the main segment\n",
    "        for offset in range(0, segment_length, min_cycle_length):\n",
    "            sub_start = start + offset\n",
    "            sub_end = sub_start + min_cycle_length\n",
    "            segment_pairs.append((sub_start, sub_end))\n",
    "    else:\n",
    "        irregular_num = segment_length//min_cycle_length\n",
    "        # Normalize each sub-segment within the main segment\n",
    "        for i in range(irregular_num):\n",
    "            sub_start = start + i * min_cycle_length\n",
    "            if i == irregular_num - 1:\n",
    "                sub_end = end\n",
    "            else:\n",
    "                sub_end = sub_start + min_cycle_length\n",
    "            segment_pairs.append((sub_start, sub_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "seq_lengths = []#234440, 234440, 119140\n",
    "tmp = df.event.diff(1).dropna()\n",
    "seq_lengths = [0] + tmp[tmp!=0].index.to_list()\n",
    "seq_lengths = torch.tensor(seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_x shape: (585717, 128)\n",
      "df_y shape: (585717, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def process_dataframe(df, segment_pairs, use_scale = False, include_diff=False, window_size=1):\n",
    "    \"\"\"\n",
    "    Process the DataFrame by applying standard scaling and calculating differences, \n",
    "    returning the final DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - segment_pairs (list of tuples): List of (start, end) index pairs.\n",
    "    - window_size (int): The window size for difference calculation.\n",
    "    - include_diff (bool): Whether to include difference calculations in the final DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - final_df (pd.DataFrame): The processed DataFrame.\n",
    "    - new_segment_pairs (list of tuples): Updated segment pairs after processing.\n",
    "    \"\"\"\n",
    "    df_tensor = torch.tensor(df.values, dtype=torch.float64).cuda()\n",
    "    window_size = window_size if include_diff else 0\n",
    "    \n",
    "    df_list_diff = []\n",
    "    df_list_y = []\n",
    "    df_list_x = []\n",
    "    cur_idx = 0\n",
    "    new_segment_pairs = []\n",
    "\n",
    "    for i in range(len(segment_pairs)):\n",
    "        start, end = segment_pairs[i]\n",
    "\n",
    "        # Extract segments and apply scaling\n",
    "        x = df_tensor[start + window_size:end - window_size, :-1]\n",
    "        y = df_tensor[start + window_size:end - window_size, -1:]\n",
    "\n",
    "        x_np = x.cpu().numpy()\n",
    "        scaler = StandardScaler()\n",
    "        if use_scale:\n",
    "            x_scaled = torch.tensor(scaler.fit_transform(x_np), dtype=torch.float64).cuda()\n",
    "        else:\n",
    "            x_scaled = x\n",
    "\n",
    "        if include_diff:\n",
    "            diff = df_tensor[start + 2 * window_size:end, :-1] - df_tensor[start:end - 2 * window_size, :-1]\n",
    "            diff_np = diff.cpu().numpy()\n",
    "            diff_scaled = torch.tensor(scaler.fit_transform(diff_np), dtype=torch.float64).cuda()\n",
    "            df_list_diff.append(diff_scaled)\n",
    "\n",
    "        add_len = (end - start - 2 * window_size)\n",
    "        new_segment_pairs.append((cur_idx, cur_idx + add_len))\n",
    "        cur_idx += add_len\n",
    "\n",
    "        df_list_y.append(y)\n",
    "        df_list_x.append(x_scaled)\n",
    "\n",
    "    df_y_tensor = torch.cat(df_list_y, dim=0)\n",
    "    df_x_tensor = torch.cat(df_list_x, dim=0)\n",
    "\n",
    "    df_y = pd.DataFrame(df_y_tensor.cpu().numpy())\n",
    "    df_x = pd.DataFrame(df_x_tensor.cpu().numpy())\n",
    "\n",
    "    if include_diff:\n",
    "        df_diff_tensor = torch.cat(df_list_diff, dim=0)\n",
    "        df_diff = pd.DataFrame(df_diff_tensor.cpu().numpy())\n",
    "        new_column_names = ['diff_' + name for name in df.columns[:-1]]\n",
    "        df_diff.columns = new_column_names\n",
    "\n",
    "    print(\"df_x shape:\", df_x.shape)\n",
    "    if include_diff:\n",
    "        print(\"df_diff shape:\", df_diff.shape)\n",
    "    print(\"df_y shape:\", df_y.shape)\n",
    "\n",
    "    if include_diff:\n",
    "        final_df = pd.concat([df_x, df_diff, df_y], axis=1)\n",
    "        final_column_names = list(df.columns[:-1]) + new_column_names + [df.columns[-1]]\n",
    "    else:\n",
    "        final_df = pd.concat([df_x, df_y], axis=1)\n",
    "        final_column_names = list(df.columns[:-1]) + [df.columns[-1]]\n",
    "\n",
    "    final_df.columns = final_column_names\n",
    "\n",
    "    return final_df, new_segment_pairs\n",
    "\n",
    "# Example usage\n",
    "window_size = 1\n",
    "\n",
    "# Assuming df and segment_pairs are defined appropriately\n",
    "df, segment_pairs = process_dataframe(df, segment_pairs, use_scale=True, include_diff=False, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = df.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(         A1        A2        A3        A4        A5        A6        A7  \\\n",
       " 0 -2.489225 -2.277009 -2.514475 -2.188626 -0.665613 -1.346316 -0.058813   \n",
       " 1 -2.276914 -2.102834 -2.132174 -2.184048 -0.334188 -0.958881  0.323031   \n",
       " 2 -1.464595 -1.439312 -1.267841 -1.501895  0.528270 -0.018544  1.138970   \n",
       " 3 -1.289208 -1.256844 -1.334328 -1.323345  0.306064 -0.010473  1.323863   \n",
       " 4 -1.963064 -1.982571 -2.140485 -1.854417 -0.356786 -0.631983  0.745068   \n",
       " \n",
       "          A8        A9       A10  ...       D24       D25       D26       D27  \\\n",
       " 0  0.063280  0.040420 -1.023329  ... -1.910036 -1.004736 -0.560572 -1.682052   \n",
       " 1  0.508772  0.570520 -0.405698  ... -1.422319 -0.624488  0.064674 -0.879682   \n",
       " 2  0.803630  0.869303 -0.453326  ... -1.417824  0.003363  1.106749  0.347471   \n",
       " 3  0.560051  0.641200 -0.430280  ... -1.311066 -0.432153  0.141989 -0.116645   \n",
       " 4  0.204299  0.242821 -0.649985  ... -1.159357 -0.268558  0.384020 -0.470631   \n",
       " \n",
       "         D28       D29       D30       D31       D32  event  \n",
       " 0 -1.692538 -1.767055 -0.931829 -1.652175 -1.625125    4.0  \n",
       " 1 -0.972380 -1.348021 -0.658849 -1.327580 -1.307864    4.0  \n",
       " 2  0.198357 -0.832286 -0.627148 -1.238214 -1.237131    4.0  \n",
       " 3 -0.051965 -1.057920 -0.496822 -1.417974 -1.367156    4.0  \n",
       " 4 -0.568014 -1.062879 -0.459838 -1.050236 -1.072779    4.0  \n",
       " \n",
       " [5 rows x 129 columns],\n",
       "               A1        A2        A3        A4        A5        A6        A7  \\\n",
       " 585712 -0.361426 -0.454924 -0.238798  0.311232  2.634398 -1.002177  0.892134   \n",
       " 585713 -1.063053 -0.921154 -0.830256 -0.578986  1.302559 -1.486091  0.119497   \n",
       " 585714 -0.913315 -1.039911 -0.954773 -0.793027 -0.082360 -1.976852 -0.672050   \n",
       " 585715 -0.682292 -0.718828 -0.759103 -0.257924  0.877915 -1.741743 -0.450525   \n",
       " 585716  0.045005  0.011307 -0.140963  0.209076  1.727204 -1.310330 -0.015579   \n",
       " \n",
       "               A8        A9       A10  ...       D24       D25       D26  \\\n",
       " 585712  0.754096  0.723103 -1.437457  ...  0.877265 -1.560879 -1.522301   \n",
       " 585713  0.023264  0.057169 -1.611715  ...  0.505837 -2.293350 -1.956726   \n",
       " 585714 -0.809150 -0.933266 -2.425518  ...  0.308159 -3.025822 -2.425823   \n",
       " 585715 -0.168614 -0.199610 -1.863221  ...  0.412200 -2.457835 -2.181077   \n",
       " 585716  0.384447  0.359097 -1.410510  ...  0.747213 -2.010642 -1.771126   \n",
       " \n",
       "              D27       D28       D29       D30       D31       D32  event  \n",
       " 585712 -1.117451 -1.203517 -1.161360 -1.514173 -1.161083 -1.153077   10.0  \n",
       " 585713 -1.684809 -1.723125 -1.700381 -2.063590 -1.548324 -1.566593   10.0  \n",
       " 585714 -2.290185 -2.167556 -2.159874 -2.397467 -2.165284 -2.214269   10.0  \n",
       " 585715 -2.026978 -2.125545 -1.927918 -2.031189 -1.709127 -1.716057   10.0  \n",
       " 585716 -1.611696 -1.701014 -1.539116 -1.750845 -1.229999 -1.300880   10.0  \n",
       " \n",
       " [5 rows x 129 columns])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5], df[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class EEG_Dataset(Dataset):\n",
    "    def __init__(self, df, indices, max_window_size, num_classes, normalize = False):\n",
    "        self.df = df\n",
    "        self.indices = indices  # List of start indices\n",
    "        self.max_window_size = max_window_size\n",
    "        self.min_window_size = max_window_size // 2\n",
    "        self.num_classes = num_classes  # Added num_classes as an instance variable\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = self.indices[idx]\n",
    "        # Randomly choose a window size between min_window_size and max_window_size\n",
    "        window_size = random.randint(self.min_window_size, self.max_window_size)\n",
    "        \n",
    "        end_idx = min(start_idx + window_size, len(self.df))\n",
    "\n",
    "        # Retrieve the sequence using the calculated indices\n",
    "        seq = self.df.iloc[start_idx:end_idx]\n",
    "        X, y = seq.values[:, :-1], seq.values[:, -1]\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "        # Normalize X\n",
    "        if self.normalize:\n",
    "            X = (X - X.mean(dim = -1, keepdim = True)) / (X.std(dim = -1, keepdim = True) + 1e-8)\n",
    "        \n",
    "        y = torch.tensor(y, dtype=torch.long)  # Ensure y is a tensor of type long\n",
    "        y = torch.nn.functional.one_hot(y, num_classes=self.num_classes)  # Correct use with instance variable\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# Assume 'df' is your DataFrame and 'event' is the column containing labels\n",
    "\n",
    "# Generate indices without mixing segments\n",
    "def generate_indices(input_df, input_pairs, max_window_size, test_size=0.2):\n",
    "    length = len(input_pairs)\n",
    "    train_length = int(length * (1- test_size))\n",
    "    training_indices = []\n",
    "    testing_indices = []\n",
    "    for iter, (start, end) in enumerate(input_pairs):\n",
    "        indices = training_indices if iter < train_length else testing_indices\n",
    "        max_index = end - max_window_size  # Calculate the maximum starting index for this segment\n",
    "        for i in range(start, max_index):\n",
    "            # Check if all labels in the window are the same\n",
    "            if len(input_df['event'][i:i + max_window_size].unique()) == 1:\n",
    "                indices.append(i)\n",
    "            else:\n",
    "                print(f\"Skipping index {i} due to multiple labels in window.\")\n",
    "    return training_indices, testing_indices\n",
    "\n",
    "# Example usage\n",
    "max_window_size = 128\n",
    "shuffle(segment_pairs)  # Shuffle the indices to randomize the data order\n",
    "train_indices, test_indices = generate_indices(df, segment_pairs, max_window_size)\n",
    "\n",
    "shuffle(train_indices)  # Shuffle the indices to randomize the data order\n",
    "shuffle(test_indices)  # Shuffle the indices to randomize the data order\n",
    "\n",
    "# Assuming you have an EEG_Dataset class defined as before\n",
    "trainset = EEG_Dataset(df=df, indices=train_indices, max_window_size=max_window_size, num_classes = num_classes, normalize=False)\n",
    "testset = EEG_Dataset(df=df, indices=test_indices, max_window_size=max_window_size, num_classes = num_classes, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of overlapping windows: 0\n"
     ]
    }
   ],
   "source": [
    "def check_overlap(train_indices, test_indices, window_size, trials = None):\n",
    "    # Generate train and test window ranges\n",
    "    train_windows = [(idx, idx + window_size) for idx in train_indices]\n",
    "    test_windows = [(idx, idx + window_size) for idx in test_indices]\n",
    "    \n",
    "    # Sort train and test windows by their start indices\n",
    "    train_windows.sort()\n",
    "    test_windows.sort()\n",
    "\n",
    "    overlaps = []\n",
    "    train_idx = 0\n",
    "    trials = len(test_windows) if trials is None else trials\n",
    "\n",
    "    # Check for overlapping windows\n",
    "    for idx, (t_start, t_end) in enumerate(test_windows):\n",
    "        if idx > trials:\n",
    "            break\n",
    "        # Advance the train_idx to the relevant window range\n",
    "        while train_idx < len(train_windows) and train_windows[train_idx][1] <= t_start:\n",
    "            train_idx += 1\n",
    "\n",
    "        # Check if the current test window overlaps with any train window\n",
    "        for tr_start, tr_end in train_windows[train_idx:]:\n",
    "            if tr_start < t_end and tr_end > t_start:  # Overlapping condition\n",
    "                overlaps.append((tr_start, tr_end, t_start, t_end))\n",
    "                print (f\"Overlap found between train window {tr_start}-{tr_end} and test window {t_start}-{t_end}\")\n",
    "                break  # No need to check further once an overlap is found\n",
    "\n",
    "    return overlaps\n",
    "\n",
    "# Check for overlaps\n",
    "overlaps = check_overlap(train_indices, test_indices, max_window_size, trials = 100)\n",
    "print(\"Number of overlapping windows:\", len(overlaps))\n",
    "if overlaps:\n",
    "    print(\"Example of overlapping windows:\", overlaps[:5])  # Print the first 5 overlapping windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.setting.data_config import DataConfig\n",
    "from tools.setting.ml_params import MLParameters\n",
    "from trainer_hub import TrainerHub\n",
    "import torch\n",
    "\n",
    "data_config = DataConfig(dataset_name = 'eeg-sub-01', task_type='multi_class_classification', obs_shape=[num_features], label_size=num_classes)\n",
    "\n",
    "#  Set training configuration from the AlgorithmConfig class, returning them as a Namespace object.\n",
    "ml_params = MLParameters(core_model = 'gpt', encoder_model = 'none')\n",
    "\n",
    "# Set the device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Initialize the TrainerHub class with the training configuration, data configuration, device, and use_print and use_wandb flags\n",
    "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d134e0681340189075a03c2bf1d33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac22e6baaff64045b90e7a0714660ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/6504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100][50/6504][Time 19.53]\n",
      "Unified LR across all optimizers: 0.0001995308238189185\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.3163\tGen: 0.6635\tRec: 0.6063\tE: 0.3732\tR: 0.2592\tP: 0.9532\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.4594\n",
      "precision: 0.0656\n",
      "recall: 0.1400\n",
      "f1_score: 0.0786\n",
      "\n",
      "[0/100][100/6504][Time 18.39]\n",
      "Unified LR across all optimizers: 0.00019907191565870155\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1598\tGen: 0.4757\tRec: 0.4501\tE: 0.1858\tR: 0.1348\tP: 0.7675\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.5156\n",
      "precision: 0.0773\n",
      "recall: 0.1468\n",
      "f1_score: 0.0882\n",
      "\n",
      "[0/100][150/6504][Time 18.18]\n",
      "Unified LR across all optimizers: 0.00019861406295796434\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1320\tGen: 0.4314\tRec: 0.4141\tE: 0.1495\tR: 0.1153\tP: 0.7132\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.4453\n",
      "precision: 0.0858\n",
      "recall: 0.1457\n",
      "f1_score: 0.0906\n",
      "\n",
      "[0/100][200/6504][Time 18.55]\n",
      "Unified LR across all optimizers: 0.00019815726328921765\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1242\tGen: 0.4095\tRec: 0.3936\tE: 0.1405\tR: 0.1085\tP: 0.6793\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.5109\n",
      "precision: 0.1007\n",
      "recall: 0.1657\n",
      "f1_score: 0.1110\n",
      "\n",
      "[0/100][250/6504][Time 18.76]\n",
      "Unified LR across all optimizers: 0.00019770151423055492\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1190\tGen: 0.3965\tRec: 0.3821\tE: 0.1335\tR: 0.1047\tP: 0.6604\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.5203\n",
      "precision: 0.2296\n",
      "recall: 0.1992\n",
      "f1_score: 0.1382\n",
      "\n",
      "[0/100][300/6504][Time 18.77]\n",
      "Unified LR across all optimizers: 0.00019724681336564005\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1144\tGen: 0.3820\tRec: 0.3684\tE: 0.1286\tR: 0.1014\tP: 0.6346\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.5375\n",
      "precision: 0.1754\n",
      "recall: 0.1923\n",
      "f1_score: 0.1197\n",
      "\n",
      "[0/100][350/6504][Time 18.78]\n",
      "Unified LR across all optimizers: 0.00019679315828369438\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1083\tGen: 0.3687\tRec: 0.3566\tE: 0.1199\tR: 0.0957\tP: 0.6178\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.4969\n",
      "precision: 0.2500\n",
      "recall: 0.2021\n",
      "f1_score: 0.1468\n",
      "\n",
      "[0/100][400/6504][Time 18.74]\n",
      "Unified LR across all optimizers: 0.00019634054657948372\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1015\tGen: 0.3598\tRec: 0.3486\tE: 0.1120\tR: 0.0897\tP: 0.6091\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.5312\n",
      "precision: 0.1239\n",
      "recall: 0.2063\n",
      "f1_score: 0.1392\n",
      "\n",
      "[0/100][450/6504][Time 18.66]\n",
      "Unified LR across all optimizers: 0.00019588897585330582\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0971\tGen: 0.3492\tRec: 0.3383\tE: 0.1069\tR: 0.0851\tP: 0.5927\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.5344\n",
      "precision: 0.1434\n",
      "recall: 0.2284\n",
      "f1_score: 0.1615\n",
      "\n",
      "[0/100][500/6504][Time 18.78]\n",
      "Unified LR across all optimizers: 0.00019543844371097777\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0909\tGen: 0.3430\tRec: 0.3337\tE: 0.1001\tR: 0.0811\tP: 0.5884\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.5578\n",
      "precision: 0.1741\n",
      "recall: 0.2157\n",
      "f1_score: 0.1822\n",
      "\n",
      "[0/100][550/6504][Time 18.72]\n",
      "Unified LR across all optimizers: 0.00019498894776382288\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0813\tGen: 0.3370\tRec: 0.3285\tE: 0.0889\tR: 0.0717\tP: 0.5870\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.5656\n",
      "precision: 0.1678\n",
      "recall: 0.2372\n",
      "f1_score: 0.1837\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer_hub.train(trainset, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_hub.test(testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
