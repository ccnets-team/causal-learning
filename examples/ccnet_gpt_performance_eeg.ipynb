{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author:\n",
    "        \n",
    "        PARK, JunHo, junho@ccnets.org\n",
    "\n",
    "        Kim, Jinsu \n",
    "\n",
    "        KIM, JeongYoong, jeongyoong@ccnets.org\n",
    "        \n",
    "    COPYRIGHT (c) 2024. CCNets. All Rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path_append = \"../\"\n",
    "sys.path.append(path_append)  # Go up one directory from where you are.\n",
    "\n",
    "from nn.utils.init import set_random_seed\n",
    "set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>...</th>\n",
       "      <th>D24</th>\n",
       "      <th>D25</th>\n",
       "      <th>D26</th>\n",
       "      <th>D27</th>\n",
       "      <th>D28</th>\n",
       "      <th>D29</th>\n",
       "      <th>D30</th>\n",
       "      <th>D31</th>\n",
       "      <th>D32</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3549.790315</td>\n",
       "      <td>4533.538497</td>\n",
       "      <td>3619.665186</td>\n",
       "      <td>3077.291188</td>\n",
       "      <td>-1380.325575</td>\n",
       "      <td>6120.066816</td>\n",
       "      <td>-4072.820600</td>\n",
       "      <td>-2256.511456</td>\n",
       "      <td>1820.012261</td>\n",
       "      <td>-2815.635423</td>\n",
       "      <td>...</td>\n",
       "      <td>-7240.845997</td>\n",
       "      <td>7034.252627</td>\n",
       "      <td>8458.062496</td>\n",
       "      <td>5905.223463</td>\n",
       "      <td>6147.660515</td>\n",
       "      <td>2458.073582</td>\n",
       "      <td>-7465.876831</td>\n",
       "      <td>-3604.133966</td>\n",
       "      <td>-5445.224315</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3551.227812</td>\n",
       "      <td>4534.850995</td>\n",
       "      <td>3622.540181</td>\n",
       "      <td>3077.322438</td>\n",
       "      <td>-1377.575581</td>\n",
       "      <td>6123.066810</td>\n",
       "      <td>-4069.851856</td>\n",
       "      <td>-2252.167714</td>\n",
       "      <td>1825.168502</td>\n",
       "      <td>-2803.072947</td>\n",
       "      <td>...</td>\n",
       "      <td>-7227.283522</td>\n",
       "      <td>7039.627617</td>\n",
       "      <td>8463.874985</td>\n",
       "      <td>5911.598451</td>\n",
       "      <td>6153.504254</td>\n",
       "      <td>2463.354822</td>\n",
       "      <td>-7461.033090</td>\n",
       "      <td>-3594.258985</td>\n",
       "      <td>-5435.693082</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3556.727802</td>\n",
       "      <td>4539.850986</td>\n",
       "      <td>3629.040169</td>\n",
       "      <td>3081.978679</td>\n",
       "      <td>-1370.419344</td>\n",
       "      <td>6130.348047</td>\n",
       "      <td>-4063.508118</td>\n",
       "      <td>-2249.292720</td>\n",
       "      <td>1828.074746</td>\n",
       "      <td>-2804.041695</td>\n",
       "      <td>...</td>\n",
       "      <td>-7227.158522</td>\n",
       "      <td>7048.502600</td>\n",
       "      <td>8473.562467</td>\n",
       "      <td>5921.348433</td>\n",
       "      <td>6163.004236</td>\n",
       "      <td>2469.854810</td>\n",
       "      <td>-7460.470591</td>\n",
       "      <td>-3591.540240</td>\n",
       "      <td>-5433.568086</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3557.915300</td>\n",
       "      <td>4541.225983</td>\n",
       "      <td>3628.540169</td>\n",
       "      <td>3083.197427</td>\n",
       "      <td>-1372.263090</td>\n",
       "      <td>6130.410547</td>\n",
       "      <td>-4062.070620</td>\n",
       "      <td>-2251.667715</td>\n",
       "      <td>1825.856000</td>\n",
       "      <td>-2803.572946</td>\n",
       "      <td>...</td>\n",
       "      <td>-7224.189777</td>\n",
       "      <td>7042.346362</td>\n",
       "      <td>8464.593734</td>\n",
       "      <td>5917.660940</td>\n",
       "      <td>6160.972990</td>\n",
       "      <td>2467.011066</td>\n",
       "      <td>-7458.158095</td>\n",
       "      <td>-3597.008980</td>\n",
       "      <td>-5437.474329</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3553.352808</td>\n",
       "      <td>4535.757243</td>\n",
       "      <td>3622.477681</td>\n",
       "      <td>3079.572434</td>\n",
       "      <td>-1377.763080</td>\n",
       "      <td>6125.598056</td>\n",
       "      <td>-4066.570612</td>\n",
       "      <td>-2255.136459</td>\n",
       "      <td>1821.981008</td>\n",
       "      <td>-2808.041687</td>\n",
       "      <td>...</td>\n",
       "      <td>-7219.971035</td>\n",
       "      <td>7044.658857</td>\n",
       "      <td>8466.843729</td>\n",
       "      <td>5914.848445</td>\n",
       "      <td>6156.785498</td>\n",
       "      <td>2466.948566</td>\n",
       "      <td>-7457.501846</td>\n",
       "      <td>-3585.821500</td>\n",
       "      <td>-5428.630595</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588018</th>\n",
       "      <td>-623.326974</td>\n",
       "      <td>2269.261431</td>\n",
       "      <td>2575.479615</td>\n",
       "      <td>285.733846</td>\n",
       "      <td>907.388947</td>\n",
       "      <td>-491.014719</td>\n",
       "      <td>-2998.447586</td>\n",
       "      <td>1886.043389</td>\n",
       "      <td>1659.637557</td>\n",
       "      <td>416.296105</td>\n",
       "      <td>...</td>\n",
       "      <td>-7176.689865</td>\n",
       "      <td>2116.667963</td>\n",
       "      <td>-901.138961</td>\n",
       "      <td>-227.327706</td>\n",
       "      <td>-657.170662</td>\n",
       "      <td>3025.322534</td>\n",
       "      <td>-12313.149124</td>\n",
       "      <td>-3810.071086</td>\n",
       "      <td>-5620.505241</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588019</th>\n",
       "      <td>-627.420717</td>\n",
       "      <td>2264.448940</td>\n",
       "      <td>2570.323375</td>\n",
       "      <td>281.077605</td>\n",
       "      <td>903.482705</td>\n",
       "      <td>-490.702219</td>\n",
       "      <td>-3001.260080</td>\n",
       "      <td>1884.387142</td>\n",
       "      <td>1657.012562</td>\n",
       "      <td>414.702358</td>\n",
       "      <td>...</td>\n",
       "      <td>-7179.502360</td>\n",
       "      <td>2118.074210</td>\n",
       "      <td>-900.607712</td>\n",
       "      <td>-227.046456</td>\n",
       "      <td>-659.389408</td>\n",
       "      <td>3027.760030</td>\n",
       "      <td>-12307.211635</td>\n",
       "      <td>-3809.946086</td>\n",
       "      <td>-5621.098990</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588020</th>\n",
       "      <td>-631.764459</td>\n",
       "      <td>2260.730197</td>\n",
       "      <td>2566.917131</td>\n",
       "      <td>275.546365</td>\n",
       "      <td>902.045207</td>\n",
       "      <td>-493.545964</td>\n",
       "      <td>-3006.103821</td>\n",
       "      <td>1886.199639</td>\n",
       "      <td>1658.512560</td>\n",
       "      <td>424.202340</td>\n",
       "      <td>...</td>\n",
       "      <td>-7177.439864</td>\n",
       "      <td>2118.199210</td>\n",
       "      <td>-900.920211</td>\n",
       "      <td>-226.140208</td>\n",
       "      <td>-659.764407</td>\n",
       "      <td>3027.103781</td>\n",
       "      <td>-12305.774138</td>\n",
       "      <td>-3805.633594</td>\n",
       "      <td>-5614.880251</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588021</th>\n",
       "      <td>-625.076971</td>\n",
       "      <td>2265.605188</td>\n",
       "      <td>2573.354619</td>\n",
       "      <td>281.702604</td>\n",
       "      <td>904.982702</td>\n",
       "      <td>-490.795969</td>\n",
       "      <td>-3001.416330</td>\n",
       "      <td>1888.387135</td>\n",
       "      <td>1659.418808</td>\n",
       "      <td>420.077348</td>\n",
       "      <td>...</td>\n",
       "      <td>-7172.002374</td>\n",
       "      <td>2119.730457</td>\n",
       "      <td>-898.170216</td>\n",
       "      <td>-224.515211</td>\n",
       "      <td>-656.576913</td>\n",
       "      <td>3032.822520</td>\n",
       "      <td>-12303.742892</td>\n",
       "      <td>-3804.133597</td>\n",
       "      <td>-5614.192752</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588022</th>\n",
       "      <td>-623.639474</td>\n",
       "      <td>2268.636432</td>\n",
       "      <td>2576.229614</td>\n",
       "      <td>286.515095</td>\n",
       "      <td>909.795193</td>\n",
       "      <td>-484.358481</td>\n",
       "      <td>-2996.353839</td>\n",
       "      <td>1895.730871</td>\n",
       "      <td>1668.950040</td>\n",
       "      <td>431.983576</td>\n",
       "      <td>...</td>\n",
       "      <td>-7171.908624</td>\n",
       "      <td>2129.230440</td>\n",
       "      <td>-889.357733</td>\n",
       "      <td>-216.577726</td>\n",
       "      <td>-649.358176</td>\n",
       "      <td>3039.572508</td>\n",
       "      <td>-12297.899153</td>\n",
       "      <td>-3793.383617</td>\n",
       "      <td>-5603.130273</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>588023 rows Ã— 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 A1           A2           A3           A4           A5  \\\n",
       "0       3549.790315  4533.538497  3619.665186  3077.291188 -1380.325575   \n",
       "1       3551.227812  4534.850995  3622.540181  3077.322438 -1377.575581   \n",
       "2       3556.727802  4539.850986  3629.040169  3081.978679 -1370.419344   \n",
       "3       3557.915300  4541.225983  3628.540169  3083.197427 -1372.263090   \n",
       "4       3553.352808  4535.757243  3622.477681  3079.572434 -1377.763080   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "588018  -623.326974  2269.261431  2575.479615   285.733846   907.388947   \n",
       "588019  -627.420717  2264.448940  2570.323375   281.077605   903.482705   \n",
       "588020  -631.764459  2260.730197  2566.917131   275.546365   902.045207   \n",
       "588021  -625.076971  2265.605188  2573.354619   281.702604   904.982702   \n",
       "588022  -623.639474  2268.636432  2576.229614   286.515095   909.795193   \n",
       "\n",
       "                 A6           A7           A8           A9          A10  ...  \\\n",
       "0       6120.066816 -4072.820600 -2256.511456  1820.012261 -2815.635423  ...   \n",
       "1       6123.066810 -4069.851856 -2252.167714  1825.168502 -2803.072947  ...   \n",
       "2       6130.348047 -4063.508118 -2249.292720  1828.074746 -2804.041695  ...   \n",
       "3       6130.410547 -4062.070620 -2251.667715  1825.856000 -2803.572946  ...   \n",
       "4       6125.598056 -4066.570612 -2255.136459  1821.981008 -2808.041687  ...   \n",
       "...             ...          ...          ...          ...          ...  ...   \n",
       "588018  -491.014719 -2998.447586  1886.043389  1659.637557   416.296105  ...   \n",
       "588019  -490.702219 -3001.260080  1884.387142  1657.012562   414.702358  ...   \n",
       "588020  -493.545964 -3006.103821  1886.199639  1658.512560   424.202340  ...   \n",
       "588021  -490.795969 -3001.416330  1888.387135  1659.418808   420.077348  ...   \n",
       "588022  -484.358481 -2996.353839  1895.730871  1668.950040   431.983576  ...   \n",
       "\n",
       "                D24          D25          D26          D27          D28  \\\n",
       "0      -7240.845997  7034.252627  8458.062496  5905.223463  6147.660515   \n",
       "1      -7227.283522  7039.627617  8463.874985  5911.598451  6153.504254   \n",
       "2      -7227.158522  7048.502600  8473.562467  5921.348433  6163.004236   \n",
       "3      -7224.189777  7042.346362  8464.593734  5917.660940  6160.972990   \n",
       "4      -7219.971035  7044.658857  8466.843729  5914.848445  6156.785498   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "588018 -7176.689865  2116.667963  -901.138961  -227.327706  -657.170662   \n",
       "588019 -7179.502360  2118.074210  -900.607712  -227.046456  -659.389408   \n",
       "588020 -7177.439864  2118.199210  -900.920211  -226.140208  -659.764407   \n",
       "588021 -7172.002374  2119.730457  -898.170216  -224.515211  -656.576913   \n",
       "588022 -7171.908624  2129.230440  -889.357733  -216.577726  -649.358176   \n",
       "\n",
       "                D29           D30          D31          D32  event  \n",
       "0       2458.073582  -7465.876831 -3604.133966 -5445.224315      5  \n",
       "1       2463.354822  -7461.033090 -3594.258985 -5435.693082      5  \n",
       "2       2469.854810  -7460.470591 -3591.540240 -5433.568086      5  \n",
       "3       2467.011066  -7458.158095 -3597.008980 -5437.474329      5  \n",
       "4       2466.948566  -7457.501846 -3585.821500 -5428.630595      5  \n",
       "...             ...           ...          ...          ...    ...  \n",
       "588018  3025.322534 -12313.149124 -3810.071086 -5620.505241     10  \n",
       "588019  3027.760030 -12307.211635 -3809.946086 -5621.098990     10  \n",
       "588020  3027.103781 -12305.774138 -3805.633594 -5614.880251     10  \n",
       "588021  3032.822520 -12303.742892 -3804.133597 -5614.192752     10  \n",
       "588022  3039.572508 -12297.899153 -3793.383617 -5603.130273     10  \n",
       "\n",
       "[588023 rows x 129 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# https://github.com/N-Nieto/Inner_Speech_Dataset\n",
    "\n",
    "# Load the Inner Speech Dataset\n",
    "# =============================\n",
    "# This dataset comprises raw EEG data collected from subject 'sub-01' during session 'ses-01'.\n",
    "# Source: https://github.com/N-Nieto/Inner_Speech_Dataset\n",
    "#\n",
    "# Overview:\n",
    "# - The dataset is part of a study on inner speech, capturing brain activity via EEG.\n",
    "# - Each row in the dataset corresponds to a timestamp of EEG readings.\n",
    "# - Columns represent various EEG channels (electrodes placed on the scalp).\n",
    "#\n",
    "# Usage:\n",
    "# - The data is primarily used for cognitive neuroscience research, focusing on the neural correlates of inner speech.\n",
    "# - Users can analyze EEG signals to investigate brain activity patterns associated with the cognitive processes of inner speech.\n",
    "#\n",
    "# File Structure:\n",
    "# - Located at '../data/RAW_EEG/sub-01/sub-01_ses-01.csv' relative to this script.\n",
    "# - It is advisable to preprocess the data (filtering, normalization) before detailed analysis.\n",
    "#\n",
    "# Example:\n",
    "# - To load this data into a DataFrame for analysis and processing, use the following code snippet.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = None\n",
    "for csv in [\"../data/RAW_EEG/sub-01/sub-01_ses-01.csv\", \"../data/RAW_EEG/sub-01/sub-01_ses-02.csv\", \"../data/RAW_EEG/sub-01/sub-01_ses-03.csv\"]:\n",
    "    tmp_df = pd.read_csv(path_append + csv)\n",
    "    if df is None:\n",
    "        df = tmp_df\n",
    "    else:\n",
    "        df = pd.concat([df, tmp_df])\n",
    "df = df.reset_index(drop=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of each class in the 'event' column:\n",
      "1     57650\n",
      "0     57650\n",
      "3     57650\n",
      "2     57650\n",
      "13    57650\n",
      "12    57650\n",
      "11    57650\n",
      "10    57650\n",
      "6     28825\n",
      "9     28825\n",
      "8     28825\n",
      "7     28825\n",
      "5     11523\n",
      "Name: event, dtype: int64\n",
      "\n",
      "Maximum class number:\n",
      "13\n",
      "\n",
      "Expected number of classes (from num_classes variable): 14\n",
      "\n",
      "DataFrame with reset class labels:\n",
      "                 A1           A2           A3           A4           A5  \\\n",
      "0       3549.790315  4533.538497  3619.665186  3077.291188 -1380.325575   \n",
      "1       3551.227812  4534.850995  3622.540181  3077.322438 -1377.575581   \n",
      "2       3556.727802  4539.850986  3629.040169  3081.978679 -1370.419344   \n",
      "3       3557.915300  4541.225983  3628.540169  3083.197427 -1372.263090   \n",
      "4       3553.352808  4535.757243  3622.477681  3079.572434 -1377.763080   \n",
      "...             ...          ...          ...          ...          ...   \n",
      "588018  -623.326974  2269.261431  2575.479615   285.733846   907.388947   \n",
      "588019  -627.420717  2264.448940  2570.323375   281.077605   903.482705   \n",
      "588020  -631.764459  2260.730197  2566.917131   275.546365   902.045207   \n",
      "588021  -625.076971  2265.605188  2573.354619   281.702604   904.982702   \n",
      "588022  -623.639474  2268.636432  2576.229614   286.515095   909.795193   \n",
      "\n",
      "                 A6           A7           A8           A9          A10  ...  \\\n",
      "0       6120.066816 -4072.820600 -2256.511456  1820.012261 -2815.635423  ...   \n",
      "1       6123.066810 -4069.851856 -2252.167714  1825.168502 -2803.072947  ...   \n",
      "2       6130.348047 -4063.508118 -2249.292720  1828.074746 -2804.041695  ...   \n",
      "3       6130.410547 -4062.070620 -2251.667715  1825.856000 -2803.572946  ...   \n",
      "4       6125.598056 -4066.570612 -2255.136459  1821.981008 -2808.041687  ...   \n",
      "...             ...          ...          ...          ...          ...  ...   \n",
      "588018  -491.014719 -2998.447586  1886.043389  1659.637557   416.296105  ...   \n",
      "588019  -490.702219 -3001.260080  1884.387142  1657.012562   414.702358  ...   \n",
      "588020  -493.545964 -3006.103821  1886.199639  1658.512560   424.202340  ...   \n",
      "588021  -490.795969 -3001.416330  1888.387135  1659.418808   420.077348  ...   \n",
      "588022  -484.358481 -2996.353839  1895.730871  1668.950040   431.983576  ...   \n",
      "\n",
      "                D24          D25          D26          D27          D28  \\\n",
      "0      -7240.845997  7034.252627  8458.062496  5905.223463  6147.660515   \n",
      "1      -7227.283522  7039.627617  8463.874985  5911.598451  6153.504254   \n",
      "2      -7227.158522  7048.502600  8473.562467  5921.348433  6163.004236   \n",
      "3      -7224.189777  7042.346362  8464.593734  5917.660940  6160.972990   \n",
      "4      -7219.971035  7044.658857  8466.843729  5914.848445  6156.785498   \n",
      "...             ...          ...          ...          ...          ...   \n",
      "588018 -7176.689865  2116.667963  -901.138961  -227.327706  -657.170662   \n",
      "588019 -7179.502360  2118.074210  -900.607712  -227.046456  -659.389408   \n",
      "588020 -7177.439864  2118.199210  -900.920211  -226.140208  -659.764407   \n",
      "588021 -7172.002374  2119.730457  -898.170216  -224.515211  -656.576913   \n",
      "588022 -7171.908624  2129.230440  -889.357733  -216.577726  -649.358176   \n",
      "\n",
      "                D29           D30          D31          D32  event  \n",
      "0       2458.073582  -7465.876831 -3604.133966 -5445.224315      4  \n",
      "1       2463.354822  -7461.033090 -3594.258985 -5435.693082      4  \n",
      "2       2469.854810  -7460.470591 -3591.540240 -5433.568086      4  \n",
      "3       2467.011066  -7458.158095 -3597.008980 -5437.474329      4  \n",
      "4       2466.948566  -7457.501846 -3585.821500 -5428.630595      4  \n",
      "...             ...           ...          ...          ...    ...  \n",
      "588018  3025.322534 -12313.149124 -3810.071086 -5620.505241      9  \n",
      "588019  3027.760030 -12307.211635 -3809.946086 -5621.098990      9  \n",
      "588020  3027.103781 -12305.774138 -3805.633594 -5614.880251      9  \n",
      "588021  3032.822520 -12303.742892 -3804.133597 -5614.192752      9  \n",
      "588022  3039.572508 -12297.899153 -3793.383617 -5603.130273      9  \n",
      "\n",
      "[588023 rows x 129 columns]\n",
      "\n",
      "New counts of each class in the 'event' column:\n",
      "1     57650\n",
      "0     57650\n",
      "3     57650\n",
      "2     57650\n",
      "12    57650\n",
      "11    57650\n",
      "10    57650\n",
      "9     57650\n",
      "5     28825\n",
      "8     28825\n",
      "7     28825\n",
      "6     28825\n",
      "4     11523\n",
      "Name: event, dtype: int64\n",
      "\n",
      "New maximum class number:\n",
      "12\n",
      "\n",
      "New expected number of classes (from num_classes variable): 13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example setup, assuming df is defined as a DataFrame\n",
    "# Assuming df['event'] contains the class labels\n",
    "\n",
    "# Print counts of each class in the 'event' column\n",
    "event_counts = df['event'].value_counts()\n",
    "print(\"Counts of each class in the 'event' column:\")\n",
    "print(event_counts)\n",
    "\n",
    "# Print the maximum class number\n",
    "max_class_number = df['event'].max()\n",
    "print(\"\\nMaximum class number:\")\n",
    "print(max_class_number)\n",
    "\n",
    "# Determine the number of classes\n",
    "print(\"\\nExpected number of classes (from num_classes variable):\", max_class_number + 1)\n",
    "\n",
    "# Reset the class labels to be in the range [0, num_classes-1]\n",
    "unique_classes = sorted(df['event'].unique())\n",
    "class_mapping = {old_class: new_class for new_class, old_class in enumerate(unique_classes)}\n",
    "\n",
    "df['event'] = df['event'].map(class_mapping)\n",
    "print(\"\\nDataFrame with reset class labels:\")\n",
    "print(df)\n",
    "\n",
    "# Verify the new counts and max class number\n",
    "new_event_counts = df['event'].value_counts()\n",
    "new_max_class_number = df['event'].max()\n",
    "num_classes = new_max_class_number + 1\n",
    "\n",
    "print(\"\\nNew counts of each class in the 'event' column:\")\n",
    "print(new_event_counts)\n",
    "print(\"\\nNew maximum class number:\")\n",
    "print(new_max_class_number)\n",
    "print(\"\\nNew expected number of classes (from num_classes variable):\", new_max_class_number + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices where the 'event' label changes: [0, 3841, 4994, 6147, 8453, 9606, 10759, 11912, 13065, 14218, 15371, 17677, 18830, 19983, 21136, 23442, 24595, 25748, 26901, 28054, 29207, 30360, 31513, 33819, 34972, 36125, 38431, 39584, 40737, 41890, 45349, 46502, 48808, 49961, 52267, 55726, 56879, 58032, 60338, 61491, 62644, 63797, 66103, 67256, 68409, 69562, 70715, 71868, 73021, 74174, 75327, 77633, 78786, 81092, 82245, 83398, 85704, 86857, 88010, 89163, 90316, 91469, 92622, 94928, 96081, 98387, 101846, 102999, 104152, 106458, 107611, 108764, 109917, 112223, 113376, 114529, 115682, 116835, 117988, 119141, 120294, 121447, 123753, 124906, 127212, 128365, 129518, 131824, 132977, 134130, 135283, 136436, 137589, 138742, 141048, 142201, 143354, 144507, 146813, 149119, 150272, 151425, 152578, 153731, 157190, 158343, 160649, 161802, 162955, 164108, 165261, 166414, 168720, 169873, 172179, 173332, 175638, 176791, 179097, 180250, 181403, 182556, 183709, 184862, 186015, 187168, 188321, 190627, 192933, 194086, 195239, 196392, 197545, 198698, 199851, 202157, 203310, 204463, 205616, 207922, 209075, 210228, 211381, 214840, 215993, 217146, 218299, 219452, 220605, 221758, 222911, 224064, 225217, 226370, 228676, 229829, 230982, 232135, 233288, 234441, 238282, 239435, 240588, 241741, 245200, 246353, 248659, 250965, 252118, 253271, 254424, 255577, 256730, 257883, 259036, 260189, 261342, 264801, 265954, 268260, 269413, 270566, 271719, 274025, 275178, 276331, 277484, 278637, 279790, 280943, 282096, 284402, 286708, 289014, 290167, 292473, 293626, 295932, 297085, 298238, 299391, 300544, 301697, 305156, 306309, 309768, 310921, 312074, 313227, 314380, 315533, 316686, 317839, 318992, 320145, 322451, 323604, 325910, 327063, 328216, 330522, 332828, 335134, 336287, 338593, 339746, 342052, 343205, 344358, 345511, 346664, 347817, 351276, 352429, 355888, 357041, 358194, 359347, 360500, 361653, 362806, 363959, 365112, 366265, 368571, 369724, 372030, 373183, 374336, 376642, 377795, 378948, 380101, 381254, 382407, 383560, 384713, 385866, 387019, 388172, 389325, 390478, 391631, 392784, 393937, 395090, 396243, 398549, 399702, 402008, 403161, 404314, 405467, 406620, 407773, 408926, 411232, 412385, 414691, 415844, 416997, 418150, 419303, 420456, 421609, 422762, 423915, 425068, 426221, 427374, 428527, 429680, 430833, 433139, 434292, 435445, 436598, 437751, 438904, 441210, 442363, 443516, 444669, 445822, 446975, 448128, 449281, 450434, 452740, 453893, 455046, 456199, 457352, 458505, 459658, 460811, 461964, 464270, 466576, 467729, 468882, 472723, 473876, 475029, 476182, 479641, 480794, 481947, 484253, 485406, 486559, 487712, 488865, 491171, 492324, 493477, 494630, 495783, 498089, 499242, 500395, 502701, 505007, 506160, 507313, 508466, 509619, 510772, 511925, 516537, 518843, 521149, 522302, 523455, 525761, 528067, 529220, 530373, 531526, 532679, 533832, 534985, 539597, 541903, 543056, 544209, 545362, 546515, 549974, 551127, 552280, 553433, 554586, 555739, 556892, 558045, 559198, 560351, 561504, 562657, 563810, 564963, 568422, 569575, 570728, 571881, 574187, 575340, 576493, 577646, 578799, 579952, 581105, 582258, 583411, 584564, 585717]\n",
      "Lengths between changes: [3841, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 2306, 1153, 1153, 1153, 3459, 1153, 2306, 1153, 2306, 3459, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 3459, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 3459, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 3459, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 3841, 1153, 1153, 1153, 3459, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 3459, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 2306, 2306, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 3459, 1153, 3459, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 2306, 2306, 2306, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 3459, 1153, 3459, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 2306, 1153, 1153, 3841, 1153, 1153, 1153, 3459, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 4612, 2306, 2306, 1153, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 4612, 2306, 1153, 1153, 1153, 1153, 3459, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 3459, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153]\n",
      "Minimum cycle length: 1153\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is defined and already includes an 'event' column\n",
    "# Assuming 'event' column contains class labels\n",
    "event_changes = df['event'].diff().ne(0)\n",
    "change_indices = event_changes[event_changes].index.tolist()\n",
    "\n",
    "# Calculate and print lengths between changes\n",
    "lengths_between_changes = [change_indices[i] - change_indices[i-1] for i in range(1, len(change_indices))]\n",
    "\n",
    "# Find the minimum cycle length where the label changes\n",
    "min_cycle_length = min(lengths_between_changes)\n",
    "\n",
    "print(\"Indices where the 'event' label changes:\", change_indices)\n",
    "print(\"Lengths between changes:\", lengths_between_changes)\n",
    "print(f\"Minimum cycle length: {min_cycle_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_tensor shape: torch.Size([588023, 128])\n",
      "         A1        A2        A3        A4        A5        A6        A7  \\\n",
      "0 -2.488196 -2.276049 -2.513326 -2.187655 -0.665328 -1.345729 -0.058758   \n",
      "1 -2.275978 -2.101950 -2.131188 -2.183079 -0.334046 -0.958461  0.322920   \n",
      "2 -1.464012 -1.438715 -1.267226 -1.501220  0.528039 -0.018531  1.138506   \n",
      "3 -1.288701 -1.256325 -1.333684 -1.322747  0.305930 -0.010463  1.323319   \n",
      "4 -1.962264 -1.981739 -2.139496 -1.853590 -0.356633 -0.631705  0.744775   \n",
      "\n",
      "         A8        A9       A10  ...       D24       D25       D26       D27  \\\n",
      "0  0.063271  0.040405 -1.022892  ... -1.909194 -1.004350 -0.560398 -1.681318   \n",
      "1  0.508570  0.570278 -0.405529  ... -1.421690 -0.624266  0.064576 -0.879298   \n",
      "2  0.803300  0.868933 -0.453136  ... -1.417197  0.003315  1.106200  0.347321   \n",
      "3  0.559827  0.640927 -0.430100  ... -1.310485 -0.432014  0.141858 -0.116593   \n",
      "4  0.204229  0.242720 -0.649710  ... -1.158842 -0.268490  0.383784 -0.470425   \n",
      "\n",
      "        D28       D29       D30       D31       D32  event  \n",
      "0 -1.691767 -1.766298 -0.931525 -1.651438 -1.624403      4  \n",
      "1 -0.971922 -1.347446 -0.658663 -1.326984 -1.307278      4  \n",
      "2  0.198306 -0.831935 -0.626976 -1.237656 -1.236575      4  \n",
      "3 -0.051907 -1.057471 -0.496707 -1.417338 -1.366544      4  \n",
      "4 -0.567732 -1.062428 -0.459739 -1.049760 -1.072294      4  \n",
      "\n",
      "[5 rows x 129 columns]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "segment_pairs = []\n",
    "\n",
    "# Correctly select only the numerical columns (exclude the 'event' column) and convert to a PyTorch tensor\n",
    "df_tensor = torch.tensor(df.iloc[:, :-1].values).float().cuda()  # Using .iloc and .values to correctly handle DataFrame slicing\n",
    "\n",
    "print(\"df_tensor shape:\", df_tensor.shape)\n",
    "# Define a function to perform robust scaling using PyTorch\n",
    "def robust_scale_gpu(data):\n",
    "    median = torch.median(data, dim=0, keepdim=True).values\n",
    "    q75, q25 = torch.quantile(data, torch.tensor([0.75, 0.25], device=data.device), dim=0, keepdim=True)\n",
    "    iqr = q75 - q25\n",
    "\n",
    "    return (data - median) / iqr\n",
    "\n",
    "def standard_scale_gpu(data):\n",
    "    mean = torch.mean(data, dim=0, keepdim=True)\n",
    "    std = torch.std(data, dim=0, keepdim=True)\n",
    "\n",
    "    return (data - mean) / (std + 1e-8)\n",
    "\n",
    "for start, end in zip(change_indices[:-1], change_indices[1:]):\n",
    "    segment_length = end - start\n",
    "    if segment_length >= min_cycle_length and segment_length % min_cycle_length == 0:\n",
    "        # Normalize each sub-segment within the main segment\n",
    "        for offset in range(0, segment_length, min_cycle_length):\n",
    "            sub_start = start + offset\n",
    "            sub_end = sub_start + min_cycle_length\n",
    "            segment = df_tensor[sub_start:sub_end, :]\n",
    "            scaled_segment = standard_scale_gpu(segment)\n",
    "            df_tensor[sub_start:sub_end, :] = scaled_segment  # Correctly place the scaled data back into the DataFrame\n",
    "            segment_pairs.append((sub_start, sub_end))\n",
    "    else:\n",
    "        irregular_num = segment_length//min_cycle_length\n",
    "        # Normalize each sub-segment within the main segment\n",
    "        for i in range(irregular_num):\n",
    "            sub_start = start + i * min_cycle_length\n",
    "            if i == irregular_num - 1:\n",
    "                sub_end = end\n",
    "            else:\n",
    "                sub_end = sub_start + min_cycle_length\n",
    "            segment = df_tensor[sub_start:sub_end, :]\n",
    "            scaled_segment = standard_scale_gpu(segment)\n",
    "            df_tensor[sub_start:sub_end, :] = scaled_segment  # Correctly place the scaled data back into the DataFrame\n",
    "            segment_pairs.append((sub_start, sub_end))\n",
    "# Optionally, convert back to DataFrame if needed for further processing\n",
    "scaled_df = pd.DataFrame(df_tensor.cpu().numpy(), columns=df.columns[:-1])\n",
    "scaled_df['event'] = df['event']\n",
    "num_features = len(scaled_df.columns) - 1\n",
    "print(scaled_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class EEG_Dataset(Dataset):\n",
    "    def __init__(self, df, indices, max_window_size):\n",
    "        self.df = df\n",
    "        self.indices = indices  # List of start indices\n",
    "        self.max_window_size = max_window_size\n",
    "        self.min_window_size = max_window_size // 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = self.indices[idx]\n",
    "        # Randomly choose a window size between min_window_size and max_window_size\n",
    "        window_size = random.randint(self.min_window_size, self.max_window_size)\n",
    "        \n",
    "        end_idx = start_idx + window_size\n",
    "        # Make sure the end index does not go out of the bounds of the DataFrame\n",
    "        end_idx = min(end_idx, len(self.df))\n",
    "\n",
    "        # Retrieve the sequence using the calculated indices\n",
    "        seq = self.df.iloc[start_idx:end_idx]\n",
    "        X, y = seq.values[:, :-1], seq.values[:, -1]\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.long)  # ensure y is a tensor of type long\n",
    "        y = torch.nn.functional.one_hot(y, num_classes=num_classes)  # correct use\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from random import shuffle\n",
    "\n",
    "# Assume 'df' is your DataFrame and 'event' is the column containing labels\n",
    "\n",
    "# Generate indices without mixing segments\n",
    "def generate_indices(input_df, segment_pairs, max_window_size):\n",
    "    indices = []\n",
    "    for start, end in segment_pairs:\n",
    "        max_index = end - max_window_size  # Calculate the maximum starting index for this segment\n",
    "        for i in range(start, max_index):\n",
    "            # Check if all labels in the window are the same\n",
    "            if len(input_df['event'][i:i + max_window_size].unique()) == 1:\n",
    "                indices.append(i)\n",
    "    return indices\n",
    "\n",
    "# Example usage\n",
    "max_window_size = 128\n",
    "indices = generate_indices(scaled_df, segment_pairs, max_window_size)\n",
    "\n",
    "# Split the indices into training and testing sets\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.2, shuffle=False)\n",
    "shuffle(train_indices)  # Shuffle the indices to randomize the data order\n",
    "\n",
    "# Assuming you have an EEG_Dataset class defined as before\n",
    "trainset = EEG_Dataset(df=scaled_df, indices=train_indices, max_window_size=max_window_size)\n",
    "testset = EEG_Dataset(df=scaled_df, indices=test_indices, max_window_size=max_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CCNets-team\\anaconda3\\lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from tools.setting.data_config import DataConfig\n",
    "from tools.setting.ml_params import MLParameters\n",
    "from trainer_hub import TrainerHub\n",
    "import torch\n",
    "\n",
    "data_config = DataConfig(dataset_name = 'eeg-sub-01', task_type='multi_class_classification', obs_shape=[num_features], label_size=num_classes)\n",
    "\n",
    "#  Set training configuration from the AlgorithmConfig class, returning them as a Namespace object.\n",
    "ml_params = MLParameters(core_model = 'gpt', encoder_model = 'none')\n",
    "\n",
    "# Set the device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Initialize the TrainerHub class with the training configuration, data configuration, device, and use_print and use_wandb flags\n",
    "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574c0f7b5ed84453b3af4af20fbb2260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8eac5cc4b344f3a1c784e6446673ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/6516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100][50/6516][Time 19.61]\n",
      "Unified LR across all optimizers: 0.0001995308238189185\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.3181\tGen: 0.6724\tRec: 0.6137\tE: 0.3766\tR: 0.2593\tP: 0.9703\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.1125\n",
      "precision: 0.0576\n",
      "recall: 0.1123\n",
      "f1_score: 0.0618\n",
      "\n",
      "[0/100][100/6516][Time 18.53]\n",
      "Unified LR across all optimizers: 0.00019907191565870155\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1616\tGen: 0.4767\tRec: 0.4516\tE: 0.1873\tR: 0.1369\tP: 0.7687\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.1594\n",
      "precision: 0.0727\n",
      "recall: 0.1361\n",
      "f1_score: 0.0723\n",
      "\n",
      "[0/100][150/6516][Time 18.53]\n",
      "Unified LR across all optimizers: 0.00019861406295796434\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1279\tGen: 0.4318\tRec: 0.4150\tE: 0.1448\tR: 0.1113\tP: 0.7181\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.1641\n",
      "precision: 0.1237\n",
      "recall: 0.1669\n",
      "f1_score: 0.1125\n",
      "\n",
      "[0/100][200/6516][Time 18.47]\n",
      "Unified LR across all optimizers: 0.00019815726328921765\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1197\tGen: 0.4148\tRec: 0.4001\tE: 0.1347\tR: 0.1054\tP: 0.6957\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.1859\n",
      "precision: 0.1566\n",
      "recall: 0.1778\n",
      "f1_score: 0.1019\n",
      "\n",
      "[0/100][250/6516][Time 18.55]\n",
      "Unified LR across all optimizers: 0.00019770151423055492\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1148\tGen: 0.3996\tRec: 0.3861\tE: 0.1285\tR: 0.1012\tP: 0.6740\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.2688\n",
      "precision: 0.2105\n",
      "recall: 0.2310\n",
      "f1_score: 0.1481\n",
      "\n",
      "[0/100][300/6516][Time 18.50]\n",
      "Unified LR across all optimizers: 0.00019724681336564005\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1131\tGen: 0.3851\tRec: 0.3718\tE: 0.1277\tR: 0.1006\tP: 0.6446\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.2906\n",
      "precision: 0.2567\n",
      "recall: 0.2445\n",
      "f1_score: 0.1814\n",
      "\n",
      "[0/100][350/6516][Time 18.59]\n",
      "Unified LR across all optimizers: 0.00019679315828369438\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1066\tGen: 0.3740\tRec: 0.3618\tE: 0.1185\tR: 0.0940\tP: 0.6309\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.3203\n",
      "precision: 0.2048\n",
      "recall: 0.2698\n",
      "f1_score: 0.1960\n",
      "\n",
      "[0/100][400/6516][Time 18.64]\n",
      "Unified LR across all optimizers: 0.00019634054657948372\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1037\tGen: 0.3607\tRec: 0.3492\tE: 0.1151\tR: 0.0920\tP: 0.6084\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.3672\n",
      "precision: 0.2983\n",
      "recall: 0.3082\n",
      "f1_score: 0.2408\n",
      "\n",
      "[0/100][450/6516][Time 18.64]\n",
      "Unified LR across all optimizers: 0.00019588897585330582\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0962\tGen: 0.3527\tRec: 0.3431\tE: 0.1053\tR: 0.0861\tP: 0.6011\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.3938\n",
      "precision: 0.3440\n",
      "recall: 0.3533\n",
      "f1_score: 0.2769\n",
      "\n",
      "[0/100][500/6516][Time 18.62]\n",
      "Unified LR across all optimizers: 0.00019543844371097777\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0930\tGen: 0.3469\tRec: 0.3373\tE: 0.1024\tR: 0.0830\tP: 0.5920\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.4203\n",
      "precision: 0.3469\n",
      "recall: 0.3820\n",
      "f1_score: 0.3004\n",
      "\n",
      "[0/100][550/6516][Time 18.67]\n",
      "Unified LR across all optimizers: 0.00019498894776382288\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0843\tGen: 0.3397\tRec: 0.3313\tE: 0.0919\tR: 0.0747\tP: 0.5882\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.5000\n",
      "precision: 0.3262\n",
      "recall: 0.4135\n",
      "f1_score: 0.3336\n",
      "\n",
      "[0/100][600/6516][Time 18.65]\n",
      "Unified LR across all optimizers: 0.00019454048562865856\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0850\tGen: 0.3330\tRec: 0.3247\tE: 0.0923\tR: 0.0757\tP: 0.5741\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.5328\n",
      "precision: 0.3868\n",
      "recall: 0.4794\n",
      "f1_score: 0.3966\n",
      "\n",
      "[0/100][650/6516][Time 18.68]\n",
      "Unified LR across all optimizers: 0.00019409305492778308\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0746\tGen: 0.3270\tRec: 0.3204\tE: 0.0799\tR: 0.0666\tP: 0.5742\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.5422\n",
      "precision: 0.3697\n",
      "recall: 0.4746\n",
      "f1_score: 0.3941\n",
      "\n",
      "[0/100][700/6516][Time 18.67]\n",
      "Unified LR across all optimizers: 0.00019364665328896346\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0740\tGen: 0.3241\tRec: 0.3173\tE: 0.0793\tR: 0.0654\tP: 0.5710\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.5625\n",
      "precision: 0.3921\n",
      "recall: 0.4935\n",
      "f1_score: 0.4124\n",
      "\n",
      "[0/100][750/6516][Time 18.65]\n",
      "Unified LR across all optimizers: 0.00019320127834542263\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0698\tGen: 0.3188\tRec: 0.3128\tE: 0.0743\tR: 0.0624\tP: 0.5644\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.5984\n",
      "precision: 0.4001\n",
      "recall: 0.5186\n",
      "f1_score: 0.4358\n",
      "\n",
      "[0/100][800/6516][Time 18.71]\n",
      "Unified LR across all optimizers: 0.00019275692773582703\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0703\tGen: 0.3157\tRec: 0.3097\tE: 0.0747\tR: 0.0628\tP: 0.5581\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.5500\n",
      "precision: 0.4577\n",
      "recall: 0.5125\n",
      "f1_score: 0.4278\n",
      "\n",
      "[0/100][850/6516][Time 18.81]\n",
      "Unified LR across all optimizers: 0.0001923135991042739\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0655\tGen: 0.3088\tRec: 0.3034\tE: 0.0699\tR: 0.0588\tP: 0.5486\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6328\n",
      "precision: 0.5509\n",
      "recall: 0.5423\n",
      "f1_score: 0.4840\n",
      "\n",
      "[0/100][900/6516][Time 18.69]\n",
      "Unified LR across all optimizers: 0.0001918712901002789\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0631\tGen: 0.3066\tRec: 0.3015\tE: 0.0676\tR: 0.0570\tP: 0.5478\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6094\n",
      "precision: 0.5055\n",
      "recall: 0.5599\n",
      "f1_score: 0.4882\n",
      "\n",
      "[0/100][950/6516][Time 18.71]\n",
      "Unified LR across all optimizers: 0.00019142999837876384\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0612\tGen: 0.3012\tRec: 0.2962\tE: 0.0649\tR: 0.0547\tP: 0.5373\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6750\n",
      "precision: 0.5364\n",
      "recall: 0.5909\n",
      "f1_score: 0.5340\n",
      "\n",
      "[0/100][1000/6516][Time 18.69]\n",
      "Unified LR across all optimizers: 0.00019098972160004388\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0619\tGen: 0.3022\tRec: 0.2975\tE: 0.0650\tR: 0.0552\tP: 0.5407\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6187\n",
      "precision: 0.5358\n",
      "recall: 0.5848\n",
      "f1_score: 0.5101\n",
      "\n",
      "[0/100][1050/6516][Time 18.72]\n",
      "Unified LR across all optimizers: 0.00019055045742981543\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0576\tGen: 0.2947\tRec: 0.2907\tE: 0.0602\tR: 0.0519\tP: 0.5306\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.6828\n",
      "precision: 0.5926\n",
      "recall: 0.6421\n",
      "f1_score: 0.5855\n",
      "\n",
      "[0/100][1100/6516][Time 18.70]\n",
      "Unified LR across all optimizers: 0.00019011220353914353\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0532\tGen: 0.2915\tRec: 0.2878\tE: 0.0546\tR: 0.0473\tP: 0.5299\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.7250\n",
      "precision: 0.6166\n",
      "recall: 0.6555\n",
      "f1_score: 0.6051\n",
      "\n",
      "[0/100][1150/6516][Time 18.68]\n",
      "Unified LR across all optimizers: 0.00018967495760444968\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0519\tGen: 0.2890\tRec: 0.2856\tE: 0.0530\tR: 0.0464\tP: 0.5258\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.7656\n",
      "precision: 0.6864\n",
      "recall: 0.6883\n",
      "f1_score: 0.6405\n",
      "\n",
      "[0/100][1200/6516][Time 18.69]\n",
      "Unified LR across all optimizers: 0.00018923871730749947\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0499\tGen: 0.2864\tRec: 0.2830\tE: 0.0519\tR: 0.0450\tP: 0.5214\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.7375\n",
      "precision: 0.6273\n",
      "recall: 0.6955\n",
      "f1_score: 0.6342\n",
      "\n",
      "[0/100][1250/6516][Time 18.72]\n",
      "Unified LR across all optimizers: 0.00018880348033539028\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0439\tGen: 0.2816\tRec: 0.2787\tE: 0.0455\tR: 0.0396\tP: 0.5184\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.7422\n",
      "precision: 0.6198\n",
      "recall: 0.6695\n",
      "f1_score: 0.6265\n",
      "\n",
      "[0/100][1300/6516][Time 18.69]\n",
      "Unified LR across all optimizers: 0.00018836924438053897\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0439\tGen: 0.2788\tRec: 0.2760\tE: 0.0446\tR: 0.0389\tP: 0.5136\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.7203\n",
      "precision: 0.6455\n",
      "recall: 0.6581\n",
      "f1_score: 0.6167\n",
      "\n",
      "[0/100][1350/6516][Time 18.69]\n",
      "Unified LR across all optimizers: 0.0001879360071406698\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0436\tGen: 0.2758\tRec: 0.2731\tE: 0.0446\tR: 0.0390\tP: 0.5083\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.7344\n",
      "precision: 0.6179\n",
      "recall: 0.7113\n",
      "f1_score: 0.6388\n",
      "\n",
      "[0/100][1400/6516][Time 18.64]\n",
      "Unified LR across all optimizers: 0.000187503766318802\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0416\tGen: 0.2749\tRec: 0.2725\tE: 0.0425\tR: 0.0378\tP: 0.5083\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.7922\n",
      "precision: 0.6872\n",
      "recall: 0.7468\n",
      "f1_score: 0.7002\n",
      "\n",
      "[0/100][1450/6516][Time 18.70]\n",
      "Unified LR across all optimizers: 0.00018707251962323787\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0414\tGen: 0.2702\tRec: 0.2679\tE: 0.0421\tR: 0.0376\tP: 0.5007\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.7703\n",
      "precision: 0.6597\n",
      "recall: 0.7401\n",
      "f1_score: 0.6851\n",
      "\n",
      "[0/100][1500/6516][Time 18.67]\n",
      "Unified LR across all optimizers: 0.0001866422647675502\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0406\tGen: 0.2685\tRec: 0.2663\tE: 0.0414\tR: 0.0367\tP: 0.4976\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.7750\n",
      "precision: 0.7019\n",
      "recall: 0.7222\n",
      "f1_score: 0.6872\n",
      "\n",
      "[0/100][1550/6516][Time 18.66]\n",
      "Unified LR across all optimizers: 0.00018621299947057073\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0385\tGen: 0.2673\tRec: 0.2653\tE: 0.0389\tR: 0.0347\tP: 0.4970\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8078\n",
      "precision: 0.7051\n",
      "recall: 0.7566\n",
      "f1_score: 0.7216\n",
      "\n",
      "[0/100][1600/6516][Time 18.69]\n",
      "Unified LR across all optimizers: 0.00018578472145637737\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0369\tGen: 0.2612\tRec: 0.2597\tE: 0.0366\tR: 0.0333\tP: 0.4866\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8281\n",
      "precision: 0.6957\n",
      "recall: 0.7847\n",
      "f1_score: 0.7307\n",
      "\n",
      "[0/100][1650/6516][Time 18.67]\n",
      "Unified LR across all optimizers: 0.00018535742845428288\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0360\tGen: 0.2600\tRec: 0.2583\tE: 0.0365\tR: 0.0328\tP: 0.4846\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.7906\n",
      "precision: 0.6963\n",
      "recall: 0.7861\n",
      "f1_score: 0.7315\n",
      "\n",
      "[0/100][1700/6516][Time 18.64]\n",
      "Unified LR across all optimizers: 0.00018493111819882223\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0357\tGen: 0.2579\tRec: 0.2563\tE: 0.0351\tR: 0.0318\tP: 0.4816\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8266\n",
      "precision: 0.7005\n",
      "recall: 0.7958\n",
      "f1_score: 0.7404\n",
      "\n",
      "[0/100][1750/6516][Time 18.73]\n",
      "Unified LR across all optimizers: 0.00018450578842974107\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0337\tGen: 0.2552\tRec: 0.2537\tE: 0.0338\tR: 0.0306\tP: 0.4783\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8234\n",
      "precision: 0.7233\n",
      "recall: 0.7958\n",
      "f1_score: 0.7465\n",
      "\n",
      "[0/100][1800/6516][Time 18.44]\n",
      "Unified LR across all optimizers: 0.00018408143689198318\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0325\tGen: 0.2533\tRec: 0.2518\tE: 0.0328\tR: 0.0298\tP: 0.4745\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8281\n",
      "precision: 0.7260\n",
      "recall: 0.8036\n",
      "f1_score: 0.7565\n",
      "\n",
      "[0/100][1850/6516][Time 18.73]\n",
      "Unified LR across all optimizers: 0.0001836580613356789\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0325\tGen: 0.2507\tRec: 0.2493\tE: 0.0326\tR: 0.0296\tP: 0.4707\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8453\n",
      "precision: 0.7120\n",
      "recall: 0.8128\n",
      "f1_score: 0.7539\n",
      "\n",
      "[0/100][1900/6516][Time 18.78]\n",
      "Unified LR across all optimizers: 0.0001832356595161332\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0327\tGen: 0.2471\tRec: 0.2456\tE: 0.0327\tR: 0.0294\tP: 0.4631\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8375\n",
      "precision: 0.7364\n",
      "recall: 0.8044\n",
      "f1_score: 0.7637\n",
      "\n",
      "[0/100][1950/6516][Time 18.78]\n",
      "Unified LR across all optimizers: 0.00018281422919381367\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0319\tGen: 0.2463\tRec: 0.2449\tE: 0.0319\tR: 0.0287\tP: 0.4617\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8641\n",
      "precision: 0.8141\n",
      "recall: 0.8266\n",
      "f1_score: 0.7857\n",
      "\n",
      "[0/100][2000/6516][Time 18.62]\n",
      "Unified LR across all optimizers: 0.00018239376813433867\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0312\tGen: 0.2444\tRec: 0.2432\tE: 0.0309\tR: 0.0282\tP: 0.4595\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8656\n",
      "precision: 0.8119\n",
      "recall: 0.8204\n",
      "f1_score: 0.7895\n",
      "\n",
      "[0/100][2050/6516][Time 18.11]\n",
      "Unified LR across all optimizers: 0.00018197427410846564\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0302\tGen: 0.2425\tRec: 0.2416\tE: 0.0295\tR: 0.0274\tP: 0.4564\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8703\n",
      "precision: 0.7903\n",
      "recall: 0.8534\n",
      "f1_score: 0.8153\n",
      "\n",
      "[0/100][2100/6516][Time 18.14]\n",
      "Unified LR across all optimizers: 0.00018155574489207887\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0274\tGen: 0.2393\tRec: 0.2387\tE: 0.0267\tR: 0.0252\tP: 0.4533\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8609\n",
      "precision: 0.7991\n",
      "recall: 0.8352\n",
      "f1_score: 0.8075\n",
      "\n",
      "[0/100][2150/6516][Time 18.16]\n",
      "Unified LR across all optimizers: 0.00018113817826617823\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0269\tGen: 0.2381\tRec: 0.2373\tE: 0.0256\tR: 0.0240\tP: 0.4525\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9031\n",
      "precision: 0.8333\n",
      "recall: 0.8585\n",
      "f1_score: 0.8444\n",
      "\n",
      "[0/100][2200/6516][Time 18.20]\n",
      "Unified LR across all optimizers: 0.00018072157201686696\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0234\tGen: 0.2371\tRec: 0.2371\tE: 0.0212\tR: 0.0209\tP: 0.4537\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8656\n",
      "precision: 0.7859\n",
      "recall: 0.8497\n",
      "f1_score: 0.8124\n",
      "\n",
      "[0/100][2250/6516][Time 18.53]\n",
      "Unified LR across all optimizers: 0.00018030592393534033\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0256\tGen: 0.2347\tRec: 0.2345\tE: 0.0239\tR: 0.0232\tP: 0.4471\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8984\n",
      "precision: 0.8366\n",
      "recall: 0.8581\n",
      "f1_score: 0.8461\n",
      "\n",
      "[0/100][2300/6516][Time 18.77]\n",
      "Unified LR across all optimizers: 0.0001798912318178735\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0244\tGen: 0.2330\tRec: 0.2328\tE: 0.0225\tR: 0.0219\tP: 0.4449\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9016\n",
      "precision: 0.8399\n",
      "recall: 0.8760\n",
      "f1_score: 0.8539\n",
      "\n",
      "[0/100][2350/6516][Time 18.69]\n",
      "Unified LR across all optimizers: 0.00017947749346581006\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0226\tGen: 0.2314\tRec: 0.2312\tE: 0.0204\tR: 0.0199\tP: 0.4438\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8906\n",
      "precision: 0.8270\n",
      "recall: 0.8852\n",
      "f1_score: 0.8537\n",
      "\n",
      "[0/100][2400/6516][Time 18.68]\n",
      "Unified LR across all optimizers: 0.0001790647066855505\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0224\tGen: 0.2305\tRec: 0.2305\tE: 0.0210\tR: 0.0207\tP: 0.4418\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.8953\n",
      "precision: 0.8400\n",
      "recall: 0.8695\n",
      "f1_score: 0.8497\n",
      "\n",
      "[0/100][2450/6516][Time 18.79]\n",
      "Unified LR across all optimizers: 0.00017865286928854052\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0208\tGen: 0.2290\tRec: 0.2288\tE: 0.0191\tR: 0.0185\tP: 0.4401\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9406\n",
      "precision: 0.8703\n",
      "recall: 0.9035\n",
      "f1_score: 0.8860\n",
      "\n",
      "[0/100][2500/6516][Time 18.78]\n",
      "Unified LR across all optimizers: 0.00017824197909125899\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0197\tGen: 0.2266\tRec: 0.2266\tE: 0.0178\tR: 0.0178\tP: 0.4362\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9297\n",
      "precision: 0.8631\n",
      "recall: 0.8973\n",
      "f1_score: 0.8774\n",
      "\n",
      "[0/100][2550/6516][Time 18.80]\n",
      "Unified LR across all optimizers: 0.00017783203391520723\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0206\tGen: 0.2245\tRec: 0.2245\tE: 0.0186\tR: 0.0184\tP: 0.4321\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9297\n",
      "precision: 0.9393\n",
      "recall: 0.9135\n",
      "f1_score: 0.8951\n",
      "\n",
      "[0/100][2600/6516][Time 18.80]\n",
      "Unified LR across all optimizers: 0.00017742303158689668\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0181\tGen: 0.2230\tRec: 0.2231\tE: 0.0159\tR: 0.0159\tP: 0.4311\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9187\n",
      "precision: 0.9313\n",
      "recall: 0.9046\n",
      "f1_score: 0.8822\n",
      "\n",
      "[0/100][2650/6516][Time 18.77]\n",
      "Unified LR across all optimizers: 0.00017701496993783762\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0191\tGen: 0.2203\tRec: 0.2203\tE: 0.0176\tR: 0.0173\tP: 0.4245\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9344\n",
      "precision: 0.8670\n",
      "recall: 0.9072\n",
      "f1_score: 0.8856\n",
      "\n",
      "[0/100][2700/6516][Time 18.78]\n",
      "Unified LR across all optimizers: 0.00017660784680452796\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0176\tGen: 0.2192\tRec: 0.2194\tE: 0.0160\tR: 0.0160\tP: 0.4243\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9328\n",
      "precision: 0.9397\n",
      "recall: 0.9200\n",
      "f1_score: 0.8974\n",
      "\n",
      "[0/100][2750/6516][Time 18.78]\n",
      "Unified LR across all optimizers: 0.0001762016600284412\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0167\tGen: 0.2167\tRec: 0.2168\tE: 0.0148\tR: 0.0147\tP: 0.4193\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9187\n",
      "precision: 0.9226\n",
      "recall: 0.9169\n",
      "f1_score: 0.8831\n",
      "\n",
      "[0/100][2800/6516][Time 18.78]\n",
      "Unified LR across all optimizers: 0.00017579640745601563\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.0181\tGen: 0.2162\tRec: 0.2163\tE: 0.0164\tR: 0.0163\tP: 0.4179\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.9422\n",
      "precision: 0.9456\n",
      "recall: 0.9220\n",
      "f1_score: 0.9110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer_hub.train(trainset, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_hub.test(testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
