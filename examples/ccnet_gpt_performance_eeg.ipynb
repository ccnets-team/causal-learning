{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author:\n",
    "        \n",
    "        PARK, JunHo, junho@ccnets.org\n",
    "\n",
    "        Kim, Jinsu \n",
    "\n",
    "        KIM, JeongYoong, jeongyoong@ccnets.org\n",
    "        \n",
    "    COPYRIGHT (c) 2024. CCNets. All Rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path_append = \"../\"\n",
    "sys.path.append(path_append)  # Go up one directory from where you are.\n",
    "\n",
    "from nn.utils.init import set_random_seed\n",
    "set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>...</th>\n",
       "      <th>D24</th>\n",
       "      <th>D25</th>\n",
       "      <th>D26</th>\n",
       "      <th>D27</th>\n",
       "      <th>D28</th>\n",
       "      <th>D29</th>\n",
       "      <th>D30</th>\n",
       "      <th>D31</th>\n",
       "      <th>D32</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3549.790315</td>\n",
       "      <td>4533.538497</td>\n",
       "      <td>3619.665186</td>\n",
       "      <td>3077.291188</td>\n",
       "      <td>-1380.325575</td>\n",
       "      <td>6120.066816</td>\n",
       "      <td>-4072.820600</td>\n",
       "      <td>-2256.511456</td>\n",
       "      <td>1820.012261</td>\n",
       "      <td>-2815.635423</td>\n",
       "      <td>...</td>\n",
       "      <td>-7240.845997</td>\n",
       "      <td>7034.252627</td>\n",
       "      <td>8458.062496</td>\n",
       "      <td>5905.223463</td>\n",
       "      <td>6147.660515</td>\n",
       "      <td>2458.073582</td>\n",
       "      <td>-7465.876831</td>\n",
       "      <td>-3604.133966</td>\n",
       "      <td>-5445.224315</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3551.227812</td>\n",
       "      <td>4534.850995</td>\n",
       "      <td>3622.540181</td>\n",
       "      <td>3077.322438</td>\n",
       "      <td>-1377.575581</td>\n",
       "      <td>6123.066810</td>\n",
       "      <td>-4069.851856</td>\n",
       "      <td>-2252.167714</td>\n",
       "      <td>1825.168502</td>\n",
       "      <td>-2803.072947</td>\n",
       "      <td>...</td>\n",
       "      <td>-7227.283522</td>\n",
       "      <td>7039.627617</td>\n",
       "      <td>8463.874985</td>\n",
       "      <td>5911.598451</td>\n",
       "      <td>6153.504254</td>\n",
       "      <td>2463.354822</td>\n",
       "      <td>-7461.033090</td>\n",
       "      <td>-3594.258985</td>\n",
       "      <td>-5435.693082</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3556.727802</td>\n",
       "      <td>4539.850986</td>\n",
       "      <td>3629.040169</td>\n",
       "      <td>3081.978679</td>\n",
       "      <td>-1370.419344</td>\n",
       "      <td>6130.348047</td>\n",
       "      <td>-4063.508118</td>\n",
       "      <td>-2249.292720</td>\n",
       "      <td>1828.074746</td>\n",
       "      <td>-2804.041695</td>\n",
       "      <td>...</td>\n",
       "      <td>-7227.158522</td>\n",
       "      <td>7048.502600</td>\n",
       "      <td>8473.562467</td>\n",
       "      <td>5921.348433</td>\n",
       "      <td>6163.004236</td>\n",
       "      <td>2469.854810</td>\n",
       "      <td>-7460.470591</td>\n",
       "      <td>-3591.540240</td>\n",
       "      <td>-5433.568086</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3557.915300</td>\n",
       "      <td>4541.225983</td>\n",
       "      <td>3628.540169</td>\n",
       "      <td>3083.197427</td>\n",
       "      <td>-1372.263090</td>\n",
       "      <td>6130.410547</td>\n",
       "      <td>-4062.070620</td>\n",
       "      <td>-2251.667715</td>\n",
       "      <td>1825.856000</td>\n",
       "      <td>-2803.572946</td>\n",
       "      <td>...</td>\n",
       "      <td>-7224.189777</td>\n",
       "      <td>7042.346362</td>\n",
       "      <td>8464.593734</td>\n",
       "      <td>5917.660940</td>\n",
       "      <td>6160.972990</td>\n",
       "      <td>2467.011066</td>\n",
       "      <td>-7458.158095</td>\n",
       "      <td>-3597.008980</td>\n",
       "      <td>-5437.474329</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3553.352808</td>\n",
       "      <td>4535.757243</td>\n",
       "      <td>3622.477681</td>\n",
       "      <td>3079.572434</td>\n",
       "      <td>-1377.763080</td>\n",
       "      <td>6125.598056</td>\n",
       "      <td>-4066.570612</td>\n",
       "      <td>-2255.136459</td>\n",
       "      <td>1821.981008</td>\n",
       "      <td>-2808.041687</td>\n",
       "      <td>...</td>\n",
       "      <td>-7219.971035</td>\n",
       "      <td>7044.658857</td>\n",
       "      <td>8466.843729</td>\n",
       "      <td>5914.848445</td>\n",
       "      <td>6156.785498</td>\n",
       "      <td>2466.948566</td>\n",
       "      <td>-7457.501846</td>\n",
       "      <td>-3585.821500</td>\n",
       "      <td>-5428.630595</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588018</th>\n",
       "      <td>-623.326974</td>\n",
       "      <td>2269.261431</td>\n",
       "      <td>2575.479615</td>\n",
       "      <td>285.733846</td>\n",
       "      <td>907.388947</td>\n",
       "      <td>-491.014719</td>\n",
       "      <td>-2998.447586</td>\n",
       "      <td>1886.043389</td>\n",
       "      <td>1659.637557</td>\n",
       "      <td>416.296105</td>\n",
       "      <td>...</td>\n",
       "      <td>-7176.689865</td>\n",
       "      <td>2116.667963</td>\n",
       "      <td>-901.138961</td>\n",
       "      <td>-227.327706</td>\n",
       "      <td>-657.170662</td>\n",
       "      <td>3025.322534</td>\n",
       "      <td>-12313.149124</td>\n",
       "      <td>-3810.071086</td>\n",
       "      <td>-5620.505241</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588019</th>\n",
       "      <td>-627.420717</td>\n",
       "      <td>2264.448940</td>\n",
       "      <td>2570.323375</td>\n",
       "      <td>281.077605</td>\n",
       "      <td>903.482705</td>\n",
       "      <td>-490.702219</td>\n",
       "      <td>-3001.260080</td>\n",
       "      <td>1884.387142</td>\n",
       "      <td>1657.012562</td>\n",
       "      <td>414.702358</td>\n",
       "      <td>...</td>\n",
       "      <td>-7179.502360</td>\n",
       "      <td>2118.074210</td>\n",
       "      <td>-900.607712</td>\n",
       "      <td>-227.046456</td>\n",
       "      <td>-659.389408</td>\n",
       "      <td>3027.760030</td>\n",
       "      <td>-12307.211635</td>\n",
       "      <td>-3809.946086</td>\n",
       "      <td>-5621.098990</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588020</th>\n",
       "      <td>-631.764459</td>\n",
       "      <td>2260.730197</td>\n",
       "      <td>2566.917131</td>\n",
       "      <td>275.546365</td>\n",
       "      <td>902.045207</td>\n",
       "      <td>-493.545964</td>\n",
       "      <td>-3006.103821</td>\n",
       "      <td>1886.199639</td>\n",
       "      <td>1658.512560</td>\n",
       "      <td>424.202340</td>\n",
       "      <td>...</td>\n",
       "      <td>-7177.439864</td>\n",
       "      <td>2118.199210</td>\n",
       "      <td>-900.920211</td>\n",
       "      <td>-226.140208</td>\n",
       "      <td>-659.764407</td>\n",
       "      <td>3027.103781</td>\n",
       "      <td>-12305.774138</td>\n",
       "      <td>-3805.633594</td>\n",
       "      <td>-5614.880251</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588021</th>\n",
       "      <td>-625.076971</td>\n",
       "      <td>2265.605188</td>\n",
       "      <td>2573.354619</td>\n",
       "      <td>281.702604</td>\n",
       "      <td>904.982702</td>\n",
       "      <td>-490.795969</td>\n",
       "      <td>-3001.416330</td>\n",
       "      <td>1888.387135</td>\n",
       "      <td>1659.418808</td>\n",
       "      <td>420.077348</td>\n",
       "      <td>...</td>\n",
       "      <td>-7172.002374</td>\n",
       "      <td>2119.730457</td>\n",
       "      <td>-898.170216</td>\n",
       "      <td>-224.515211</td>\n",
       "      <td>-656.576913</td>\n",
       "      <td>3032.822520</td>\n",
       "      <td>-12303.742892</td>\n",
       "      <td>-3804.133597</td>\n",
       "      <td>-5614.192752</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588022</th>\n",
       "      <td>-623.639474</td>\n",
       "      <td>2268.636432</td>\n",
       "      <td>2576.229614</td>\n",
       "      <td>286.515095</td>\n",
       "      <td>909.795193</td>\n",
       "      <td>-484.358481</td>\n",
       "      <td>-2996.353839</td>\n",
       "      <td>1895.730871</td>\n",
       "      <td>1668.950040</td>\n",
       "      <td>431.983576</td>\n",
       "      <td>...</td>\n",
       "      <td>-7171.908624</td>\n",
       "      <td>2129.230440</td>\n",
       "      <td>-889.357733</td>\n",
       "      <td>-216.577726</td>\n",
       "      <td>-649.358176</td>\n",
       "      <td>3039.572508</td>\n",
       "      <td>-12297.899153</td>\n",
       "      <td>-3793.383617</td>\n",
       "      <td>-5603.130273</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>588023 rows Ã— 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 A1           A2           A3           A4           A5  \\\n",
       "0       3549.790315  4533.538497  3619.665186  3077.291188 -1380.325575   \n",
       "1       3551.227812  4534.850995  3622.540181  3077.322438 -1377.575581   \n",
       "2       3556.727802  4539.850986  3629.040169  3081.978679 -1370.419344   \n",
       "3       3557.915300  4541.225983  3628.540169  3083.197427 -1372.263090   \n",
       "4       3553.352808  4535.757243  3622.477681  3079.572434 -1377.763080   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "588018  -623.326974  2269.261431  2575.479615   285.733846   907.388947   \n",
       "588019  -627.420717  2264.448940  2570.323375   281.077605   903.482705   \n",
       "588020  -631.764459  2260.730197  2566.917131   275.546365   902.045207   \n",
       "588021  -625.076971  2265.605188  2573.354619   281.702604   904.982702   \n",
       "588022  -623.639474  2268.636432  2576.229614   286.515095   909.795193   \n",
       "\n",
       "                 A6           A7           A8           A9          A10  ...  \\\n",
       "0       6120.066816 -4072.820600 -2256.511456  1820.012261 -2815.635423  ...   \n",
       "1       6123.066810 -4069.851856 -2252.167714  1825.168502 -2803.072947  ...   \n",
       "2       6130.348047 -4063.508118 -2249.292720  1828.074746 -2804.041695  ...   \n",
       "3       6130.410547 -4062.070620 -2251.667715  1825.856000 -2803.572946  ...   \n",
       "4       6125.598056 -4066.570612 -2255.136459  1821.981008 -2808.041687  ...   \n",
       "...             ...          ...          ...          ...          ...  ...   \n",
       "588018  -491.014719 -2998.447586  1886.043389  1659.637557   416.296105  ...   \n",
       "588019  -490.702219 -3001.260080  1884.387142  1657.012562   414.702358  ...   \n",
       "588020  -493.545964 -3006.103821  1886.199639  1658.512560   424.202340  ...   \n",
       "588021  -490.795969 -3001.416330  1888.387135  1659.418808   420.077348  ...   \n",
       "588022  -484.358481 -2996.353839  1895.730871  1668.950040   431.983576  ...   \n",
       "\n",
       "                D24          D25          D26          D27          D28  \\\n",
       "0      -7240.845997  7034.252627  8458.062496  5905.223463  6147.660515   \n",
       "1      -7227.283522  7039.627617  8463.874985  5911.598451  6153.504254   \n",
       "2      -7227.158522  7048.502600  8473.562467  5921.348433  6163.004236   \n",
       "3      -7224.189777  7042.346362  8464.593734  5917.660940  6160.972990   \n",
       "4      -7219.971035  7044.658857  8466.843729  5914.848445  6156.785498   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "588018 -7176.689865  2116.667963  -901.138961  -227.327706  -657.170662   \n",
       "588019 -7179.502360  2118.074210  -900.607712  -227.046456  -659.389408   \n",
       "588020 -7177.439864  2118.199210  -900.920211  -226.140208  -659.764407   \n",
       "588021 -7172.002374  2119.730457  -898.170216  -224.515211  -656.576913   \n",
       "588022 -7171.908624  2129.230440  -889.357733  -216.577726  -649.358176   \n",
       "\n",
       "                D29           D30          D31          D32  event  \n",
       "0       2458.073582  -7465.876831 -3604.133966 -5445.224315      5  \n",
       "1       2463.354822  -7461.033090 -3594.258985 -5435.693082      5  \n",
       "2       2469.854810  -7460.470591 -3591.540240 -5433.568086      5  \n",
       "3       2467.011066  -7458.158095 -3597.008980 -5437.474329      5  \n",
       "4       2466.948566  -7457.501846 -3585.821500 -5428.630595      5  \n",
       "...             ...           ...          ...          ...    ...  \n",
       "588018  3025.322534 -12313.149124 -3810.071086 -5620.505241     10  \n",
       "588019  3027.760030 -12307.211635 -3809.946086 -5621.098990     10  \n",
       "588020  3027.103781 -12305.774138 -3805.633594 -5614.880251     10  \n",
       "588021  3032.822520 -12303.742892 -3804.133597 -5614.192752     10  \n",
       "588022  3039.572508 -12297.899153 -3793.383617 -5603.130273     10  \n",
       "\n",
       "[588023 rows x 129 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# https://github.com/N-Nieto/Inner_Speech_Dataset\n",
    "\n",
    "# Load the Inner Speech Dataset\n",
    "# =============================\n",
    "# This dataset comprises raw EEG data collected from subject 'sub-01' during session 'ses-01'.\n",
    "# Source: https://github.com/N-Nieto/Inner_Speech_Dataset\n",
    "#\n",
    "# Overview:\n",
    "# - The dataset is part of a study on inner speech, capturing brain activity via EEG.\n",
    "# - Each row in the dataset corresponds to a timestamp of EEG readings.\n",
    "# - Columns represent various EEG channels (electrodes placed on the scalp).\n",
    "#\n",
    "# Usage:\n",
    "# - The data is primarily used for cognitive neuroscience research, focusing on the neural correlates of inner speech.\n",
    "# - Users can analyze EEG signals to investigate brain activity patterns associated with the cognitive processes of inner speech.\n",
    "#\n",
    "# File Structure:\n",
    "# - Located at '../data/RAW_EEG/sub-01/sub-01_ses-01.csv' relative to this script.\n",
    "# - It is advisable to preprocess the data (filtering, normalization) before detailed analysis.\n",
    "#\n",
    "# Example:\n",
    "# - To load this data into a DataFrame for analysis and processing, use the following code snippet.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = None\n",
    "for csv in [\"../data/RAW_EEG/sub-01/sub-01_ses-01.csv\", \"../data/RAW_EEG/sub-01/sub-01_ses-02.csv\", \"../data/RAW_EEG/sub-01/sub-01_ses-03.csv\"]:\n",
    "    tmp_df = pd.read_csv(path_append + csv)\n",
    "    if df is None:\n",
    "        df = tmp_df\n",
    "    else:\n",
    "        df = pd.concat([df, tmp_df])\n",
    "df = df.reset_index(drop=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of each class in the 'event' column:\n",
      "event\n",
      "1     57650\n",
      "0     57650\n",
      "3     57650\n",
      "2     57650\n",
      "13    57650\n",
      "12    57650\n",
      "11    57650\n",
      "10    57650\n",
      "6     28825\n",
      "9     28825\n",
      "8     28825\n",
      "7     28825\n",
      "5     11523\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Maximum class number:\n",
      "13\n",
      "\n",
      "Expected number of classes (from num_classes variable): 14\n",
      "\n",
      "DataFrame with reset class labels:\n",
      "                 A1           A2           A3           A4           A5  \\\n",
      "0       3549.790315  4533.538497  3619.665186  3077.291188 -1380.325575   \n",
      "1       3551.227812  4534.850995  3622.540181  3077.322438 -1377.575581   \n",
      "2       3556.727802  4539.850986  3629.040169  3081.978679 -1370.419344   \n",
      "3       3557.915300  4541.225983  3628.540169  3083.197427 -1372.263090   \n",
      "4       3553.352808  4535.757243  3622.477681  3079.572434 -1377.763080   \n",
      "...             ...          ...          ...          ...          ...   \n",
      "588018  -623.326974  2269.261431  2575.479615   285.733846   907.388947   \n",
      "588019  -627.420717  2264.448940  2570.323375   281.077605   903.482705   \n",
      "588020  -631.764459  2260.730197  2566.917131   275.546365   902.045207   \n",
      "588021  -625.076971  2265.605188  2573.354619   281.702604   904.982702   \n",
      "588022  -623.639474  2268.636432  2576.229614   286.515095   909.795193   \n",
      "\n",
      "                 A6           A7           A8           A9          A10  ...  \\\n",
      "0       6120.066816 -4072.820600 -2256.511456  1820.012261 -2815.635423  ...   \n",
      "1       6123.066810 -4069.851856 -2252.167714  1825.168502 -2803.072947  ...   \n",
      "2       6130.348047 -4063.508118 -2249.292720  1828.074746 -2804.041695  ...   \n",
      "3       6130.410547 -4062.070620 -2251.667715  1825.856000 -2803.572946  ...   \n",
      "4       6125.598056 -4066.570612 -2255.136459  1821.981008 -2808.041687  ...   \n",
      "...             ...          ...          ...          ...          ...  ...   \n",
      "588018  -491.014719 -2998.447586  1886.043389  1659.637557   416.296105  ...   \n",
      "588019  -490.702219 -3001.260080  1884.387142  1657.012562   414.702358  ...   \n",
      "588020  -493.545964 -3006.103821  1886.199639  1658.512560   424.202340  ...   \n",
      "588021  -490.795969 -3001.416330  1888.387135  1659.418808   420.077348  ...   \n",
      "588022  -484.358481 -2996.353839  1895.730871  1668.950040   431.983576  ...   \n",
      "\n",
      "                D24          D25          D26          D27          D28  \\\n",
      "0      -7240.845997  7034.252627  8458.062496  5905.223463  6147.660515   \n",
      "1      -7227.283522  7039.627617  8463.874985  5911.598451  6153.504254   \n",
      "2      -7227.158522  7048.502600  8473.562467  5921.348433  6163.004236   \n",
      "3      -7224.189777  7042.346362  8464.593734  5917.660940  6160.972990   \n",
      "4      -7219.971035  7044.658857  8466.843729  5914.848445  6156.785498   \n",
      "...             ...          ...          ...          ...          ...   \n",
      "588018 -7176.689865  2116.667963  -901.138961  -227.327706  -657.170662   \n",
      "588019 -7179.502360  2118.074210  -900.607712  -227.046456  -659.389408   \n",
      "588020 -7177.439864  2118.199210  -900.920211  -226.140208  -659.764407   \n",
      "588021 -7172.002374  2119.730457  -898.170216  -224.515211  -656.576913   \n",
      "588022 -7171.908624  2129.230440  -889.357733  -216.577726  -649.358176   \n",
      "\n",
      "                D29           D30          D31          D32  event  \n",
      "0       2458.073582  -7465.876831 -3604.133966 -5445.224315      4  \n",
      "1       2463.354822  -7461.033090 -3594.258985 -5435.693082      4  \n",
      "2       2469.854810  -7460.470591 -3591.540240 -5433.568086      4  \n",
      "3       2467.011066  -7458.158095 -3597.008980 -5437.474329      4  \n",
      "4       2466.948566  -7457.501846 -3585.821500 -5428.630595      4  \n",
      "...             ...           ...          ...          ...    ...  \n",
      "588018  3025.322534 -12313.149124 -3810.071086 -5620.505241      9  \n",
      "588019  3027.760030 -12307.211635 -3809.946086 -5621.098990      9  \n",
      "588020  3027.103781 -12305.774138 -3805.633594 -5614.880251      9  \n",
      "588021  3032.822520 -12303.742892 -3804.133597 -5614.192752      9  \n",
      "588022  3039.572508 -12297.899153 -3793.383617 -5603.130273      9  \n",
      "\n",
      "[588023 rows x 129 columns]\n",
      "\n",
      "New counts of each class in the 'event' column:\n",
      "event\n",
      "1     57650\n",
      "0     57650\n",
      "3     57650\n",
      "2     57650\n",
      "12    57650\n",
      "11    57650\n",
      "10    57650\n",
      "9     57650\n",
      "5     28825\n",
      "8     28825\n",
      "7     28825\n",
      "6     28825\n",
      "4     11523\n",
      "Name: count, dtype: int64\n",
      "\n",
      "New maximum class number:\n",
      "12\n",
      "\n",
      "New expected number of classes (from num_classes variable): 13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example setup, assuming df is defined as a DataFrame\n",
    "# Assuming df['event'] contains the class labels\n",
    "\n",
    "# Print counts of each class in the 'event' column\n",
    "event_counts = df['event'].value_counts()\n",
    "print(\"Counts of each class in the 'event' column:\")\n",
    "print(event_counts)\n",
    "\n",
    "# Print the maximum class number\n",
    "max_class_number = df['event'].max()\n",
    "print(\"\\nMaximum class number:\")\n",
    "print(max_class_number)\n",
    "\n",
    "# Determine the number of classes\n",
    "print(\"\\nExpected number of classes (from num_classes variable):\", max_class_number + 1)\n",
    "\n",
    "# Reset the class labels to be in the range [0, num_classes-1]\n",
    "unique_classes = sorted(df['event'].unique())\n",
    "class_mapping = {old_class: new_class for new_class, old_class in enumerate(unique_classes)}\n",
    "\n",
    "df['event'] = df['event'].map(class_mapping)\n",
    "print(\"\\nDataFrame with reset class labels:\")\n",
    "print(df)\n",
    "\n",
    "# Verify the new counts and max class number\n",
    "new_event_counts = df['event'].value_counts()\n",
    "new_max_class_number = df['event'].max()\n",
    "num_classes = new_max_class_number + 1\n",
    "\n",
    "print(\"\\nNew counts of each class in the 'event' column:\")\n",
    "print(new_event_counts)\n",
    "print(\"\\nNew maximum class number:\")\n",
    "print(new_max_class_number)\n",
    "print(\"\\nNew expected number of classes (from num_classes variable):\", new_max_class_number + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices where the 'event' label changes: [0, 3841, 4994, 6147, 8453, 9606, 10759, 11912, 13065, 14218, 15371, 17677, 18830, 19983, 21136, 23442, 24595, 25748, 26901, 28054, 29207, 30360, 31513, 33819, 34972, 36125, 38431, 39584, 40737, 41890, 45349, 46502, 48808, 49961, 52267, 55726, 56879, 58032, 60338, 61491, 62644, 63797, 66103, 67256, 68409, 69562, 70715, 71868, 73021, 74174, 75327, 77633, 78786, 81092, 82245, 83398, 85704, 86857, 88010, 89163, 90316, 91469, 92622, 94928, 96081, 98387, 101846, 102999, 104152, 106458, 107611, 108764, 109917, 112223, 113376, 114529, 115682, 116835, 117988, 119141, 120294, 121447, 123753, 124906, 127212, 128365, 129518, 131824, 132977, 134130, 135283, 136436, 137589, 138742, 141048, 142201, 143354, 144507, 146813, 149119, 150272, 151425, 152578, 153731, 157190, 158343, 160649, 161802, 162955, 164108, 165261, 166414, 168720, 169873, 172179, 173332, 175638, 176791, 179097, 180250, 181403, 182556, 183709, 184862, 186015, 187168, 188321, 190627, 192933, 194086, 195239, 196392, 197545, 198698, 199851, 202157, 203310, 204463, 205616, 207922, 209075, 210228, 211381, 214840, 215993, 217146, 218299, 219452, 220605, 221758, 222911, 224064, 225217, 226370, 228676, 229829, 230982, 232135, 233288, 234441, 238282, 239435, 240588, 241741, 245200, 246353, 248659, 250965, 252118, 253271, 254424, 255577, 256730, 257883, 259036, 260189, 261342, 264801, 265954, 268260, 269413, 270566, 271719, 274025, 275178, 276331, 277484, 278637, 279790, 280943, 282096, 284402, 286708, 289014, 290167, 292473, 293626, 295932, 297085, 298238, 299391, 300544, 301697, 305156, 306309, 309768, 310921, 312074, 313227, 314380, 315533, 316686, 317839, 318992, 320145, 322451, 323604, 325910, 327063, 328216, 330522, 332828, 335134, 336287, 338593, 339746, 342052, 343205, 344358, 345511, 346664, 347817, 351276, 352429, 355888, 357041, 358194, 359347, 360500, 361653, 362806, 363959, 365112, 366265, 368571, 369724, 372030, 373183, 374336, 376642, 377795, 378948, 380101, 381254, 382407, 383560, 384713, 385866, 387019, 388172, 389325, 390478, 391631, 392784, 393937, 395090, 396243, 398549, 399702, 402008, 403161, 404314, 405467, 406620, 407773, 408926, 411232, 412385, 414691, 415844, 416997, 418150, 419303, 420456, 421609, 422762, 423915, 425068, 426221, 427374, 428527, 429680, 430833, 433139, 434292, 435445, 436598, 437751, 438904, 441210, 442363, 443516, 444669, 445822, 446975, 448128, 449281, 450434, 452740, 453893, 455046, 456199, 457352, 458505, 459658, 460811, 461964, 464270, 466576, 467729, 468882, 472723, 473876, 475029, 476182, 479641, 480794, 481947, 484253, 485406, 486559, 487712, 488865, 491171, 492324, 493477, 494630, 495783, 498089, 499242, 500395, 502701, 505007, 506160, 507313, 508466, 509619, 510772, 511925, 516537, 518843, 521149, 522302, 523455, 525761, 528067, 529220, 530373, 531526, 532679, 533832, 534985, 539597, 541903, 543056, 544209, 545362, 546515, 549974, 551127, 552280, 553433, 554586, 555739, 556892, 558045, 559198, 560351, 561504, 562657, 563810, 564963, 568422, 569575, 570728, 571881, 574187, 575340, 576493, 577646, 578799, 579952, 581105, 582258, 583411, 584564, 585717]\n",
      "Lengths between changes: [3841, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 2306, 1153, 1153, 1153, 3459, 1153, 2306, 1153, 2306, 3459, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 3459, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 3459, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 3459, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 3841, 1153, 1153, 1153, 3459, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 3459, 1153, 2306, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 2306, 2306, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 3459, 1153, 3459, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 2306, 2306, 2306, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 3459, 1153, 3459, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 2306, 2306, 1153, 1153, 3841, 1153, 1153, 1153, 3459, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 2306, 1153, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 4612, 2306, 2306, 1153, 1153, 2306, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 4612, 2306, 1153, 1153, 1153, 1153, 3459, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 3459, 1153, 1153, 1153, 2306, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153]\n",
      "Minimum cycle length: 1153\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is defined and already includes an 'event' column\n",
    "# Assuming 'event' column contains class labels\n",
    "event_changes = df['event'].diff().ne(0)\n",
    "change_indices = event_changes[event_changes].index.tolist()\n",
    "\n",
    "# Calculate and print lengths between changes\n",
    "lengths_between_changes = [change_indices[i] - change_indices[i-1] for i in range(1, len(change_indices))]\n",
    "\n",
    "# Find the minimum cycle length where the label changes\n",
    "min_cycle_length = min(lengths_between_changes)\n",
    "\n",
    "print(\"Indices where the 'event' label changes:\", change_indices)\n",
    "print(\"Lengths between changes:\", lengths_between_changes)\n",
    "print(f\"Minimum cycle length: {min_cycle_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_pairs = []\n",
    "for start, end in zip(change_indices[:-1], change_indices[1:]):\n",
    "    segment_length = end - start\n",
    "    if segment_length >= min_cycle_length and segment_length % min_cycle_length == 0:\n",
    "        # Normalize each sub-segment within the main segment\n",
    "        for offset in range(0, segment_length, min_cycle_length):\n",
    "            sub_start = start + offset\n",
    "            sub_end = sub_start + min_cycle_length\n",
    "            segment_pairs.append((sub_start, sub_end))\n",
    "    else:\n",
    "        irregular_num = segment_length//min_cycle_length\n",
    "        # Normalize each sub-segment within the main segment\n",
    "        for i in range(irregular_num):\n",
    "            sub_start = start + i * min_cycle_length\n",
    "            if i == irregular_num - 1:\n",
    "                sub_end = end\n",
    "            else:\n",
    "                sub_end = sub_start + min_cycle_length\n",
    "            segment_pairs.append((sub_start, sub_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "seq_lengths = []#234440, 234440, 119140\n",
    "tmp = df.event.diff(1).dropna()\n",
    "seq_lengths = [0] + tmp[tmp!=0].index.to_list()\n",
    "seq_lengths = torch.tensor(seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\junho\\AppData\\Local\\Temp\\ipykernel_54052\\2692472686.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_scaled = torch.tensor(x - x.mean(dim=0), dtype=torch.float64).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_x shape: (585717, 128)\n",
      "df_y shape: (585717, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def process_dataframe(df, segment_pairs, use_scale = False, include_diff=False, window_size=1):\n",
    "    \"\"\"\n",
    "    Process the DataFrame by applying standard scaling and calculating differences, \n",
    "    returning the final DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - segment_pairs (list of tuples): List of (start, end) index pairs.\n",
    "    - window_size (int): The window size for difference calculation.\n",
    "    - include_diff (bool): Whether to include difference calculations in the final DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - final_df (pd.DataFrame): The processed DataFrame.\n",
    "    - new_segment_pairs (list of tuples): Updated segment pairs after processing.\n",
    "    \"\"\"\n",
    "    df_tensor = torch.tensor(df.values, dtype=torch.float64).cuda()\n",
    "    window_size = window_size if include_diff else 0\n",
    "    \n",
    "    df_list_diff = []\n",
    "    df_list_y = []\n",
    "    df_list_x = []\n",
    "    cur_idx = 0\n",
    "    new_segment_pairs = []\n",
    "\n",
    "    for i in range(len(segment_pairs)):\n",
    "        start, end = segment_pairs[i]\n",
    "\n",
    "        # Extract segments and apply scaling\n",
    "        x = df_tensor[start + window_size:end - window_size, :-1]\n",
    "        y = df_tensor[start + window_size:end - window_size, -1:]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        if use_scale:\n",
    "            x_scaled = torch.tensor(x - x.mean(dim=0), dtype=torch.float64).cuda()\n",
    "        else:\n",
    "            x_scaled = x\n",
    "\n",
    "        if include_diff:\n",
    "            diff = df_tensor[start + 2 * window_size:end, :-1] - df_tensor[start:end - 2 * window_size, :-1]\n",
    "            diff_np = diff.cpu().numpy()\n",
    "            diff_scaled = torch.tensor(scaler.fit_transform(diff_np), dtype=torch.float64).cuda()\n",
    "            df_list_diff.append(diff_scaled)\n",
    "\n",
    "        add_len = (end - start - 2 * window_size)\n",
    "        new_segment_pairs.append((cur_idx, cur_idx + add_len))\n",
    "        cur_idx += add_len\n",
    "\n",
    "        df_list_y.append(y)\n",
    "        df_list_x.append(x_scaled)\n",
    "\n",
    "    df_y_tensor = torch.cat(df_list_y, dim=0)\n",
    "    df_x_tensor = torch.cat(df_list_x, dim=0)\n",
    "\n",
    "    df_y = pd.DataFrame(df_y_tensor.cpu().numpy())\n",
    "    df_x = pd.DataFrame(df_x_tensor.cpu().numpy())\n",
    "\n",
    "    if include_diff:\n",
    "        df_diff_tensor = torch.cat(df_list_diff, dim=0)\n",
    "        df_diff = pd.DataFrame(df_diff_tensor.cpu().numpy())\n",
    "        new_column_names = ['diff_' + name for name in df.columns[:-1]]\n",
    "        df_diff.columns = new_column_names\n",
    "\n",
    "    print(\"df_x shape:\", df_x.shape)\n",
    "    if include_diff:\n",
    "        print(\"df_diff shape:\", df_diff.shape)\n",
    "    print(\"df_y shape:\", df_y.shape)\n",
    "\n",
    "    if include_diff:\n",
    "        final_df = pd.concat([df_x, df_diff, df_y], axis=1)\n",
    "        final_column_names = list(df.columns[:-1]) + new_column_names + [df.columns[-1]]\n",
    "    else:\n",
    "        final_df = pd.concat([df_x, df_y], axis=1)\n",
    "        final_column_names = list(df.columns[:-1]) + [df.columns[-1]]\n",
    "\n",
    "    final_df.columns = final_column_names\n",
    "\n",
    "    return final_df, new_segment_pairs\n",
    "\n",
    "# Example usage\n",
    "window_size = 1\n",
    "\n",
    "# Assuming df and segment_pairs are defined appropriately\n",
    "df, segment_pairs = process_dataframe(df, segment_pairs, use_scale=True, include_diff=False, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = df.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(          A1         A2         A3         A4        A5         A6         A7  \\\n",
       " 0 -16.853855 -17.158468 -18.909440 -14.939126 -5.522919 -10.424824  -0.457257   \n",
       " 1 -15.416358 -15.845970 -16.034446 -14.907876 -2.772924  -7.424829   2.511487   \n",
       " 2  -9.916368 -10.845980  -9.534458 -10.251634  4.383313  -0.143593   8.855225   \n",
       " 3  -8.728870  -9.470982 -10.034457  -9.032887  2.539566  -0.081093  10.292723   \n",
       " 4 -13.291362 -14.939722 -16.096946 -12.657880 -2.960424  -4.893584   5.792731   \n",
       " \n",
       "          A8        A9        A10  ...        D24        D25        D26  \\\n",
       " 0  0.617003  0.393158 -20.814277  ... -53.114494 -14.202435  -5.211260   \n",
       " 1  4.960745  5.549399  -8.251801  ... -39.552019  -8.827445   0.601229   \n",
       " 2  7.835740  8.455644  -9.220549  ... -39.427020   0.047539  10.288711   \n",
       " 3  5.460744  6.236898  -8.751800  ... -36.458275  -6.108700   1.319978   \n",
       " 4  1.992001  2.361905 -13.220541  ... -32.239533  -3.796204   3.569974   \n",
       " \n",
       "          D27        D28        D29        D30        D31        D32  event  \n",
       " 0 -13.364242 -13.734146 -22.270828 -16.534336 -50.263215 -48.822465    4.0  \n",
       " 1  -6.989254  -7.890407 -16.989588 -11.690595 -40.388233 -39.291233    4.0  \n",
       " 2   2.760728   1.609575 -10.489600 -11.128096 -37.669488 -37.166237    4.0  \n",
       " 3  -0.926765  -0.421671 -13.333345  -8.815601 -43.138228 -41.072479    4.0  \n",
       " 4  -3.739260  -4.609163 -13.395845  -8.159352 -31.950749 -32.228746    4.0  \n",
       " \n",
       " [5 rows x 129 columns],\n",
       "               A1        A2        A3        A4         A5         A6  \\\n",
       " 585712 -2.640010 -3.232160 -1.678065  1.999346  17.060354 -13.720242   \n",
       " 585713 -7.765001 -6.544654 -5.834307 -3.719394   8.435370 -20.345230   \n",
       " 585714 -6.671253 -7.388402 -6.709306 -5.094391  -0.533363 -27.063968   \n",
       " 585715 -4.983756 -5.107157 -5.334308 -1.656897   5.685375 -23.845224   \n",
       " 585716  0.328734  0.080334 -0.990566  1.343097  11.185365 -17.938985   \n",
       " \n",
       "                A7        A8         A9        A10  ...        D24        D25  \\\n",
       " 585712  10.319772  8.351378   8.008116 -25.004778  ...  26.349610 -18.978960   \n",
       " 585713   1.382288  0.257643   0.633130 -28.036023  ...  15.193380 -27.885193   \n",
       " 585714  -7.773945 -8.961090 -10.335600 -42.192246  ...   9.255891 -36.791427   \n",
       " 585715  -5.211450 -1.867353  -2.210615 -32.411014  ...  12.380886 -29.885190   \n",
       " 585716  -0.180209  4.257635   3.976874 -24.536029  ...  22.443367 -24.447700   \n",
       " \n",
       "               D26        D27        D28        D29        D30        D31  \\\n",
       " 585712 -23.324626 -11.940513 -17.009563 -16.428580 -33.588213 -22.112763   \n",
       " 585713 -29.980864 -18.003002 -24.353300 -24.053566 -45.775691 -29.487749   \n",
       " 585714 -37.168351 -24.471740 -30.634538 -30.553554 -53.181927 -41.237727   \n",
       " 585715 -33.418358 -21.659246 -30.040789 -27.272310 -45.056942 -32.550243   \n",
       " 585716 -27.137119 -17.221754 -24.040800 -21.772320 -38.838203 -23.425260   \n",
       " \n",
       "               D32  event  \n",
       " 585712 -21.697732   10.0  \n",
       " 585713 -29.478968   10.0  \n",
       " 585714 -41.666445   10.0  \n",
       " 585715 -32.291462   10.0  \n",
       " 585716 -24.478977   10.0  \n",
       " \n",
       " [5 rows x 129 columns])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5], df[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class EEG_Dataset(Dataset):\n",
    "    def __init__(self, df, indices, max_window_size, num_classes, normalize = False):\n",
    "        self.df = df\n",
    "        self.indices = indices  # List of start indices\n",
    "        self.max_window_size = max_window_size\n",
    "        self.min_window_size = max_window_size // 2\n",
    "        self.num_classes = num_classes  # Added num_classes as an instance variable\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = self.indices[idx]\n",
    "        # Randomly choose a window size between min_window_size and max_window_size\n",
    "        window_size = random.randint(self.min_window_size, self.max_window_size)\n",
    "        \n",
    "        end_idx = min(start_idx + window_size, len(self.df))\n",
    "\n",
    "        # Retrieve the sequence using the calculated indices\n",
    "        seq = self.df.iloc[start_idx:end_idx]\n",
    "        X, y = seq.values[:, :-1], seq.values[:, -1]\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "        # Normalize X\n",
    "        if self.normalize:\n",
    "            X = (X - X.mean(dim = -1, keepdim = True)) / (X.std(dim = -1, keepdim = True) + 1e-8)\n",
    "        \n",
    "        y = torch.tensor(y, dtype=torch.long)  # Ensure y is a tensor of type long\n",
    "        y = torch.nn.functional.one_hot(y, num_classes=self.num_classes)  # Correct use with instance variable\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# Assume 'df' is your DataFrame and 'event' is the column containing labels\n",
    "\n",
    "# Generate indices without mixing segments\n",
    "def generate_indices(input_df, input_pairs, max_window_size, test_size=0.2):\n",
    "    length = len(input_pairs)\n",
    "    train_length = int(length * (1- test_size))\n",
    "    training_indices = []\n",
    "    testing_indices = []\n",
    "    for iter, (start, end) in enumerate(input_pairs):\n",
    "        indices = training_indices if iter < train_length else testing_indices\n",
    "        max_index = end - max_window_size  # Calculate the maximum starting index for this segment\n",
    "        for i in range(start, max_index):\n",
    "            # Check if all labels in the window are the same\n",
    "            if len(input_df['event'][i:i + max_window_size].unique()) == 1:\n",
    "                indices.append(i)\n",
    "            else:\n",
    "                print(f\"Skipping index {i} due to multiple labels in window.\")\n",
    "    return training_indices, testing_indices\n",
    "\n",
    "# Example usage\n",
    "max_window_size = 128\n",
    "shuffle(segment_pairs)  # Shuffle the indices to randomize the data order\n",
    "train_indices, test_indices = generate_indices(df, segment_pairs, max_window_size)\n",
    "\n",
    "shuffle(train_indices)  # Shuffle the indices to randomize the data order\n",
    "shuffle(test_indices)  # Shuffle the indices to randomize the data order\n",
    "\n",
    "# Assuming you have an EEG_Dataset class defined as before\n",
    "trainset = EEG_Dataset(df=df, indices=train_indices, max_window_size=max_window_size, num_classes = num_classes, normalize=False)\n",
    "testset = EEG_Dataset(df=df, indices=test_indices, max_window_size=max_window_size, num_classes = num_classes, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of overlapping windows: 0\n"
     ]
    }
   ],
   "source": [
    "def check_overlap(train_indices, test_indices, window_size, trials = None):\n",
    "    # Generate train and test window ranges\n",
    "    train_windows = [(idx, idx + window_size) for idx in train_indices]\n",
    "    test_windows = [(idx, idx + window_size) for idx in test_indices]\n",
    "    \n",
    "    # Sort train and test windows by their start indices\n",
    "    train_windows.sort()\n",
    "    test_windows.sort()\n",
    "\n",
    "    overlaps = []\n",
    "    train_idx = 0\n",
    "    trials = len(test_windows) if trials is None else trials\n",
    "\n",
    "    # Check for overlapping windows\n",
    "    for idx, (t_start, t_end) in enumerate(test_windows):\n",
    "        if idx > trials:\n",
    "            break\n",
    "        # Advance the train_idx to the relevant window range\n",
    "        while train_idx < len(train_windows) and train_windows[train_idx][1] <= t_start:\n",
    "            train_idx += 1\n",
    "\n",
    "        # Check if the current test window overlaps with any train window\n",
    "        for tr_start, tr_end in train_windows[train_idx:]:\n",
    "            if tr_start < t_end and tr_end > t_start:  # Overlapping condition\n",
    "                overlaps.append((tr_start, tr_end, t_start, t_end))\n",
    "                print (f\"Overlap found between train window {tr_start}-{tr_end} and test window {t_start}-{t_end}\")\n",
    "                break  # No need to check further once an overlap is found\n",
    "\n",
    "    return overlaps\n",
    "\n",
    "# Check for overlaps\n",
    "overlaps = check_overlap(train_indices, test_indices, max_window_size, trials = 100)\n",
    "print(\"Number of overlapping windows:\", len(overlaps))\n",
    "if overlaps:\n",
    "    print(\"Example of overlapping windows:\", overlaps[:5])  # Print the first 5 overlapping windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\junho\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\junho\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from tools.setting.data_config import DataConfig\n",
    "from tools.setting.ml_params import MLParameters\n",
    "from trainer_hub import TrainerHub\n",
    "import torch\n",
    "\n",
    "data_config = DataConfig(dataset_name = 'eeg-sub-01', task_type='multi_class_classification', obs_shape=[num_features], label_size=num_classes)\n",
    "\n",
    "#  Set training configuration from the AlgorithmConfig class, returning them as a Namespace object.\n",
    "ml_params = MLParameters(core_model = 'gpt', encoder_model = 'none')\n",
    "\n",
    "# Set the device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Initialize the TrainerHub class with the training configuration, data configuration, device, and use_print and use_wandb flags\n",
    "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd43c67d93b04a66834aa4589ce847a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239017a37b934806bfbcc424e5e129fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/6504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100][50/6504][Time 25.74]\n",
      "Unified LR across all optimizers: 0.0001995308238189185\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.3937\tGen: 13.2188\tRec: 13.1591\tE: 0.4522\tR: 0.3328\tP: 25.9977\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.1211\n",
      "precision: 0.0778\n",
      "recall: 0.1006\n",
      "f1_score: 0.0790\n",
      "\n",
      "[0/100][100/6504][Time 24.54]\n",
      "Unified LR across all optimizers: 0.00019907191565870155\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1707\tGen: 12.4107\tRec: 12.3955\tE: 0.1862\tR: 0.1554\tP: 24.7240\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.1211\n",
      "precision: 0.1391\n",
      "recall: 0.1229\n",
      "f1_score: 0.1042\n",
      "\n",
      "[0/100][150/6504][Time 24.19]\n",
      "Unified LR across all optimizers: 0.00019861406295796434\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1915\tGen: 11.8921\tRec: 11.8763\tE: 0.2071\tR: 0.1756\tP: 23.5603\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.1797\n",
      "precision: 0.1585\n",
      "recall: 0.1472\n",
      "f1_score: 0.1344\n",
      "\n",
      "[0/100][200/6504][Time 27.18]\n",
      "Unified LR across all optimizers: 0.00019815726328921765\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.1932\tGen: 11.6085\tRec: 11.5973\tE: 0.2034\tR: 0.1810\tP: 22.9554\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.1875\n",
      "precision: 0.1328\n",
      "recall: 0.1795\n",
      "f1_score: 0.1338\n",
      "\n",
      "[0/100][250/6504][Time 27.96]\n",
      "Unified LR across all optimizers: 0.00019770151423055492\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.2007\tGen: 11.1115\tRec: 11.1010\tE: 0.2101\tR: 0.1892\tP: 21.9595\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.2305\n",
      "precision: 0.1670\n",
      "recall: 0.1673\n",
      "f1_score: 0.1561\n",
      "\n",
      "[0/100][300/6504][Time 27.57]\n",
      "Unified LR across all optimizers: 0.00019724681336564005\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.2033\tGen: 10.8094\tRec: 10.8009\tE: 0.2108\tR: 0.1931\tP: 21.3669\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.2891\n",
      "precision: 0.2426\n",
      "recall: 0.2200\n",
      "f1_score: 0.2100\n",
      "\n",
      "[0/100][350/6504][Time 24.23]\n",
      "Unified LR across all optimizers: 0.00019679315828369438\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.2057\tGen: 10.2663\tRec: 10.2599\tE: 0.2097\tR: 0.1972\tP: 20.2707\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.2852\n",
      "precision: 0.2462\n",
      "recall: 0.2227\n",
      "f1_score: 0.1923\n",
      "\n",
      "[0/100][400/6504][Time 24.06]\n",
      "Unified LR across all optimizers: 0.00019634054657948372\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.2012\tGen: 10.1416\tRec: 10.1357\tE: 0.2006\tR: 0.1888\tP: 20.1214\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.2852\n",
      "precision: 0.1879\n",
      "recall: 0.2435\n",
      "f1_score: 0.2049\n",
      "\n",
      "[0/100][450/6504][Time 26.12]\n",
      "Unified LR across all optimizers: 0.00019588897585330582\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.2095\tGen: 10.0257\tRec: 10.0204\tE: 0.2084\tR: 0.1980\tP: 19.8736\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.3008\n",
      "precision: 0.2243\n",
      "recall: 0.2508\n",
      "f1_score: 0.2263\n",
      "\n",
      "[0/100][500/6504][Time 24.12]\n",
      "Unified LR across all optimizers: 0.00019543844371097777\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.2204\tGen: 9.7950\tRec: 9.7898\tE: 0.2223\tR: 0.2117\tP: 19.3835\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.3008\n",
      "precision: 0.1949\n",
      "recall: 0.2213\n",
      "f1_score: 0.1904\n",
      "\n",
      "[0/100][550/6504][Time 24.16]\n",
      "Unified LR across all optimizers: 0.00019498894776382288\n",
      "--------------------Training Metrics--------------------\n",
      "Trainer:  gpt\n",
      "Inf: 0.2286\tGen: 9.6123\tRec: 9.6066\tE: 0.2273\tR: 0.2154\tP: 18.9870\n",
      "--------------------Test Metrics------------------------\n",
      "accuracy: 0.3438\n",
      "precision: 0.2729\n",
      "recall: 0.2744\n",
      "f1_score: 0.2644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer_hub.train(trainset, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_hub.test(testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
