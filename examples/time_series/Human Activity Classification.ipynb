{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Activity Classification Dataset\n",
    "https://www.kaggle.com/datasets/rabieelkharoua/human-activity-classification-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "path_append = \"../../\"\n",
    "sys.path.append(path_append)  # Go up one directory from where you are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_ids = range(1600, 1651)  # Subject_id: 1600 ~ 1650\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "for device in ['watch', 'phone']:\n",
    "    for sensor in ['accel', 'gyro']:\n",
    "        dfs[f'{device}_{sensor}'] = pd.DataFrame()\n",
    "        \n",
    "        for file_id in tqdm(file_ids, desc=f'Processing {device}/{sensor}'):\n",
    "            file_path = f'../../data/Human Activity Classification Dataset/wisdm-dataset/wisdm-dataset/raw/{device}/{sensor}/data_{file_id}_{sensor}_{device}.txt'\n",
    "            try:\n",
    "                data = pd.read_csv(file_path, sep=\",\", lineterminator=\";\", header=None, on_bad_lines='skip')\n",
    "\n",
    "                # add column names\n",
    "                if len(data.columns) < 6:\n",
    "                    additional_cols = {i: None for i in range(len(data.columns), 6)}\n",
    "                    data = data.assign(**additional_cols)\n",
    "                data.columns = ['Subject_id', 'Activity_code', 'Timestamp', 'x', 'y', 'z']\n",
    "\n",
    "                \n",
    "                data['device'] = device\n",
    "                data['sensor'] = sensor\n",
    "                dfs[f'{device}_{sensor}'] = pd.concat([dfs[f'{device}_{sensor}'], data], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "# concat all dataframes\n",
    "df = pd.concat(dfs.values(), ignore_index=True)\n",
    "\n",
    "print(f\"Final merged dataframe has {len(df)} records:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Subject_id'] = df['Subject_id'].str.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Subject_id', 'Activity_code', 'Timestamp', 'x', 'y', 'z'])\n",
    "\n",
    "min_trial_length = 3\n",
    "\n",
    "segment_pairs = []\n",
    "\n",
    "grouped = df.groupby(['Subject_id', 'device', 'sensor'])\n",
    "\n",
    "for name, group in grouped:\n",
    "    start_indices = group.index[0:-min_trial_length+1]\n",
    "    for start in start_indices:\n",
    "        end = start + min_trial_length\n",
    "        if end <= group.index[-1]:\n",
    "            segment_pairs.append((start, end))\n",
    "\n",
    "print(\"Segment Pairs:\", segment_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.preprocessing.data_frame import auto_preprocess_dataframe\n",
    "\n",
    "target_columns = ['Activity_code']\n",
    "drop_columns = ['Subject_id']\n",
    "encode_columns = ['device',\t'sensor']\n",
    "\n",
    "df, description = auto_preprocess_dataframe(df, target_columns, drop_columns, encode_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from tools.preprocessing.template_dataset import TemplateDataset\n",
    "\n",
    "len_segment_pairs = len(segment_pairs)\n",
    "shuffle(segment_pairs)  # Shuffle the indices to randomize the data order\n",
    "\n",
    "min_window_size = 64\n",
    "max_window_size = 128\n",
    "\n",
    "def generate_indices(input_df, input_pairs, max_window_size):\n",
    "    indices = []\n",
    "    for iter, (start, end) in enumerate(input_pairs):\n",
    "        max_index = end - max_window_size  # Calculate the maximum starting index for this segment\n",
    "        for i in range(start, max_index):\n",
    "            # Check if all labels in the window are the same\n",
    "            if len(input_df['event'][i:i + max_window_size].unique()) == 1:\n",
    "                indices.append(i)\n",
    "            else:\n",
    "                print(f\"Skipping index {i} due to multiple labels in window.\")\n",
    "    return indices\n",
    "\n",
    "train_indices = generate_indices(df, segment_pairs[:int(0.8*len_segment_pairs)], max_window_size)\n",
    "test_indices = generate_indices(df, segment_pairs[int(0.8*len_segment_pairs):], max_window_size)\n",
    "\n",
    "shuffle(train_indices)\n",
    "shuffle(test_indices)\n",
    "\n",
    "\n",
    "# predict the next value in the sequence\n",
    "df_x = df.iloc[:, :-1] # all columns except the last one\n",
    "df_y = df.iloc[:, -1:] # only the last column\n",
    "\n",
    "# Assuming you have an EEG_Dataset class defined as before\n",
    "trainset = TemplateDataset(df_x, df_y, indices = train_indices, min_window_size =min_window_size, max_window_size = max_window_size)\n",
    "testset = TemplateDataset(df_x, df_y, indices = test_indices, min_window_size =max_window_size, max_window_size = max_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.setting.data_config import DataConfig\n",
    "from tools.setting.ml_params import MLParameters\n",
    "from trainer_hub import TrainerHub\n",
    "\n",
    "num_features = description['num_features']\n",
    "num_classes = description['num_classes']\n",
    "data_config = DataConfig(dataset_name = 'Human_Activity_Classification', task_type='multi_class_classification', obs_shape=[num_features], label_size=num_classes)\n",
    "\n",
    "#  Set training configuration from the AlgorithmConfig class, returning them as a Namespace object.\n",
    "ml_params = MLParameters(ccnet_network = 'gpt', encoder_network = 'none')\n",
    "ml_params.algorithm.error_function = 'mae'\n",
    "ml_params.model.ccnet_config.num_layers = 3\n",
    "ml_params.training.batch_size = 64\n",
    "ml_params.training.num_epoch = 5\n",
    "\n",
    "# Set the device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Initialize the TrainerHub class with the training configuration, data configuration, device, and use_print and use_wandb flags\n",
    "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb= False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_hub.train(trainset, testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
