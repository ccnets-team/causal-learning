{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author:\n",
    "        \n",
    "        PARK, JunHo, junho@ccnets.org\n",
    "\n",
    "        \n",
    "        KIM, JeongYoong, jeongyoong@ccnets.org\n",
    "        \n",
    "    COPYRIGHT (c) 2024. CCNets. All Rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Air Quality Data of India (2010-2023)\n",
    "\n",
    "https://www.kaggle.com/datasets/abhisheksjha/time-series-air-quality-data-of-india-2010-2023/\n",
    "\n",
    "*This notebook's data preprocessing is based on [here](https://www.kaggle.com/code/sotiristzamaras/india-s-air-quality-eda-ensemble-forecasting-pt-1).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "> <h1 style = 'font-family: Times New Roman'><b> <b style = 'color: #42c2f5'>1.</b> Import Necessary Libraries </b></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np   \n",
    "import pandas as pd  \n",
    "\n",
    "\n",
    "path_append = \"../../\"\n",
    "sys.path.append(path_append)  # Go up one directory from where you are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Time Series Air Quality Data of India\"\n",
    "\n",
    "DATASET_SRC = path_append + f'../data/Time Series Air Quality Data of India (2010-2023)'\n",
    "\n",
    "df_states = pd.read_csv(f'{DATASET_SRC}/stations_info.csv')\n",
    "df_states.drop(columns=['agency', 'station_location', 'start_month'], inplace=True)\n",
    "df_states.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_states = df_states['state'].unique()\n",
    "unique_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_state_df(state_name):\n",
    "    '''\n",
    "    Combine all state files into a single dataframe and attaching the city information.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        state_name (str): The name of the state\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "        df (DataFrame): The combined dataframe from all files of a specific state\n",
    "    '''\n",
    "    \n",
    "    state_code = df_states[df_states['state'] == state_name]['file_name'].iloc[0][:2]\n",
    "    state_files = glob.glob(f'{DATASET_SRC}/{state_code}*.csv')\n",
    "    print(f'Combining a total of {len(state_files)} files...\\n')\n",
    "\n",
    "    combined_df = []\n",
    "    \n",
    "    for state_file in state_files:\n",
    "        file_name = state_file.split(f'{DATASET_SRC}')[1][1:-4]\n",
    "        file_df = pd.read_csv(state_file)\n",
    "        file_df['city'] = df_states[df_states['file_name'] == file_name]['city'].values[0]\n",
    "        file_df['city'] = file_df['city'].astype('string')\n",
    "        combined_df.append(file_df)\n",
    "        \n",
    "    return pd.concat(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combine_state_df('Delhi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the 'From Date' column the index as datetime\n",
    "def create_dt_index(dataframe):\n",
    "    dataframe = dataframe.drop(columns='To Date')\n",
    "    dataframe['From Date'] = pd.to_datetime(dataframe['From Date'])\n",
    "    dataframe = dataframe.rename(columns={'From Date': 'datetime'})\n",
    "    return dataframe.set_index('datetime')\n",
    "\n",
    "df = create_dt_index(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_groups = {\n",
    "    \"Xylene (ug/m3)\":    [\"Xylene ()\"],\n",
    "    \"MP-Xylene (ug/m3)\": [\"MP-Xylene ()\"],\n",
    "    \"Benzene (ug/m3)\":   [\"Benzene ()\"],\n",
    "    \"Toluene (ug/m3)\":   [\"Toluene ()\"],\n",
    "    \"SO2 (ug/m3)\":       [\"SO2 ()\"],\n",
    "    \"NOx (ug/m3)\":       [\"NOx (ppb)\"],\n",
    "    \"Ozone (ug/m3)\":     [\"Ozone (ppb)\"],\n",
    "    \"AT (degree C)\":     [\"AT ()\"],\n",
    "    \"WD (degree)\":       [\"WD (degree C)\", \"WD (deg)\", \"WD ()\"],\n",
    "    \"WS (m/s)\":          [\"WS ()\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns(dataframe, columns):\n",
    "    '''\n",
    "    Merges column records into a single column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        dataframe (DataFrame): The DataFrame to edit\n",
    "        column (str): The name of the column to merge records into\n",
    "        cols_to_merge (list[str]): A list of column names to retrieve records\n",
    "    '''\n",
    "    \n",
    "    for column, cols_to_merge in columns.items():\n",
    "        # Check if the original column exist, otherwise create it\n",
    "        if column not in dataframe.columns and any(name in dataframe.columns for name in cols_to_merge):\n",
    "            dataframe[column] = np.nan\n",
    "\n",
    "        for col_name in cols_to_merge:\n",
    "            if col_name in dataframe.columns:\n",
    "                dataframe[column] = dataframe[column].fillna(dataframe[col_name])\n",
    "                dataframe = dataframe.drop(columns=[col_name])\n",
    "            \n",
    "    return dataframe\n",
    "df = merge_columns(df, reduction_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='all')\n",
    "df = df.dropna(how='all', axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold value indicating how much of the dataset needs to be not missing.\n",
    "threshold = 0.6\n",
    "df = df.dropna(thresh=df.shape[0]*threshold, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.resample('60min').mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PM2.5 (ug/m3)'] = df['PM2.5 (ug/m3)'].mask(df['PM2.5 (ug/m3)'].gt(950))\n",
    "df['CO (mg/m3)'] = df['CO (mg/m3)'].mask(((df.index > '2015') & df['CO (mg/m3)'].gt(35)))\n",
    "df['Ozone (ug/m3)'] = df['Ozone (ug/m3)'].mask(df['Ozone (ug/m3)'].gt(185))\n",
    "df['NOx (ug/m3)'] = df['NOx (ug/m3)'].mask((\n",
    "    ((df.index < '2013') & (df['NOx (ug/m3)'].gt(380))) |\n",
    "    ((df.index > '2015') & (df.index < '2016') & (df['NOx (ug/m3)'].gt(400))) |\n",
    "    ((df.index > '2016') & (df['NOx (ug/m3)'].gt(450)))\n",
    "))\n",
    "df = df.interpolate(method='pad')\n",
    "df = df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = pd.to_datetime(df.index, format='%d.%m.%Y %H:%M:%S')\n",
    "timestamp = date_time.map(pd.Timestamp.timestamp)\n",
    "day = 24*60*60\n",
    "year = (365.2425)*day\n",
    "\n",
    "df['Day sin'] = np.sin(timestamp * (2 * np.pi / day))\n",
    "df['Day cos'] = np.cos(timestamp * (2 * np.pi / day))\n",
    "df['Year sin'] = np.sin(timestamp * (2 * np.pi / year))\n",
    "df['Year cos'] = np.cos(timestamp * (2 * np.pi / year))\n",
    "\n",
    "DATETIME_FEATURES = ['Day sin', 'Day cos', 'Year sin', 'Year cos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.query('datetime > 2014')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tools.preprocessing.template_dataset import TemplateDataset\n",
    "\n",
    "train_df, testset = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.25, shuffle=False)\n",
    "# predict the next value in the sequence\n",
    "train_df_x = train_df.iloc[:, :-1] # all columns except the last one\n",
    "train_df_y = train_df.iloc[:, -1:] # only the last column\n",
    "\n",
    "val_df_x = val_df.iloc[:, :-1] # all columns except the last one\n",
    "val_df_y = val_df.iloc[:, -1:] # only the last column\n",
    "\n",
    "print('train df shape: ', train_df.shape)\n",
    "print('test df shape: ', val_df.shape)\n",
    "min_seq_len = 16    \n",
    "max_seq_len = 32\n",
    "trainset = TemplateDataset(train_df_x, train_df_y, min_seq_len = min_seq_len, max_seq_len = max_seq_len)\n",
    "valset = TemplateDataset(val_df_x, val_df_y, min_seq_len = max_seq_len, max_seq_len = max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.setting.data_config import DataConfig\n",
    "from tools.setting.ml_params import MLParameters\n",
    "from trainer_hub import TrainerHub\n",
    "\n",
    "num_features = trainset.X.shape[1]\n",
    "num_classes = trainset.y.shape[1]\n",
    "data_config = DataConfig(dataset_name = dataset_name, task_type='regression', obs_shape=[num_features], label_size=num_classes)\n",
    "\n",
    "#  Set training configuration from the AlgorithmConfig class, returning them as a Namespace object.\n",
    "ml_params = MLParameters(model_name = 'gpt')\n",
    "ml_params.training.error_function = 'mae'\n",
    "ml_params.training.num_epoch = 1\n",
    "\n",
    "# Set the device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Initialize the TrainerHub class with the training configuration, data configuration, device, and use_print and use_wandb flags\n",
    "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer_hub.train(trainset, valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_hub.test(valset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
