{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author:\n",
    "        \n",
    "        PARK, JunHo, junho@ccnets.org\n",
    "        \n",
    "    COPYRIGHT (c) 2024. CCNets. All Rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proving CCNet as a Causal Model: Two Evidences for Conditional Independence and Causal Generation\n",
    "\n",
    "## Overview of CCNet\n",
    "\n",
    "CCNet (Cooperative Causal Network) introduces a framework for understanding causal dynamics in observed data. It decouples direct causes of observations (X) from target outcomes (Y) by leveraging neural networks to identify explanatory factors (E) that are conditionally independent of Y, given X.\n",
    "\n",
    "CCNet not only ensures that E remains independent of Y, given X, but also generates new observations (X) that maintain the statistical appearance and distribution of the original dataset based on the causal graph. By manipulating Y through the causal model, CCNet validates that the generated data follows the same underlying causal relationships as the original data, thereby reinforcing causal generation over mere causal inference, which is limited in the current study.\n",
    "\n",
    "## Methodology for Causal Learning\n",
    "\n",
    "CCNet uses three interconnected neural network models—Explainer, Reasoner, and Producer—to establish the conditional independence of E from Y, given X, and to generate realistic data.\n",
    "\n",
    "## Experiment Description\n",
    "\n",
    "### Objective\n",
    "\n",
    "Validate CCNet's effectiveness in:\n",
    "1. **Conditional Independence**: Demonstrating that E is conditionally independent of Y, given X.\n",
    "2. **Causal Generation**: Showing that altering Y can generate new X while maintaining the distribution of the original dataset.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The credit card fraud detection dataset includes:\n",
    "- **`df_y_class`**: Binary fraud status for classification.\n",
    "- **`df_y_amount`**: Continuous transaction amount for regression.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "#### Dataset Preparation\n",
    "\n",
    "- **CausalModelDataset**: Configures features X in the dataset with either `df_y_class` or `df_y_amount` as the target.\n",
    "- **EncodingDataset**: Generates explanatory encodings (E) from the data for prediction modeling.\n",
    "\n",
    "#### Training Process\n",
    "\n",
    "##### Conditional Independence Proof\n",
    "\n",
    "1. **Causal Model Training**: Train the causal model iteratively to discover E and understand the relationships between X and Y.\n",
    "2. **Prediction Modeling**:\n",
    "   - **Selected Target Classifier/Regressor**: Train on encodings (E) for the selected target (`df_y_class` or `df_y_amount`).\n",
    "   - **Non-Selected Target Classifier**: Train on encodings (E) for the non-selected target (`df_y_amount` if `df_y_class` is selected, and vice versa).\n",
    "   - **Feature Switching**: Alternate between selected and non-selected targets to demonstrate that E predicts the non-selected Y but not the selected Y, proving conditional independence regardless of the specific data domain.\n",
    "\n",
    "##### Data Generation Proof\n",
    "\n",
    "1. **Train CCNet**: Train CCNet on the credit card fraud detection training set.\n",
    "\n",
    "###### First Part\n",
    "\n",
    "1. **Train Classifier**: Train a classifier on the original dataset.\n",
    "2. **Recreate Test Set**: Use the causal model to generate a new test set by randomly altering Y.\n",
    "3. **Evaluation**: Test the classifier on the recreated test set and compare the results with the original test set to confirm that the real data distribution is preserved.\n",
    "\n",
    "###### Second Part\n",
    "\n",
    "1. **Data Generation**: Generate new observations (X) by randomly altering Y (fraud class), ensuring that the generated data maintains the appearance and frequency distribution of the original dataset.\n",
    "2. **Classifier Training as Validators**:\n",
    "   - **Original Data Classifier**: Train a classifier on the original dataset.\n",
    "   - **Generated Data Classifier**: Train a classifier on the data generated with randomly sampled labels.\n",
    "3. **Performance Evaluation**:\n",
    "   - **Prediction Alignment**: Evaluate how well the classifier trained on the original data predicts the labels of the generated data.\n",
    "   - **Comparative Analysis**: Compare the performance metrics of classifiers trained on the original data and the generated data to validate their alignment and similarity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5   \n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321  \\\n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22   \n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838  \\\n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount   \n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62  \\\n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd \n",
    "path_append = \"../../\"\n",
    "sys.path.append(path_append)  # Go up one directory from where you are.\n",
    "\n",
    "# https://fraud-detection-handbook.github.io/fraud-detection-handbook/Chapter_7_DeepLearning/FeedForwardNeuralNetworks.html\n",
    "\n",
    "dataroot = path_append + \"../data/credit_card_fraud_detection/creditcard.csv\"\n",
    "df = pd.read_csv(dataroot)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.83 %of the dataset\n",
      "Frauds 0.17 %of the dataset\n"
     ]
    }
   ],
   "source": [
    "print('No Frauds', round(df['Class'].value_counts()[0] / len(df) *100,2), '%of the dataset')\n",
    "print('Frauds', round(df['Class'].value_counts()[1] / len(df) *100,2), '%of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "      <th>Null Count</th>\n",
       "      <th>Scaled</th>\n",
       "      <th>Encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <td>-2.215465</td>\n",
       "      <td>1.822074</td>\n",
       "      <td>-4.864903e-17</td>\n",
       "      <td>1.109630</td>\n",
       "      <td>0</td>\n",
       "      <td>Minmax</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <td>-56.407510</td>\n",
       "      <td>2.454930</td>\n",
       "      <td>-5.987573e-19</td>\n",
       "      <td>1.958696</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V2</th>\n",
       "      <td>-72.715728</td>\n",
       "      <td>22.057729</td>\n",
       "      <td>-2.250329e-17</td>\n",
       "      <td>1.651309</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V3</th>\n",
       "      <td>-48.325589</td>\n",
       "      <td>9.382558</td>\n",
       "      <td>-2.035775e-17</td>\n",
       "      <td>1.516255</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V4</th>\n",
       "      <td>-5.683171</td>\n",
       "      <td>16.875344</td>\n",
       "      <td>1.102711e-17</td>\n",
       "      <td>1.415869</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V5</th>\n",
       "      <td>-113.743307</td>\n",
       "      <td>34.801666</td>\n",
       "      <td>-1.277349e-17</td>\n",
       "      <td>1.380247</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V6</th>\n",
       "      <td>-26.160506</td>\n",
       "      <td>73.301626</td>\n",
       "      <td>1.120175e-17</td>\n",
       "      <td>1.332271</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V7</th>\n",
       "      <td>-43.557242</td>\n",
       "      <td>120.589494</td>\n",
       "      <td>1.142628e-17</td>\n",
       "      <td>1.237094</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V8</th>\n",
       "      <td>-73.216718</td>\n",
       "      <td>20.007208</td>\n",
       "      <td>4.989644e-20</td>\n",
       "      <td>1.194353</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V9</th>\n",
       "      <td>-13.434066</td>\n",
       "      <td>15.594995</td>\n",
       "      <td>-5.987573e-19</td>\n",
       "      <td>1.098632</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V10</th>\n",
       "      <td>-24.588262</td>\n",
       "      <td>23.745136</td>\n",
       "      <td>7.709000e-18</td>\n",
       "      <td>1.088850</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V11</th>\n",
       "      <td>-4.797473</td>\n",
       "      <td>12.018913</td>\n",
       "      <td>-1.955940e-17</td>\n",
       "      <td>1.020713</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V12</th>\n",
       "      <td>-18.683715</td>\n",
       "      <td>7.848392</td>\n",
       "      <td>4.790058e-18</td>\n",
       "      <td>0.999201</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V13</th>\n",
       "      <td>-5.791881</td>\n",
       "      <td>7.126883</td>\n",
       "      <td>-7.584259e-18</td>\n",
       "      <td>0.995274</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V14</th>\n",
       "      <td>-19.214325</td>\n",
       "      <td>10.526766</td>\n",
       "      <td>6.486537e-18</td>\n",
       "      <td>0.958596</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V15</th>\n",
       "      <td>-4.498945</td>\n",
       "      <td>8.877742</td>\n",
       "      <td>2.644511e-18</td>\n",
       "      <td>0.915316</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V16</th>\n",
       "      <td>-14.129855</td>\n",
       "      <td>17.315112</td>\n",
       "      <td>1.631614e-17</td>\n",
       "      <td>0.876253</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V17</th>\n",
       "      <td>-25.162799</td>\n",
       "      <td>9.253526</td>\n",
       "      <td>-4.989644e-19</td>\n",
       "      <td>0.849337</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V18</th>\n",
       "      <td>-9.498746</td>\n",
       "      <td>5.041069</td>\n",
       "      <td>1.237432e-17</td>\n",
       "      <td>0.838176</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V19</th>\n",
       "      <td>-7.213527</td>\n",
       "      <td>5.591971</td>\n",
       "      <td>4.989644e-20</td>\n",
       "      <td>0.814041</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V20</th>\n",
       "      <td>-54.497720</td>\n",
       "      <td>39.420904</td>\n",
       "      <td>-3.767181e-18</td>\n",
       "      <td>0.770925</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V21</th>\n",
       "      <td>-34.830382</td>\n",
       "      <td>27.202839</td>\n",
       "      <td>3.492751e-19</td>\n",
       "      <td>0.734524</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V22</th>\n",
       "      <td>-10.933144</td>\n",
       "      <td>10.503090</td>\n",
       "      <td>3.056157e-18</td>\n",
       "      <td>0.725702</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V23</th>\n",
       "      <td>-44.807735</td>\n",
       "      <td>22.528412</td>\n",
       "      <td>3.193372e-18</td>\n",
       "      <td>0.624460</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V24</th>\n",
       "      <td>-2.836627</td>\n",
       "      <td>4.584549</td>\n",
       "      <td>6.349322e-18</td>\n",
       "      <td>0.605647</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V25</th>\n",
       "      <td>-10.295397</td>\n",
       "      <td>7.519589</td>\n",
       "      <td>-2.819149e-18</td>\n",
       "      <td>0.521278</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V26</th>\n",
       "      <td>-2.604551</td>\n",
       "      <td>3.517346</td>\n",
       "      <td>5.363867e-18</td>\n",
       "      <td>0.482227</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V27</th>\n",
       "      <td>-22.565679</td>\n",
       "      <td>31.612198</td>\n",
       "      <td>8.607135e-19</td>\n",
       "      <td>0.403632</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V28</th>\n",
       "      <td>-15.430084</td>\n",
       "      <td>33.847808</td>\n",
       "      <td>-1.746375e-18</td>\n",
       "      <td>0.330083</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amount</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>8.834962e+01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.727486e-03</td>\n",
       "      <td>0.041527</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Min           Max          Mean         Std  Null Count   \n",
       "Time     -2.215465      1.822074 -4.864903e-17    1.109630           0  \\\n",
       "V1      -56.407510      2.454930 -5.987573e-19    1.958696           0   \n",
       "V2      -72.715728     22.057729 -2.250329e-17    1.651309           0   \n",
       "V3      -48.325589      9.382558 -2.035775e-17    1.516255           0   \n",
       "V4       -5.683171     16.875344  1.102711e-17    1.415869           0   \n",
       "V5     -113.743307     34.801666 -1.277349e-17    1.380247           0   \n",
       "V6      -26.160506     73.301626  1.120175e-17    1.332271           0   \n",
       "V7      -43.557242    120.589494  1.142628e-17    1.237094           0   \n",
       "V8      -73.216718     20.007208  4.989644e-20    1.194353           0   \n",
       "V9      -13.434066     15.594995 -5.987573e-19    1.098632           0   \n",
       "V10     -24.588262     23.745136  7.709000e-18    1.088850           0   \n",
       "V11      -4.797473     12.018913 -1.955940e-17    1.020713           0   \n",
       "V12     -18.683715      7.848392  4.790058e-18    0.999201           0   \n",
       "V13      -5.791881      7.126883 -7.584259e-18    0.995274           0   \n",
       "V14     -19.214325     10.526766  6.486537e-18    0.958596           0   \n",
       "V15      -4.498945      8.877742  2.644511e-18    0.915316           0   \n",
       "V16     -14.129855     17.315112  1.631614e-17    0.876253           0   \n",
       "V17     -25.162799      9.253526 -4.989644e-19    0.849337           0   \n",
       "V18      -9.498746      5.041069  1.237432e-17    0.838176           0   \n",
       "V19      -7.213527      5.591971  4.989644e-20    0.814041           0   \n",
       "V20     -54.497720     39.420904 -3.767181e-18    0.770925           0   \n",
       "V21     -34.830382     27.202839  3.492751e-19    0.734524           0   \n",
       "V22     -10.933144     10.503090  3.056157e-18    0.725702           0   \n",
       "V23     -44.807735     22.528412  3.193372e-18    0.624460           0   \n",
       "V24      -2.836627      4.584549  6.349322e-18    0.605647           0   \n",
       "V25     -10.295397      7.519589 -2.819149e-18    0.521278           0   \n",
       "V26      -2.604551      3.517346  5.363867e-18    0.482227           0   \n",
       "V27     -22.565679     31.612198  8.607135e-19    0.403632           0   \n",
       "V28     -15.430084     33.847808 -1.746375e-18    0.330083           0   \n",
       "Amount    0.000000  25691.160000  8.834962e+01  250.120109           0   \n",
       "Class     0.000000      1.000000  1.727486e-03    0.041527           0   \n",
       "\n",
       "        Scaled Encoded  \n",
       "Time    Minmax    None  \n",
       "V1        None    None  \n",
       "V2        None    None  \n",
       "V3        None    None  \n",
       "V4        None    None  \n",
       "V5        None    None  \n",
       "V6        None    None  \n",
       "V7        None    None  \n",
       "V8        None    None  \n",
       "V9        None    None  \n",
       "V10       None    None  \n",
       "V11       None    None  \n",
       "V12       None    None  \n",
       "V13       None    None  \n",
       "V14       None    None  \n",
       "V15       None    None  \n",
       "V16       None    None  \n",
       "V17       None    None  \n",
       "V18       None    None  \n",
       "V19       None    None  \n",
       "V20       None    None  \n",
       "V21       None    None  \n",
       "V22       None    None  \n",
       "V23       None    None  \n",
       "V24       None    None  \n",
       "V25       None    None  \n",
       "V26       None    None  \n",
       "V27       None    None  \n",
       "V28       None    None  \n",
       "Amount    None    None  \n",
       "Class     None    None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tools.preprocessing.data_frame import auto_preprocess_dataframe\n",
    "from tools.preprocessing.scaler import scale_dataframe\n",
    "\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "target_columns = ['Amount', 'Class']\n",
    "df, description = auto_preprocess_dataframe(df, target_columns)\n",
    "df, scale_amount = scale_dataframe(df, transform_columns = ['Amount'])\n",
    "scale_class = None\n",
    "\n",
    "df_x = df.drop(columns = target_columns)\n",
    "df_y_class = df['Class']\n",
    "task_type_class = 'binary_classification'\n",
    "\n",
    "df_y_amount = df['Amount']\n",
    "task_type_amount = 'regression'\n",
    "\n",
    "TEST_SIZE = 0.3\n",
    "\n",
    "X = torch.tensor(df_x.values[:], dtype=torch.float32)\n",
    "y_class = torch.tensor(df_y_class.values[:], dtype=torch.float32).unsqueeze(-1)\n",
    "y_amount = torch.tensor(df_y_amount.values[:], dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "num_features = description['num_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the labeled and unlabeled dataset classes\n",
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x.detach().cpu() if isinstance(x, torch.Tensor) else x\n",
    "        self.y = y.detach().cpu() if isinstance(y, torch.Tensor) else y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        vals = self.x[index] if isinstance(self.x, torch.Tensor) else torch.tensor(self.x[index], dtype=torch.float32)\n",
    "        label = self.y[index] if isinstance(self.y, torch.Tensor) else torch.tensor(self.y[index], dtype=torch.float32)\n",
    "        return vals, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility of experiments\n",
    "from nn.utils.init import set_random_seed\n",
    "set_random_seed(0)\n",
    "\n",
    "# Importing configuration setups for ML parameters and data\n",
    "import torch\n",
    "from tools.setting.ml_params import MLParameters\n",
    "from tools.setting.data_config import DataConfig\n",
    "from trainer_hub import TrainerHub as CasualTrainer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "def initialize_causal_trainer(task_type, target_scale):  \n",
    "    num_classes = 1\n",
    "\n",
    "    # Configuration for the data handling, defining dataset specifics and the task type\n",
    "    data_config = DataConfig(dataset_name='CreditCardFraudDetection', task_type=task_type, obs_shape=[num_features], \n",
    "                            label_size=1, label_scale= target_scale, explain_size=num_features - num_classes)\n",
    "\n",
    "    # Initializing ML parameters without a core model and setting the encoder model to 'tabnet' with specific configurations\n",
    "    ml_params = MLParameters(ccnet_network='tabnet', encoder_network='none')\n",
    "\n",
    "    # Setting training parameters and device configuration\n",
    "    ml_params.training.num_epoch = 1\n",
    "    ml_params.model.ccnet_config.num_layers = 4\n",
    "    ml_params.algorithm.error_function = 'mse'\n",
    "\n",
    "    # Create a TrainerHub instance to manage training and data processing\n",
    "    causal_trainer = CasualTrainer(ml_params, data_config, device, use_print=True, use_wandb=False)\n",
    "    \n",
    "    return causal_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nn.tabnet import TabNet \n",
    "from tools.setting.ml_params import ModelConfig\n",
    "\n",
    "class PredictionModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, task_type, num_layers=3, hidden_size=256):\n",
    "        super(PredictionModel, self).__init__()\n",
    "        \n",
    "        if task_type == 'binary_classification':\n",
    "            final_act = torch.nn.Sigmoid()\n",
    "        elif task_type == 'regression':\n",
    "            final_act = torch.nn.Identity()\n",
    "        \n",
    "        self.final_act = final_act\n",
    "        \n",
    "        model_config = ModelConfig('tabnet')\n",
    "        model_config.num_layers = num_layers\n",
    "        model_config.d_model = hidden_size\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Create a list to hold all layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(torch.nn.Linear(input_size, hidden_size))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        ## Add TabNet layers\n",
    "        layers.append(TabNet(model_config))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(torch.nn.Linear(hidden_size, output_size))\n",
    "        \n",
    "        # Register all layers\n",
    "        self.layers = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return self.final_act(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def initialize_plot():\n",
    "    plt.ion()  # Turn on interactive mode\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    ax.set_title('Metrics over Epochs')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('F1 Score / Normalized R²')\n",
    "    \n",
    "    return fig, ax\n",
    "def display_plot(fig):\n",
    "    clear_output(wait=True)  # Clear the previous output\n",
    "    display(fig)  # Display the updated figure\n",
    "    plt.pause(0.1)  # Allow the plot to update\n",
    "    \n",
    "def normalize_r2(r2_values):\n",
    "    # Normalize R² to be within [0, 1]\n",
    "    r2_min = 0  # Define min for R² (typically 0 or lower bound of your expected range)\n",
    "    r2_max = 1  # Define max for R² (typically 1 or upper bound of your expected range)\n",
    "    return (r2_values - r2_min) / (r2_max - r2_min)\n",
    "\n",
    "def plot_metric(ax, epochs, selected_results, none_selected_results, task_type_selected, task_type_non_selected):\n",
    "    ax.cla()  # Clear the previous plot without resetting the axis\n",
    "    \n",
    "    # Plot selected results\n",
    "    if task_type_selected == 'binary_classification':\n",
    "        ax.plot(epochs, selected_results['f1_score'], label='Selected Attributes (F1 Score)', linestyle='-', marker='o')\n",
    "    elif task_type_selected == 'regression':\n",
    "        normalized_r2 = normalize_r2(np.array(selected_results['r2']))\n",
    "        ax.plot(epochs, normalized_r2, label='Selected Attributes (R²)', linestyle='-', marker='o')\n",
    "    \n",
    "    # Plot non-selected results\n",
    "    if task_type_non_selected == 'binary_classification':\n",
    "        ax.plot(epochs, none_selected_results['f1_score'], label='Non-selected Attributes (F1 Score)', linestyle='--', marker='x')\n",
    "    elif task_type_non_selected == 'regression':\n",
    "        normalized_r2 = normalize_r2(np.array(none_selected_results['r2']))\n",
    "        ax.plot(epochs, normalized_r2, label='Non-selected Attributes (R²)', linestyle='--', marker='x')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_title('Metrics over Epochs')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('F1 Score / Normalized R²')\n",
    "    ax.legend()\n",
    "    display_plot(ax.figure)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score, r2_score\n",
    "\n",
    "# Testing function for the model\n",
    "def _test_prediction_model(model, dataset, task_type, target_scale, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    target_scale = torch.tensor(target_scale, dtype=torch.float).to(device) if target_scale is not None else None\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for data, labels in dataloader:\n",
    "            data, labels = data.to(device), labels.to(device).float()\n",
    "            outputs = model(data)\n",
    "            \n",
    "            if task_type == 'binary_classification':\n",
    "                preds = torch.sigmoid(outputs).round()\n",
    "            elif task_type == 'regression':\n",
    "                preds = target_scale * outputs if target_scale is not None else outputs\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    \n",
    "    if task_type == 'binary_classification':\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        return accuracy, f1\n",
    "    elif task_type == 'regression':\n",
    "        r2 = r2_score(all_labels, all_preds)\n",
    "        return r2\n",
    "\n",
    "# Function to test the prediction models and update plots\n",
    "def test_prediction_models(epoch, ax, selected_classifier, none_selected_classifier, testset_selected, testset_none_selected, \n",
    "                            task_type_selected, task_type_non_selected, scale_selected, scale_none_selected, selected_results, none_selected_results, device):\n",
    "    print(f\"Testing causal classifier on selected attributes at epoch {epoch}...\")\n",
    "    selected_metrics = _test_prediction_model(selected_classifier, testset_selected, task_type_selected, scale_selected, device)\n",
    "    if task_type_selected == 'binary_classification':\n",
    "        selected_results['f1_score'].append(selected_metrics[1])\n",
    "    elif task_type_selected == 'regression':\n",
    "        selected_results['r2'].append(selected_metrics)\n",
    "\n",
    "    print(f\"Testing classifier on non-selected attributes at epoch {epoch}...\")\n",
    "    none_selected_metrics = _test_prediction_model(none_selected_classifier, testset_none_selected, task_type_non_selected, scale_none_selected, device)\n",
    "    if task_type_non_selected == 'binary_classification':\n",
    "        none_selected_results['f1_score'].append(none_selected_metrics[1])\n",
    "    elif task_type_non_selected == 'regression':\n",
    "        none_selected_results['r2'].append(none_selected_metrics)\n",
    "    \n",
    "    # Update plots\n",
    "    if task_type_selected == 'binary_classification':\n",
    "        epochs = range(1, len(selected_results['f1_score']) + 1)\n",
    "    elif task_type_selected == 'regression':\n",
    "        epochs = range(1, len(selected_results['r2']) + 1)\n",
    "    plot_metric(ax, epochs, selected_results, none_selected_results, task_type_selected, task_type_non_selected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Function to train classifier\n",
    "DECAY_RATE = 0.01\n",
    "ITERATION_100K = 100000\n",
    "gamma = pow(DECAY_RATE, 1 / ITERATION_100K)\n",
    "\n",
    "def train_prediction_model(model, trainset, task_type, target_scale, num_epochs=5, gamma=gamma, device='cuda'):\n",
    "    model.train()\n",
    "    train_loader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "    len_loader = len(train_loader)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        sum_loss = 0\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "\n",
    "            if task_type == 'binary_classification':\n",
    "                # Using binary cross-entropy loss for binary classification\n",
    "                loss = torch.nn.functional.binary_cross_entropy_with_logits(outputs, labels)\n",
    "            elif task_type == 'regression':\n",
    "                if target_scale != None:\n",
    "                    scaled_labels = labels\n",
    "                else:\n",
    "                    scaled_labels = target_scale * labels\n",
    "                # Using mean squared error for regression\n",
    "                loss = torch.nn.functional.mse_loss(outputs, scaled_labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            sum_loss += loss.item()\n",
    "        \n",
    "        avg_loss = sum_loss / len_loader\n",
    "        print(f\"Epoch: {epoch + 1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(f\"Final Learning rate: {optimizer.param_groups[0]['lr']:.8f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_causal_and_prediction_models(X, selected_y, non_selected_y, task_type_selected, task_type_non_selected, scale_selected, scale_non_selected, num_epoch = 10):\n",
    "    # Split data for \"selected\" target\n",
    "    train_x_selected, test_x_selected, train_y_selected, test_y_selected = train_test_split(\n",
    "        X, selected_y, test_size=TEST_SIZE, shuffle=False\n",
    "    )\n",
    "\n",
    "    # Split data for \"non_selected\" target\n",
    "    train_x_non_selected, test_x_non_selected, train_y_non_selected, test_y_non_selected = train_test_split(\n",
    "        X, non_selected_y, test_size=TEST_SIZE, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Initialize the plot and line objects only once\n",
    "    fig, ax = initialize_plot()\n",
    "\n",
    "    # Create LabeledDataset instances\n",
    "    causal_model_dataset = LabeledDataset(train_x_selected, train_y_selected)\n",
    "    # Initialize causal trainer\n",
    "    causal_trainer = initialize_causal_trainer(task_type_selected, scale_selected)\n",
    "\n",
    "    # Create dictionaries to store results\n",
    "    selected_results_dict = {'f1_score': [], 'r2': []}\n",
    "    non_selected_results_dict = {'f1_score': [], 'r2': []}\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        if epoch > 0:\n",
    "            print(f\"Training causal model at epoch {epoch}...\")\n",
    "            causal_trainer.train(causal_model_dataset)\n",
    "        causal_model = causal_trainer.ccnet\n",
    "\n",
    "        selected_prediction_model = PredictionModel(causal_model.explain_size, 1, task_type=task_type_selected).to(device)\n",
    "        non_selected_prediction_model = PredictionModel(causal_model.explain_size, 1, task_type=task_type_non_selected).to(device)\n",
    "\n",
    "        # Train and evaluate classifiers on the explanation datasets\n",
    "        print(\"Training causal classifier on selected attributes...\")\n",
    "        train_explain_selected = causal_model.explain(train_x_selected.to(device))\n",
    "        trainset_explain_selected = LabeledDataset(train_explain_selected, train_y_selected)\n",
    "        test_explain_selected = causal_model.explain(test_x_selected.to(device))\n",
    "        testset_explain_selected = LabeledDataset(test_explain_selected, test_y_selected)\n",
    "        train_prediction_model(selected_prediction_model, trainset_explain_selected, task_type_selected, scale_selected, device = device)\n",
    "\n",
    "        print(\"Training classifier on non-selected attributes...\")\n",
    "        train_explain_non_selected = causal_model.explain(train_x_non_selected.to(device))\n",
    "        trainset_explain_non_selected = LabeledDataset(train_explain_non_selected, train_y_non_selected)\n",
    "        test_explain_non_selected = causal_model.explain(test_x_non_selected.to(device))\n",
    "        testset_explain_non_selected = LabeledDataset(test_explain_non_selected, test_y_non_selected)\n",
    "        train_prediction_model(non_selected_prediction_model, trainset_explain_non_selected, task_type_non_selected, scale_non_selected, device = device)\n",
    "        \n",
    "        # Test classifiers\n",
    "        test_prediction_models(epoch, ax, \n",
    "                               selected_prediction_model, non_selected_prediction_model,\n",
    "                               testset_explain_selected, testset_explain_non_selected,\n",
    "                               task_type_selected, task_type_non_selected, \n",
    "                               scale_selected, scale_non_selected, \n",
    "                               selected_results_dict, non_selected_results_dict, \n",
    "                               device)\n",
    "\n",
    "        print(\"selected_results_dict\", selected_results_dict)\n",
    "        print(\"non_selected_results_dict\", non_selected_results_dict)\n",
    "\n",
    "    plt.ioff()  # Turn off interactive mode\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_results_dict {'f1_score': [0.49895913352997406, 0.49853862948094935, 0.8130663765213311, 0.4993407984249477], 'r2': []}\n",
      "non_selected_results_dict {'f1_score': [], 'r2': [-93.70123094481859, -47.54808169383229, -63.41650176088844, -51.973590930697284]}\n",
      "Training causal model at epoch 4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5814244fcdf4bb3bcd72fcf0203a1b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dae83c5993d47c98f87ff116d7383f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][55/3115][Time 359.11]\n",
      "Unified LR across all optimizers: 0.0001509107459320063\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0065\tGen: 0.3581\tRec: 0.3589\tE: 0.0081\tR: 0.0154\tP: 0.7335\n",
      "[0/1][155/3115][Time 9.83]\n",
      "Unified LR across all optimizers: 0.0001504593342318875\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0080\tGen: 0.3694\tRec: 0.3714\tE: 0.0107\tR: 0.0548\tP: 1.0885\n",
      "[0/1][255/3115][Time 10.02]\n",
      "Unified LR across all optimizers: 0.00015000927281681126\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0059\tGen: 0.3657\tRec: 0.3675\tE: 0.0022\tR: 0.0376\tP: 0.7211\n",
      "[0/1][355/3115][Time 10.26]\n",
      "Unified LR across all optimizers: 0.0001495605576477381\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0068\tGen: 0.3665\tRec: 0.3678\tE: 0.0048\tR: 0.0224\tP: 0.8351\n",
      "[0/1][455/3115][Time 9.83]\n",
      "Unified LR across all optimizers: 0.00014911318469771047\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0023\tGen: 0.3623\tRec: 0.3626\tE: 0.0018\tR: 0.0048\tP: 0.7382\n",
      "[0/1][555/3115][Time 9.76]\n",
      "Unified LR across all optimizers: 0.00014866714995181608\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0053\tGen: 0.3621\tRec: 0.3624\tE: 0.0203\tR: 0.0173\tP: 0.7076\n",
      "[0/1][655/3115][Time 9.85]\n",
      "Unified LR across all optimizers: 0.0001482224494071524\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0037\tGen: 0.3659\tRec: 0.3666\tE: 0.0022\tR: 0.0175\tP: 0.7254\n",
      "[0/1][755/3115][Time 9.82]\n",
      "Unified LR across all optimizers: 0.00014777907907279062\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0095\tGen: 0.3627\tRec: 0.3659\tE: 0.0044\tR: 0.0510\tP: 0.7632\n",
      "[0/1][855/3115][Time 9.81]\n",
      "Unified LR across all optimizers: 0.00014733703496973974\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0058\tGen: 0.3636\tRec: 0.3648\tE: 0.0078\tR: 0.0408\tP: 0.7406\n",
      "[0/1][955/3115][Time 10.15]\n",
      "Unified LR across all optimizers: 0.00014689631313091094\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0088\tGen: 0.3580\tRec: 0.3609\tE: 0.0047\tR: 0.0651\tP: 0.7230\n",
      "[0/1][1055/3115][Time 9.97]\n",
      "Unified LR across all optimizers: 0.00014645690960108225\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0081\tGen: 0.3579\tRec: 0.3596\tE: 0.0040\tR: 0.0287\tP: 0.7779\n",
      "[0/1][1155/3115][Time 9.92]\n",
      "Unified LR across all optimizers: 0.0001460188204368623\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0072\tGen: 0.3627\tRec: 0.3646\tE: 0.0051\tR: 0.0470\tP: 0.7423\n",
      "[0/1][1255/3115][Time 10.29]\n",
      "Unified LR across all optimizers: 0.00014558204170665547\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0077\tGen: 0.3542\tRec: 0.3563\tE: 0.0036\tR: 0.0396\tP: 0.6999\n",
      "[0/1][1355/3115][Time 10.45]\n",
      "Unified LR across all optimizers: 0.00014514656949062672\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0064\tGen: 0.3585\tRec: 0.3597\tE: 0.0082\tR: 0.0334\tP: 0.7393\n",
      "[0/1][1455/3115][Time 9.99]\n",
      "Unified LR across all optimizers: 0.0001447123998806661\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0078\tGen: 0.3603\tRec: 0.3623\tE: 0.0079\tR: 0.0368\tP: 0.7474\n",
      "[0/1][1555/3115][Time 9.67]\n",
      "Unified LR across all optimizers: 0.00014427952898035364\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0033\tGen: 0.3572\tRec: 0.3585\tE: 0.0011\tR: 0.0231\tP: 0.7647\n",
      "[0/1][1655/3115][Time 9.89]\n",
      "Unified LR across all optimizers: 0.0001438479529049249\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0052\tGen: 0.3644\tRec: 0.3650\tE: 0.0106\tR: 0.0172\tP: 0.8053\n",
      "[0/1][1755/3115][Time 10.04]\n",
      "Unified LR across all optimizers: 0.00014341766778123532\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0054\tGen: 0.3683\tRec: 0.3697\tE: 0.0032\tR: 0.0284\tP: 0.7650\n",
      "[0/1][1855/3115][Time 10.66]\n",
      "Unified LR across all optimizers: 0.00014298866974772633\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0117\tGen: 0.3678\tRec: 0.3702\tE: 0.0127\tR: 0.0519\tP: 0.7558\n",
      "[0/1][1955/3115][Time 10.32]\n",
      "Unified LR across all optimizers: 0.00014256095495439\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0065\tGen: 0.3649\tRec: 0.3657\tE: 0.0067\tR: 0.0261\tP: 0.7020\n",
      "[0/1][2055/3115][Time 9.88]\n",
      "Unified LR across all optimizers: 0.00014213451956273458\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0069\tGen: 0.3665\tRec: 0.3683\tE: 0.0056\tR: 0.0266\tP: 0.7630\n",
      "[0/1][2155/3115][Time 10.09]\n",
      "Unified LR across all optimizers: 0.00014170935974575053\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0062\tGen: 0.3680\tRec: 0.3692\tE: 0.0060\tR: 0.0349\tP: 0.8489\n",
      "[0/1][2255/3115][Time 10.24]\n",
      "Unified LR across all optimizers: 0.00014128547168787558\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0101\tGen: 0.3615\tRec: 0.3643\tE: 0.0085\tR: 0.0484\tP: 0.8009\n",
      "[0/1][2355/3115][Time 10.05]\n",
      "Unified LR across all optimizers: 0.00014086285158496092\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0090\tGen: 0.3660\tRec: 0.3686\tE: 0.0058\tR: 0.0604\tP: 0.8137\n",
      "[0/1][2455/3115][Time 10.05]\n",
      "Unified LR across all optimizers: 0.00014044149564423683\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0120\tGen: 0.3591\tRec: 0.3636\tE: 0.0056\tR: 0.1024\tP: 0.6911\n",
      "[0/1][2555/3115][Time 10.08]\n",
      "Unified LR across all optimizers: 0.00014002140008427877\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0076\tGen: 0.3649\tRec: 0.3660\tE: 0.0131\tR: 0.0233\tP: 0.7982\n",
      "[0/1][2655/3115][Time 9.97]\n",
      "Unified LR across all optimizers: 0.00013960256113497327\n",
      "--------------------Training Metrics--------------------\n",
      "CCNet:  Three Tabnet\n",
      "Inf: 0.0055\tGen: 0.3659\tRec: 0.3671\tE: 0.0057\tR: 0.0278\tP: 0.7175\n"
     ]
    }
   ],
   "source": [
    "train_causal_and_prediction_models(X, y_class, y_amount, task_type_class, task_type_amount, scale_class, scale_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_causal_and_prediction_models(X, y_amount, y_class, task_type_amount, task_type_class, scale_amount, scale_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection: Handling Imbalanced Dataset with CCNet\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial explores the use of a Cooperative Network (CCNet) to address challenges associated with imbalanced datasets in the domain of credit card fraud detection. By leveraging the power of data generation, we aim to enhance the diversity and volume of training data, thereby improving the robustness and accuracy of models designed to identify fraudulent transactions.\n",
    "\n",
    "## Tutorial Goals\n",
    "\n",
    "The objectives of this tutorial are designed to guide you through the process of enhancing data quality and model performance:\n",
    "\n",
    "### Dataset Recreation with CCNet\n",
    "- **Understand Data Augmentation**: Learn how encoding techniques can be used to generate synthetic data instances that closely mimic the characteristics of real-world fraudulent and non-fraudulent transactions.\n",
    "- **Impact on Model Training**: Assess how augmenting the dataset influences the training process and subsequently, the model's ability to generalize from training to real-world scenarios.\n",
    "\n",
    "### Model Training and Evaluation\n",
    "- **Dual Model Training**: Train two distinct models to directly compare performance metrics:\n",
    "  - A model trained on the **original dataset**.\n",
    "  - A model trained on the **CCNet-augmented dataset**.\n",
    "- **Performance Metrics**: Use the F1 score, a critical measure for models operating on imbalanced datasets, to evaluate and compare the effectiveness of these models.\n",
    "\n",
    "### Testing and Validation\n",
    "- **Independent Model Testing**: Conduct a thorough evaluation of both models using a standalone test set that was not involved in the training phase.\n",
    "- **Objective Analysis**: Critically analyze the outcomes to validate whether data augmentation through CCNet offers a tangible benefit in detecting credit card fraud.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By the end of this tutorial, participants will not only grasp the theoretical underpinnings of using synthetic data to combat data imbalance but also gain hands-on experience in applying these concepts through CCNet to potentially enhance model performance in fraud detection tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility of experiments\n",
    "from nn.utils.init import set_random_seed\n",
    "set_random_seed(0)\n",
    "\n",
    "# Importing configuration setups for ML parameters and data\n",
    "import torch\n",
    "from tools.setting.ml_params import MLParameters\n",
    "from tools.setting.data_config import DataConfig\n",
    "from trainer_hub import TrainerHub\n",
    "\n",
    "# Configuration for the data handling, defining dataset specifics and the task type\n",
    "data_config = DataConfig(dataset_name='CreditCardFraudDetection', task_type='binary_classification', obs_shape=[num_features], label_size=1, explain_size=num_features - 1)\n",
    "\n",
    "# Initializing ML parameters without a core model and setting the encoder model to 'tabnet' with specific configurations\n",
    "ml_params = MLParameters(ccnet_network='tabnet', encoder_network='none')\n",
    "\n",
    "# Setting training parameters and device configuration\n",
    "ml_params.training.num_epoch = 10\n",
    "ml_params.model.ccnet_config.num_layers = 4\n",
    "ml_params.algorithm.error_function = 'mse'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Create a TrainerHub instance to manage training and data processing\n",
    "trainer_hub = TrainerHub(ml_params, data_config, device, use_print=True, use_wandb=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(X, test_size=TEST_SIZE, shuffle=False)\n",
    "y_train, y_test = train_test_split(y_class, test_size=TEST_SIZE, shuffle=False)\n",
    "\n",
    "# Labeled datasets for supervised learning tasks\n",
    "trainset = LabeledDataset(X_train, y_train)  # Corrected to include training data\n",
    "testset = LabeledDataset(X_test, y_test)     # Test set with proper labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_hub.train(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tools.loader import collate_fn\n",
    "\n",
    "len_trainset = len(trainset)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=256, collate_fn=collate_fn, shuffle=False)\n",
    "causal_model = trainer_hub.ccnet\n",
    "# Initialize the recreated dataset container\n",
    "\n",
    "# Generate synthetic data through the model to augment the training dataset\n",
    "data = X_train.to(device)\n",
    "# Generate a large amount of synthetic data\n",
    "explain = causal_model.explain(data)\n",
    "\n",
    "generated_data, generated_label = causal_model.generate(explain)\n",
    "\n",
    "recreated_training_data = generated_data.squeeze(1).detach().cpu()\n",
    "recreated_labels = generated_label.detach().cpu().argmax(dim=-1).unsqueeze(-1)\n",
    "\n",
    "\n",
    "print(f\"Recreated Training Data Shape: {recreated_training_data.shape}\")\n",
    "print(f\"Recreated Labels Shape: {recreated_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming recreated_dataset is a PyTorch tensor already available in your context\n",
    "ccnet_recreated_dataset = LabeledDataset(recreated_training_data.numpy(), recreated_labels.numpy())\n",
    "\n",
    "# Print the shapes of the datasets for verification\n",
    "print(f\"Labeled Original Trainset Shape: {len(trainset)}, {trainset.x.shape[1]}\")\n",
    "print(f\"CCNet Recreated Dataset Shape: {len(ccnet_recreated_dataset)}, {ccnet_recreated_dataset.x.shape[1]}\")\n",
    "\n",
    "print(f\"Labeled Original Testset Shape: {len(testset)}, {testset.x.shape[1]}\")\n",
    "\n",
    "# Retrieve number of features and classes from the recreated dataset\n",
    "num_features = recreated_training_data.shape[1]\n",
    "num_classes = recreated_labels.shape[1]\n",
    "\n",
    "print(f\"Number of Features: {num_features}\")\n",
    "print(f\"Number of Classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised_model(model, dataset, num_epoch=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    # Initialize the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # Ensure reproducibility by resetting the random seed\n",
    "    # Create DataLoader for batch processing\n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    # Training loop\n",
    "    for epoch in range(num_epoch):  # Train for 2 epochs as an example\n",
    "        for i, (data, label) in enumerate(trainloader):\n",
    "            data = data.to(device).clone().detach()\n",
    "            label = label.to(device).float()\n",
    "            # Perform forward pass\n",
    "            output = model(data)\n",
    "            # Compute loss\n",
    "            loss = torch.nn.functional.binary_cross_entropy(output, label)\n",
    "            # Backward pass to compute gradients\n",
    "            loss.backward()\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train a model on the recreated dataset\n",
    "model_trained_on_recreated = PredictionModel(input_size=num_features, output_size=num_classes, task_type='binary_classification').to(device)\n",
    "train_supervised_model(model_trained_on_recreated, ccnet_recreated_dataset)\n",
    "\n",
    "# Initialize and train a model on the original dataset\n",
    "model_trained_on_original = PredictionModel(input_size=num_features, output_size=num_classes, task_type='binary_classification').to(device)\n",
    "train_supervised_model(model_trained_on_original, trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_f1_score(model, testset, batch_size=256):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    # DataLoader for testing\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # No gradient computation needed during inference\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(data)\n",
    "            # Process output for binary classification\n",
    "            predicted = (output.squeeze() > 0.5).long()\n",
    "            y_true.extend(label.squeeze().long().cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Compute and return the F1 score\n",
    "    score = f1_score(y_true, y_pred, average='binary')\n",
    "    return score\n",
    "\n",
    "# Calculate F1 scores for both models\n",
    "f1_score_original = get_f1_score(model_trained_on_original, testset)\n",
    "f1_score_recreated = get_f1_score(model_trained_on_recreated, testset)\n",
    "\n",
    "# Output the results\n",
    "print(\"F1 score of the supervised learning model trained on the original data: \", f1_score_original)\n",
    "print(\"F1 score of the supervised learning model trained on the recreated data: \", f1_score_recreated)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
